---
title: "CYO Project - Efficacy of Several Popular ML Algorithms"
author: "KST"
date: "11-June-2025"
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 5
    fig_caption: true
    latex_engine: lualatex
    includes:
      in_header: cyo_formatting.tex
  html_document:
    toc: true
    toc_depth: 5
    df_print: paged
comments: Report for final submission. Reorganised chapters for easier navigation & comprehension
version: 0.5.3
---

\newpage

# Acknowledgements {.unnumbered}

As earlier, I would like to thank Professor Rafael A Irizzary , The Teaching & Support Staff of the Department of Biostatistics, The Harvard University and the edX platform for providing students from all over world the opportunity to learn the fundamentals of Data Science at their own pace and convenience. The availability of such facilities and the accommodating structure of the course is valuable beyond words to those who are disadvantaged for time and who otherwise cannot afford to attend classroom sessions or learn according to a fixed schedule.

There are several references listed towards the end of the this document and I would like to express my heartfelt gratitude to the Authors, Contributors and Collaborators of the articles that they refer to.

I would also like to thank the Authors, Contributors and Collaborators of the articles available at the URL listed below, for the guidance provided by these articles in the execution of the project and the preparation of this report.

<https://en.wikipedia.org/wiki/International_Classification_of_Diseases>

<https://en.wikipedia.org/wiki/List_of_ICD-9_codes>

<https://keras3.posit.co/>

<https://tensorflow.rstudio.com/>

<https://stringr.tidyverse.org/>

[https://stackoverflow.com/questions/50820409/sorting-output-from-variable-importance-fcaret-package](https://stackoverflow.com/questions/50820409/sorting-output-from-variable-importance-caret-package){.uri}

<https://www.r-bloggers.com/2018/12/rstudio-pandoc-html-to-markdown/>

<https://rmarkdown.rstudio.com/docs/reference/pandoc_convert.html>

<https://dplyr.tidyverse.org/reference/mutate_all.html>

<https://www.r-bloggers.com/2023/01/imputation-in-r-top-3-ways-for-imputing-missing-data/>

<https://datatricks.co.uk/one-hot-encoding-in-r-three-simple-methods>

<https://cran.r-project.org/web/views/MachineLearning.html>

<https://www.efta.int/sites/default/files/R2022.pdf>

<https://www.ebpi.uzh.ch/dam/jcr:dc0cef17-29c7-4e61-8d33-e690561ab7ae/mi_intro20191001.pdf>

<https://bookdown.org/yihui/rmarkdown/markdown-syntax.html>

<https://bookdown.org/yihui/bookdown/tables.html>

<https://bookdown.org/yihui/rmarkdown-cookbook/purl.html>

\*Note: All referenced URLs in this report were current and the content accessible at the time of writing this report.

\newpage

# Introduction

## Project Background

As our awareness grows about the multitude of Machine Learning (ML) Algorithms available for Data Analysis and the kind of problems they solve, one of the most important questions that arises is, which of them works the best in any given situation? Each of these Algorithms have their own strengths and weaknesses. A lot of the time, the datasets and the analysis tasks at hand themselves have a large role to play in the efficacy of these Algorithms. Even if the datasets address similar topics, the structure and contents of the datasets vary widely.

Through the course of this Project we will investigate the utility of several popular and well respected ML Algorithms as applied to a few publicly available datasets to understand what kind of performance and efficiency we can expect from them.

### Project Success Criteria

There are no defined quantitative Project Success Criteria as such. Success is assessed qualitatively based on the the insight we can gain from the project and if it will help us foster our learning about these Algorithms and increase our awareness of their applicability in different situations.

Within this context, our Project can be seen as being successful if Readers, when handed a new dataset to analyse, can use the information from the Project to answer questions like :-

1.  How to assess the dataset and the associated complexity
2.  How to get started with analysing the data
3.  What kind of processing might be required
4.  Which Algorithms to use to get some initial insight
5.  Which ones to use for more detailed analysis.

## Project Approach

The first task in starting off with the Project is to obtain publicly available and unencumbered datasets that can help satisfy the eligibility criteria for the Project to be considered authentic, insightful and innovative while not being a repetition or modification of work that is already done. The University of California, Irvine (UCI) Machine Learning datasets (<https://archive.ics.uci.edu/datasets>) in particular have been worked upon by a multitude of students from those at the Undergraduate level to those who are submitting their Doctoral thesis. Even a cursory Internet search throws up several pages of independent projects for each of the popular datasets. Needless to say, the first task is itself quite daunting.

As a part of this project we will try to build something that is useful practically. We will also try to choose datasets that are still not as frequently used as the others.

\newpage

As a common theme, we will choose datasets that are related to Cardiovascular Diseases (CVD). As anybody with personal experience can tell, CVD can have a huge bearing on the quality of life of those afflicted and their caregivers.

As a quick reminder of the debilitating nature of CVD, here is what the World Health Organisation (WHO) has to say about them.

**Cardiovascular diseases (CVDs)**

------------------------------------------------------------------------

Cardiovascular diseases (CVDs) are the leading cause of death globally, taking an estimated 17.9 million lives each year. CVDs are a group of disorders of the heart and blood vessels and include coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. More than four out of five CVD deaths are due to heart attacks and strokes, and one third of these deaths occur prematurely in people under 70 years of age.

------------------------------------------------------------------------

More Information is available about CVD at <https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1>

### Choice of Datasets

Our choices for the datasets, in increasing order of complexity and difficulty of prediction of the outcome(s) are:-

1.  Heart_Failure_Prediction (HFP) - The dataset is available at

    <https://www.openml.org/search?type=data&status=active&id=45950&sort=runs>.

    This is a relatively newer dataset. It is very simple as well

    The dataset uses the "[[Attribute-Relation File Format]{.underline}](https://en.wikipedia.org/wiki/Attribute-Relation_File_Format "Attribute-Relation File Format")".

2.  Myocardial infarction complications (MIC) - The dataset is available at <https://archive.ics.uci.edu/dataset/579/myocardial+infarction+complications>.

    The DOI in the dataset description points to the availability of the Comma Separated Variable (CSV) version of the dataset hosted at the University of Leicester's repository at

    <https://figshare.le.ac.uk/articles/dataset/Myocardial_infarction_complications_Database/12045261/3>.

    We will use the CSV version available at the University of Leicester's repository as it is easier to process. The CSV file is already formatted with the column headers and unknown values are represented using "?" marks.

3.  Diabetes 130-US Hospitals for Years 1999-2008 (d130) - The dataset is available at

    <https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008>.

    This dataset is more dated having been donated in 2014. Needless to say, there are several volumes of Projects based on this dataset. The list of Attributes/Features is very well described in the Research Paper available at

    <https://doi.org/10.1155/2014/781670>.

    The dataset is available in CSV format.

Through all 3 datasets, we will also explore the utility of Artificial Neural Networks and their suitability for the tasks at hand.

\newpage

### Dataset partitioning scheme for Training, Validation and Testing

We will create Training and Testing(Holdout sets) in the ratio of 80:20.

We will further split the Training set into "Training CV" and "Testing CV" sets in the ratio of 80:20. We will use "Training CV" for training our Algorithms and "Testing CV" for validation. For those who have not guessed it already, CV stands for Cross Validation.

**We will follow this split or partitioning scheme for all of our datasets.**

The choice of 80:20 is based on the size of the datasets which are quite small. With 80:20, we have the ability to gauge the performance of the algorithms on "Testing" data that is likely to vary from the "Training" data simulating actual performance.

With a split like 90:10, we would likely end up with too little "Testing" data to simulate actual performance.

With a split like 70:30 or 60:40, we would likely end up with too little "Training" data.

### Project Report

For the Project report, We will organise the Analysis and Results for each dataset into a separate chapter. This will help prevent overcrowding of information into any single chapter and also make it easier to navigate and follow the report.

To keep the length of the Report manageable and to avoid confusion and needless repetition. I have included only the report text and the outputs in the PDF format of the Report while the Rmd format contains the whole report text and code. As might be expected, the R file contains the entire code.

\newpage

## Additional Notes and Cautions

Here we record some Notes and Cautions that are related to this Project. This might help the Readers in understanding the Report better and also help them avoid some of the problems that I have faced and spent much time recovering from.

**Artificial Neural Networks (ANN)**

Artificial Neural Networks( ANN) are today some of the most sought after Machine Learning Techniques. They offer a lot of flexibility in building Algorithms due to their programmable nature. There are many ANN packages available. Some are pretty basic and used for simple tasks while some are extremely advanced and used for building some of the most complex [[Large Language Models (LLM)]{.underline}](https://en.wikipedia.org/wiki/Large_language_model).

A summary of the various Machine Learning packages, including ANN, that are available for the R language and their relative merits and demerits is provided at :-

<https://cran.r-project.org/web/views/MachineLearning.html>

Just like with the rest of our Project, our ambitions with ANN will be very grounded. ANN can be built to be extremely sophisticated and accurate. Building such ANN require plenty of time, effort and expertise. For this Project, our main aim will be to introduce ANN as a choice for Machine Learning and demonstrate how they can be built for achieving different goals. It is meant for those who are just starting off with ANN.

**For building ANN, we will use Keras and particularly Keras 3 which is the current major revision of Keras. This report has been prepared with "keras3 version 1.3.0" released 2025-03-03.**

Keras is well documented, easy to understand and use, particularly so for beginners. However, it is not a native R package in entirety. Keras is a deep learning API written in Python and capable of running on top of back-end libraries like JAX, TensorFlow or PyTorch.

The Keras R Package provides some wrapper functions that format the code written in R into a format understood by TensorFlow or JAX. All computation is done in Python and the results returned to R. This requires the installation of a Python Virtual Environment and the required Python libraries into the same.

The Procedure is fairly involved, needs large downloads, and is only recommended for those who are interested in running the code to check if it works as demonstrated.

Also care should be taken to ensure that only Keras 3 packages for R are installed and not the older Keras 2 packages as they could lead to conflicts and inexplicable behaviour with no explicitly displayed warnings about the version mismatches. For historical reasons [CRAN](https://cran.r-project.org) refers to Keras 2 as Keras.

If Graphical Processing Unit (GPU) usage for computations is desired, the installation is even more complex as the requisite packages for GPU computing need to be installed on the Operating System as well as within the Python Virtual Environment created for Keras. I have deliberately disabled usage of GPU computing in the environment to avoid any additional complexity. Most of the ANN are extremely simple and should run on any modern Laptop computer in quick time.

**The other behaviour to watch out for in Keras is that the results are not always consistent unless the Random Number Generator (RNG) seed is set for multiple different processing entities. Even with the RNG seed set, the models often provide different predictions between runs or when multiple models are run one after the other. This behaviour is observed even with all parameters being unchanged and using the CPU (instead of the GPU) for computations. After numerous failures and plenty of time lost, I have used a rather inelegant hack to get it to provide consistent results. However, the hack by itself is not sufficient when tuning the ANN, it is necessary to frequently check that the ANN works as intended by restarting R and running it afresh. Though frustrating during the tuning stage, it saves a lot of time and heartache in the longer run.**

You can read more about Keras and Keras for R at the following Internet URL.

<https://www.keras.io>

<https://keras.posit.co>

Keras provides 2 main API varieties, Sequential API and Functional API with differing complexities and capabilities. More information is available at the URL mentioned.

**Using Different Backend Frameworks with Keras**

You can read more about TensorFlow and JAX at the following URL:-

<https://tensorflow.rstudio.com>

<https://www.tensorflow.org/learn>

<https://docs.jax.dev/en/latest/beginner_guide.html#beginner-guide>

I have used JAX as the Back-end, if the readers and evaluators decide to choose a different Back-end, the results might be slightly different.

**Tuning Keras**

Tuning Keras requires a decent understanding of the API, various parameters and how they affect predictions. Some of the parameters that have been tuned in this report are :-

a.  Sequential API
    1.  Types of Layers
    2.  Numbers of Layers
    3.  Number of Units in each Layer
    4.  Activation Functions
    5.  Dropouts
    6.  Batch Size
    7.  Number of Epochs
    8.  Class Weights
b.  Functional API
    1.  All parameters in Sequential API
    2.  Merging Layers

All of these parameters work in concert to affect the predictions.

While changes due to batch size or the number of epochs are easy to track and visualise, Even small changes in parameters like the number of layers, number of units in a layer, dropout rates and the RNG seed used for dropout can sometimes vary the predictions drastically.

For tuning, the creators of Keras recommend using the Keras Tuner which is natively built in Python.

<https://keras.io/keras_tuner/>

An R wrapper is available,

<https://eagerai.github.io/kerastuneR/>

I have not used the Keras Tuner and have manually tuned the ANN in this report. The reasons are twofold.

-   Gain some insight into how each parameter affects the results and by how much.
-   Avoid any additional complexity in building the ANN.

The Readers can definitely explore using the Keras Tuner as they deem fit.

**Printing Keras Models**

I have disabled printing of the Models for ANN in the Rmd File and the R file. The reason is twofold.

1.  One needs to have an understanding of Keras API to understand what they mean.
2.  Reading them is not easy, they can be quite long and not very intelligible.

Readers who are interested in viewing them can uncomment the lines and have a look. I have provided the full code in both the Rmd and R files. The code provides greater insight into the way the Model is built rather than the Model Prints.

**Imputation**

Many packages are available for imputation and some of them like [Visualization and Imputation of Missing Values (VIM)](https://cran.r-project.org/web/packages/VIM/index.html)and [Multivariate Imputation by Chained Equations (mice)](https://cran.r-project.org/web/packages/mice/index.html) are extremely capable and good at imputing values. The choice of the packages is largely dependent on the dataset and the missing values and the algorithms that can be used to impute the missing values. Not to restate what is well articulated already, please refer to the following URL for more information about the topic.

<https://www.efta.int/sites/default/files/R2022.pdf>

<https://www.ebpi.uzh.ch/dam/jcr:dc0cef17-29c7-4e61-8d33-e690561ab7ae/mi_intro20191001.pdf>

For some of our datasets, particular the MIC dataset, imputation is an absolute necessity to run algorithms like ANN and Random Forest as these algorithms cannot work with missing values.

The accuracy of these algorithms is also largely dependent on imputation and any imputation that introduces inaccuracies into the dataset, also causes the predictions from the algorithms to be inaccurate.

ANN particularly suffer a lot from inaccurately imputed values.

This inaccuracy manifests itself in both Training and Testing. In Training, it can lead to inappropriate tuning of the ANN. In Testing, it often leads to results showing a poorly performing ANN because the Testing data has inaccurately imputed values for the features.

The imputation packages also offer a lot of tuning capabilities that can be used to increase the accuracy of the imputations. I have used capabilities that can be run in sufficient time and with sufficiently capable computing resources.

**R Markdown Conversion to PDF**

As indicated in the sibling report for the "MovieLens" project, R Markdown conversion to PDF is not very straightforward.

This particularly affects the column widths of tables and is quite well known within the LaTeX user community.

Tables generated using R code can be handled a lot better using the "chunk" options, the "kable" and "kableExtra" packages.

For more details, please refer to

<https://bookdown.org/yihui/rmarkdown/markdown-syntax.html>

<https://bookdown.org/yihui/bookdown/tables.html>

Being able to reproduce the content in the PDF document as closely as possible to the Rmd document requires good knowledge of LaTeX. I have done what I could to keep the documents as close as possible. When in doubt, please consider the Rmd document to be the single and most authoritative source of truth.

\newpage

**Usage of Forward Pipe Operator (\|\>) and Tidyverse magrittr Pipe Operator (%\>%)**

Both types of Pipe Operators are used in the code.

The Forward Pipe Operator( \|\>) is available in the R Base package. It is used in places where it should make it more convenient for those who wish to compare the code with ANN examples from other sources.

The Tidyverse [[magrittr]{.underline}](https://magrittr.tidyverse.org/articles/magrittr.html) Pipe Operator (%\>%) is available from the Tidyverse package and has been already used extensively in the courses in the "Data Science Series" and should be convenient for anybody who wishes to follow or compare the data processing and transformation operations using Tidyverse.

**XGBoost**

The versions of XGBoost available on CRAN at <https://cran.r-project.org> are quite dated. The newer packages are available at <https://dmlc.r-universe.dev> and [[https://cloud.r-project.org]{.underline}](https://cloud.r-project.org){.uri}[.]{.underline}

The newer packages use slightly different syntax which are more friendly and are also more accurate.

Support for "categoricals" is available in R through factors from XGBoost version 3.0 onwards, however the feature is considered experimental. According to the developers, only the Python package is fully supported.

<https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html>

I have not used "categoricals" in this report to avoid any additional complexity.

**This report has been prepared with "xgboost version 3.0.0.1" released 2025-03-14.**

If readers are already using an older version of XGBoost, they may see some differences in the values reported.

**Reuse of Variable names**

Compared to the sibling project, where I made a concious effort to keep the names of variables used for intermediate processing and reporting results completely separate, in this project, I have reused the variable names used for intermediate processing and reporting results as the code has a lot of similarities. Using the same variable names makes it easier to build, reproduce, debug and follow the code. However for this to work the variables need to be sanitised or deleted before reusing their names again.

As an example, almost all of the ANN follow a similar pattern and also use the same variable names making it a lot easier to compare different ANN.

In some places where I need to compile the variable names for reporting at the end, I have made a conscious effort to make the variable names self describing. Sometimes this has lead to extraordinarily long variable names.

**One-Hot Encoding**

For one-hot encoding, either the "one_hot" function from the "mltools" package can be used or a combination of the "dummyVars" function from the "caret" package and the generic predict function from the R "stats" package can be used.

Using the one_hot function is simple and the code is easier to follow. However, one caveat is that one_hot only supports unordered factors. dummyVars supports both ordered and unordered factors. dummyVars has a slightly more complex interface that needs a little bit of additional effort to follow.

I have used both one_hot and dummyVars in the Project based on need and simplicity.

There are a few places in the MIC dataset, where I have used two instances of dummyVars, one for the predictors and one for the outcome. The only reason for doing it was simplicity over brevity or concise code. I had initially built the code with only a single instance of "dummyVars" but soon realised that it would be too confusing for the readers to follow along. The biggest drawback in using a single instance is that the outcome could easily leak into the predictors. Keeping them separate avoids any such leakages.

**ConfusionMatrices**

I have used a lot of confusion matrices to depict the results, though they appear overwhelming or tedious sometimes, they are an Analyst's best friend when categorical outcomes are involved. At a glance, they can provide a lot of very important information about metrics like Overall Accuracy, Balanced Accuracy, Sensitivity, Specificity, The number of accurate and inaccurate predictions organised very conveniently. They help us easily gauge the performance of our Algorithms.

```{r setup, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE)

##########################################################
# Begin CYO Project
##########################################################

# As part of initial setup, Check if required libraries are installed already, if installed, load them, if not installed, install and load them.

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(OpenML)) install.packages("openML", repos = "http://cran.us.r-project.org")
if(!require(farff)) install.packages("farff", repos = "http://cran.us.r-project.org")
if(!require(devtools)) install.packages("devtools", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(class)) install.packages("class", repos = "http://cran.us.r-project.org")
if(!require(naivebayes)) install.packages("naivebayes", repos = "http://cran.us.r-project.org")
if(!require(reticulate)) install.packages("reticulate", repos = "http://cran.us.r-project.org")
if(!require(tensorflow)) install.packages("tensorflow", repos = "http://cran.us.r-project.org")
if(!require(keras3)) install.packages("keras3", repos = "http://cran.us.r-project.org")
if(!require(mice)) install.packages("mice", repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages('xgboost', repos = c('https://dmlc.r-universe.dev', 'https://cloud.r-project.org'))
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(adabag)) install.packages("adabag", repos = "http://cran.us.r-project.org")
if(!require(mltools)) install.packages("mltools", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
#keras3::install_keras(backend = c("tensorflow", "jax")) #Comment Out after initial installation

library(tidyverse)
library(caret)
library(ggthemes)
library(gridExtra)
library(purrr)
library(readr)
library(OpenML)
library(farff)
library(devtools)
library(randomForest)
library(class)
library(naivebayes)
library(nnet)
library(reticulate)
library(tensorflow)
library(keras3)
library(mice)
library(ranger)
library(ggplot2)
library(ggthemes)
library(xgboost)
library(rpart)
library(adabag)
library(mltools)
library(data.table)
library(matrixStats)
library(devtools)
library(kableExtra)

# Set Up Environment to disable GPU computing and use JAX
Sys.setenv("CUDA_VISIBLE_DEVICES" = "-1")
Sys.setenv("JAX_PLATFORMS" = "cpu")
use_backend("jax")

# Initialise Parallelisation
#### Use Parallel Library #####

library(parallel)               # Load parallel package, a core R package
library(doParallel)             # Load doparallel backend for functions that use it
nCores <- 8                     # Register 8 Threads (4 Cores with Hyperthreading)
registerDoParallel(nCores)      # Register Threads with doParallel
makeCluster(nCores)             # Turn on Parallel Processing for Registered number of Threads
```

\newpage

# Heart Failure Prediction (HFP)

The first dataset that we will pick up is the Heart_Failure_Prediction(HFP) dataset. The dataset is extremely simple, well structured and easy to work with..

## Dataset Introduction

The dataset, consists of 5000 clinical records of patients with Heart Failure. It includes the attributes described below.

+-----------+-------------------------+---------------------------------+-----------+--------------+
| Col Id    | Name                    | Description                     | Type      | Values       |
+:=========:+=========================+=================================+:==========+==============+
| 1         | age                     | age of the patient              | Numeric   | 48 Distinct  |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 2         | anaemia                 | presence of anaemia             | Binary    | 0: No        |
|           |                         |                                 |           |              |
|           |                         |                                 |           | 1: Yes       |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 3         | creatinine_phospokinase | level of creatinine             | Numeric   | 290 Distinct |
|           |                         |                                 |           |              |
|           |                         | phospokinase in the blood       |           |              |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 4         | diabetes                | presence of diabetes            | Binary    | 0: No        |
|           |                         |                                 |           |              |
|           |                         |                                 |           | 1: Yes       |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 5         | ejection_fraction       | percentage of blood leaving     | Numeric   | 17 Distinct  |
|           |                         |                                 |           |              |
|           |                         | the heart at each contraction   |           |              |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 6         | high_blood_pressure     | presence of high blood pressure | Binary    | 0: No        |
|           |                         |                                 |           |              |
|           |                         |                                 |           | 1: Yes       |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 7         | platelets               | platelet count in the blood     | Numeric   | 203 Distinct |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 8         | serum_creatinine        | level of serum creatinine       | Numeric   | 43 Distinct  |
|           |                         |                                 |           |              |
|           |                         | in the blood                    |           |              |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 9         | serum_sodium            | level of serum sodium           | Numeric   | 27 Distinct  |
|           |                         |                                 |           |              |
|           |                         | in the blood                    |           |              |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 10        | sex                     | gender of the patient           | Binary    | 0: Female    |
|           |                         |                                 |           |              |
|           |                         |                                 |           | 1: Male      |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 11        | smoking                 | smoking status of the patient   | Binary    | 0: No        |
|           |                         |                                 |           |              |
|           |                         |                                 |           | 1: Yes       |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 12        | time                    | follow-up time                  | Numeric   | 155 Distinct |
+-----------+-------------------------+---------------------------------+-----------+--------------+
|           |                         |                                 |           |              |
+-----------+-------------------------+---------------------------------+-----------+--------------+
| 13        | DEATH_EVENT             | indicator of death occurrence   | Binary    | 0: No        |
|           |                         |                                 |           |              |
|           |                         | during the follow-up period     |           | 1: Yes       |
+-----------+-------------------------+---------------------------------+-----------+--------------+

: Heart Failure Prediction - Attributes/Features

Exact units of the attributes are not provided but it is quite easy to decipher some of them like age (years) and time (days) from the range of values. The actual units are not relevant to our predictions.

**\*Note: For this dataset, we will treat Numeric variables as being Continuous**

This dataset can be used for analysing the relationship between various clinical attributes and the occurrence of death events in patients with heart failure. It can help in predicting the risk factors associated with heart failure mortality.

## Analysis

As indicated in the Introduction, the dataset is available from the <https://openml.org> website and is provided in the "[Attribute-Relation File Format](https://en.wikipedia.org/wiki/Attribute-Relation_File_Format "Attribute-Relation File Format")" (ARF) format. The data can easily be extracted into a data frame for further processing. Let us have a look at the structure of the dataset. We will also record the survival rate of patients for reference.

```{r Download HFP Dataset, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

##########################################################
# Begin Analysis of HFP Dataset
##########################################################

##########################################################
# Download the Raw Data from the respective repositories as the source for truth for the Datasets
##########################################################

# Heart_Failure_Prediction
# https://www.openml.org/search?type=data&status=active&id=45950&sort=runs
# https://api.openml.org/data/download/22120391/dataset - Since dataset is in arff format, we will use the openML tools to download and save the data locally
# openML saves the Data Automatically to a local Cache and reads from the Cache within the same R Session. However the Cache is not persistent between R sessions and we can instead store a binary value of the Dataset if required for offline working. 

hfp_rdata_file <- "heart_failure_prediction.RData"

if(file.exists(hfp_rdata_file)){
      load(hfp_rdata_file)
      rm(hfp_rdata_file)
  
} else {
  hfp <- getOMLDataSet(data.id = 45950L)
  save(hfp, file = "heart_failure_prediction.RData")
  rm(hfp_rdata_file)
}

# Extract Data from the Downloaded OpenML Dataset 

hfp_data <- hfp$data

# Print for Visualisation
print("Structure of the HFP dataset", quote=FALSE)
str(hfp_data)


# Reecord & print Survival Rate for the whole dataset. This will serve as our reference to evaluate the accuracy of our algorithms
hfp_survival_rate <- 1 - mean(hfp_data$DEATH_EVENT)

# Print survival rate for reference
print(c("The Survival Rate of patients in the HFP dataset is :", hfp_survival_rate), quote=FALSE)

```

### Dataset Preparation

Very little preparation if any is required to work with this compact and extremely well structured dataset. The only dataset modification required is to convert the binary variables and the outcome as "factors". We will treat the rest of the variables as continuous variables.

We will then create our Training, Testing, Training CV and Testing CV datasets.

```{r Load HFP Datasets for initial Analysis, Use 80% for Training and 20% for Testing, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


# Prepare Data for further analysis taking advantage of R's capabilities. Convert Binary Variables as factors. 



hfp_data <- hfp_data %>% 
              mutate(anaemia = as.factor(anaemia), diabetes = as.factor(diabetes), high_blood_pressure = as.factor(high_blood_pressure), sex=as.factor(sex), smoking=as.factor(smoking), DEATH_EVENT = as.factor(DEATH_EVENT)) 

# Create Datasets for Training and Testing

set.seed(1024)

hfp_test_index <- createDataPartition(y = hfp_data$DEATH_EVENT, times = 1, p = 0.2, list = FALSE)
hfp_train <- hfp_data[-hfp_test_index,]
hfp_test <- hfp_data[hfp_test_index,]

# Create Datasets for Cross Validation 

set.seed(1024)

hfp_test_index_cv <- createDataPartition(y = hfp_train$DEATH_EVENT, times = 1, p = 0.2, list = FALSE)
hfp_cv_train_set <- hfp_train[-hfp_test_index_cv,]
hfp_cv_test_set <- hfp_train[hfp_test_index_cv,]


```

**Distribution of Variables**

Let us visualise the distribution of the variables. We will only look at the values for the "Training" dataset.

```{r Visualise HFP Datasets for initial Analysis, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Generate Histograms to visualise the distribution of values across various ranges. Arrange two graphs per row 

grid.arrange(histogram(hfp_train$age, type="count", xlab = "Age"), histogram(hfp_train$anaemia, type="count", xlab = "Anaemia"), histogram(hfp_train$creatinine_phosphokinase, type="count", xlab = "Creatinine Phospokinase"), histogram(hfp_train$diabetes, type="count", xlab = "Diabetes"), ncol = 2)

grid.arrange(histogram(hfp_train$ejection_fraction, type="count", xlab = "Ejection Fraction"), histogram(hfp_train$high_blood_pressure, type="count", xlab = "High Blood Pressure"), histogram(hfp_train$platelets, type="count", xlab = "Platelets"),histogram(hfp_train$serum_creatinine, type="count", xlab = "Serum Creatinine"),ncol =2)

grid.arrange(histogram(hfp_train$serum_sodium, type="count", xlab = "Serum Sodium"), histogram(hfp_train$sex, type="count", xlab = "Sex"), histogram(hfp_train$smoking, type="count", xlab = "Smoking"),histogram(hfp_train$time, type="count", xlab = "Time"),ncol =2)


```

We will start with some basic algorithms first and then move on to more advanced algorithms.

Let us analyse the dataset using Random Forest and Naive Bayes.

### Random Forest

For analysis using Random Forest, we will use the inbuilt function within the "randomForest" library to tune Random Forest to choose the right "$m_{try}$" values. We will also use the "matrix notation" instead of the "formula notation" as it is a lot easier to program and a lot quicker in terms of performance.

```{r Perform Initial Analysis for HFP Dataset using Random Forest, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width="50%"}


# Use the inbuilt tuneRF functionality to find the best mtry value to reduce additional computation

# Use Matrix Notation rather than Formula Notation as it is a lot more efficient

print("=====================================",quote = FALSE)
print("The Random Forest mtry values and error rates are",quote = FALSE) 

set.seed(1024)
best_mtry_rf_tune_hfp <- tuneRF(hfp_cv_train_set[,1:12], hfp_cv_train_set[,13],stepFactor = 0.5, improve = 0.00001,trace = TRUE, plot = TRUE, doBest = TRUE) 

# Print summary about Random Forest. Only print what is necessary and easy to comprehend
print("=====================================",quote = FALSE)
print( c(" Details for Random Forest for the HFP Dataset are: "),quote = FALSE, justify = "left") 
print(c("Prediction Type :",best_mtry_rf_tune_hfp$type),quote = FALSE)
print(c("Number of Trees (ntree) :",best_mtry_rf_tune_hfp$ntree),quote = FALSE)
print(c("mtry value :",best_mtry_rf_tune_hfp$mtry),quote = FALSE)
print("=====================================",quote = FALSE)

# Extract and print the variables in order of decreasing importance
imp <- as.data.frame(randomForest::importance(best_mtry_rf_tune_hfp, type=2))
imp <- data.frame(Importance = imp$MeanDecreaseGini,
           names   = rownames(imp))
imp <- imp[order(imp$Importance, decreasing = TRUE),]

print("The Variables in order of decreasing importance in prediction are:")
print("=============================================",quote = FALSE)
knitr::kable(x = imp, col.names = c("Col Id", "Importance", "Names"), caption = "Heart Failure Prediction - Variable Importance (MeanDecreaseGini)")
print("=============================================",quote = FALSE)

# Generate Predictions for the Test Set
pred_rf_hfp <- predict(best_mtry_rf_tune_hfp, newdata = hfp_cv_test_set[,1:12], type = "response" )

# Print overall accuracy
print("The Accuracy of the Predictions are:",quote = FALSE)
print("=============================================",quote = FALSE)
mean(pred_rf_hfp == hfp_cv_test_set$DEATH_EVENT)
print("=============================================",quote = FALSE)

# Print confusion matrix
print("The confusion Matrix for the predictions is:",quote = FALSE)
print("=============================================")
confusionMatrix(data = pred_rf_hfp,reference =  hfp_cv_test_set$DEATH_EVENT)
print("=============================================",quote = FALSE)

# Remove data and variables that are not required anymore
rm(best_mtry_rf_tune_hfp, pred_rf_hfp,imp)
```

We have amazing results using Random Forest, It is not very surprising given how well the dataset is populated and structured.

To understand more about using Random Forest for Classification Problems and how the Variable Importance is calculated, have a look at :-

<https://cran.r-project.org/web/packages/randomForest/randomForest.pdf>

<https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings>

### Naive Bayes

Let us look at Naive Bayes next. We will make some changes to the default parameters based on our observations of the distributions of the variables. We will use the Kernel Distribution Estimation (KDE) for all our continuous variables instead of using the Gaussian Distribution.

```{r Perform Initial Analysis for HFP Dataset using Naive Bayes, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Naive Bayes

# Train Naive Bayes on the Training Set      
fit_nb_train_hfp <- naive_bayes(x = hfp_cv_train_set[,1:12], y = hfp_cv_train_set[,13], usekernel = TRUE, usepoisson = FALSE, laplace = 1)
summary(fit_nb_train_hfp)

# Generate Predictions for the Test Set
pred_nb_test_hfp <- predict(fit_nb_train_hfp, newdata = hfp_cv_test_set, type = c("class"))

# Print overall accuracy
print("The Accuracy of the Predictions are:",quote=FALSE)
print("=============================================",quote = FALSE)
mean(pred_nb_test_hfp == hfp_cv_test_set$DEATH_EVENT)

# Print confusion matrix
print("The confusion Matrix for the predictions is:",quote = FALSE)
print("=============================================",quote = FALSE)
confusionMatrix(data = pred_nb_test_hfp,reference =  hfp_cv_test_set$DEATH_EVENT)
print("=============================================",quote = FALSE)


# Remove data and variables that are not required anymore
rm(fit_nb_train_hfp, pred_nb_test_hfp)

```

Naive Bayes gives us some decent results with tuning, though the results are not as good as those provided by Random Forest.

For those looking to understand how to tune Naive Bayes using different distributions and estimations. Please have a look at

<https://search.r-project.org/CRAN/refmans/naivebayes/html/naive_bayes.html>

Here is an extract that provides some detail for immediate reference.

------------------------------------------------------------------------

The class "numeric" contains "double" (double precision floating point numbers) and "integer". Depending on the parameters `usekernel` and `usepoisson` different class conditional distributions are applied to columns in the dataset with the class "numeric":

-   If `usekernel=FALSE` and `poisson=FALSE` then Gaussian distribution is applied to each "numeric" variable ("numeric"&"integer" or "numeric"&"double")

-   If `usekernel=TRUE` and `poisson=FALSE` then kernel density estimation (KDE) is applied to each "numeric" variable ("numeric"&"integer" or "numeric"&"double")

-   If `usekernel=FALSE` and `poisson=TRUE` then Gaussian distribution is applied to each "double" vector and Poisson to each "integer" vector. (Gaussian: "numeric" & "double"; Poisson: "numeric" & "integer")

-   If `usekernel=TRUE` and `poisson=TRUE` then kernel density estimation (KDE) is applied to each "double" vector and Poisson to each "integer" vector. (KDE: "numeric" & "double"; Poisson: "numeric" & "integer")

By default `usekernel=FALSE` and `poisson=FALSE`, thus Gaussian is applied to each numeric variable.

On the other hand, "character", "factor" and "logical" variables are assigned to the Categorical distribution with Bernoulli being its special case.

------------------------------------------------------------------------

\newpage

### Artificial Neural Networks (ANN)

Let us investigate using Artificial Neural Networks (ANN) next. For ANN, we will use the Keras 3 package as explained in the "Introduction" chapter. Since our dataset is very simple, it is a very good candidate for using the Keras Sequential API.

```{r Perform Initial Analysis for HFP Dataset using Neural Networks using Keras, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width="75%"}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################

# Since Neural Networks do not understand Factors, we will need to use the Original Numeric Values. As we have only Binary Factors, One-Hot-Encoding or other methods are not required. 

hfp_data_orig <- hfp$data

# Create Datasets for Training and Testing

hfp_train_orig <- hfp_data_orig[-hfp_test_index,]
hfp_test_orig <- hfp_data_orig[hfp_test_index,]

# Create Datasets for Cross Validation 

hfp_cv_train_orig_set <- hfp_train_orig[-hfp_test_index_cv,]
hfp_cv_test_orig_set <- hfp_train_orig[hfp_test_index_cv,]

# Remove Variables that are not needed anymore
rm(hfp_test_index, hfp_test_index_cv)

# We also need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation

# Because there are no missing values or single value variables in the datset, it is not necessary to check for columns that can cause NaN generation during scaling

feature_names <- colnames(hfp_cv_train_orig_set) %>% setdiff("DEATH_EVENT")

train_features <- as.matrix(hfp_cv_train_orig_set[feature_names])
train_targets <- as.matrix(as.numeric(hfp_cv_train_orig_set$DEATH_EVENT))

val_features <- as.matrix(hfp_cv_test_orig_set[feature_names])
val_targets <- as.matrix(as.numeric(hfp_cv_test_orig_set$DEATH_EVENT))

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))


# Let us build the Nueral Network 

model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(192, activation = "relu") |>
  layer_dense(192, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(192, activation = "relu") |>
  layer_dense(192, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(96, activation = "relu") |>
  layer_dense(96, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

# Let us print the model for visualisation. 
# commented out for Report creation
# print("The Keras Sequential API model is",quote = FALSE)
# print("=============================================",quote = FALSE)
# model
# print("=============================================",quote = FALSE)

# Collect counts to derive initial weights
counts <- table(hfp_cv_train_orig_set$DEATH_EVENT) # Counts for Training Set 

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set",quote = FALSE)
# counts
# print("counts for testing set", qoute=FALSE)
# table(hfp_cv_test_orig_set$DEATH_EVENT) # Counts for Validation Set

# Setup weights. Weights are modified manually. Changes are not updated automatically 
weight_for_0 = (1 / counts["0"]) 
weight_for_1 = (1 / counts["1"]) 

################# Train the Model #################

# Setup Metrics
metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)

# Compile Model
model |> compile(
  optimizer = optimizer_adam(1e-2),
  loss = "binary_crossentropy",
  metrics = metrics
)


# Fit Model
class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)

plot_seq_api_model <- model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 512,
  epochs = 30,
  verbose = 0 # Set verbose=2 during the tuning stage
)

# Print metrics for visualisation
print("The model metrics and trend during training are",quote = FALSE)
print("=============================================",quote = FALSE)
plot(plot_seq_api_model)
print("=============================================",quote = FALSE)

# Prepare Predictions

val_pred <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }

pred_correct <- hfp_cv_test_orig_set$DEATH_EVENT == val_pred

# Print overall accuracy
print("=============================================",quote = FALSE)
print(c("The Accuracy of the Predictions is: ", mean(pred_correct)), quote=FALSE)
print("=============================================",quote = FALSE)


# Collect death events
deaths <- hfp_cv_test_orig_set$DEATH_EVENT == 1

# Prepare and print summary for  death events 
n_deaths_detected <- sum(deaths & pred_correct)
n_deaths_missed <- sum(deaths & !pred_correct)
n_live_flagged <- sum(!deaths & !pred_correct)

print("=============================================",quote = FALSE)
print(c("deaths detected :", n_deaths_detected),quote = FALSE)
print(c("deaths missed :", n_deaths_missed),quote = FALSE)
print(c("live cases flagged :", n_live_flagged),quote = FALSE)
print("=============================================",quote = FALSE)

# Print confusion matrix
print("The confusion Matrix for the predictions is:",quote = FALSE)
print("=============================================",quote = FALSE)
confusionMatrix(data = as.factor(val_pred), reference =  as.factor(hfp_cv_test_orig_set$DEATH_EVENT))
print("=============================================",quote = FALSE)

# Remove data that is no longer required
rm(callbacks,class_weight,hfp_cv_train_orig_set, hfp_cv_test_orig_set,  metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct, val_pred, weight_for_0, weight_for_1, plot_seq_api_model)
```

We have pretty good results with ANN, though they are not as good as what Random Forest provides.

\newpage

## Predictions for Holdout Set

Now let us see how our algorithms do for our Holdout or Test set. We should expect some slight differences in the accuracy of the predictions. We will only evaluate Random Forest and ANN as our choices for the Holdout set.

### Random Forest

```{r Perform Final Analysis for HFP Dataset using Random Forest, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width= "50%"}

# Perform Finaly Analysis on Holdout set
# Use the inbuilt tuneRF functionality to find the best mtry value to reduce additional computation

# Use Matrix Notation rather than Formula Notation as it is a lot more efficient

print("=====================================",quote = FALSE)
print("The Random Forest mtry values and error rates are",quote = FALSE) 
set.seed(1024)
best_mtry_rf_tune_hfp_final <- tuneRF(hfp_train[,1:12], hfp_train[,13],stepFactor = 0.5, improve = 0.00001,trace = TRUE, plot = TRUE, doBest = TRUE) 


# Print summary about Random Forest. Only print what is necessary and easy to comprehend
print("=====================================",quote = FALSE)
print( c(" Details for Random Forest  for the HFP Dataset are: "),quote = FALSE, justify = "left") 
print(c("Prediction Type: ",best_mtry_rf_tune_hfp_final$type),quote = FALSE)
print(c("Number of Trees (ntree): ",best_mtry_rf_tune_hfp_final$ntree),quote = FALSE)
print(c("mtry value: ",best_mtry_rf_tune_hfp_final$mtry),quote = FALSE)
print("=====================================",quote = FALSE)

# Extract, Sort and Print Random Forest Variable importance in order of decreasing importance 

imp <- as.data.frame(randomForest::importance(best_mtry_rf_tune_hfp_final))
imp <- data.frame(Importance = imp$MeanDecreaseGini,
           names   = rownames(imp))
imp <- imp[order(imp$Importance, decreasing = TRUE),]

print("The Variables in order of decreasing importance in prediction are:")
print("=============================================",quote = FALSE)
knitr::kable(x = imp, col.names = c("Col Id", "Importance", "Names"), caption = "Heart Failure Prediction - Variable Importance (MeanDecreaseGini)")
print("=============================================",quote = FALSE)

# Generate Predictions for the Test Set
pred_rf_hfp_final <- predict(best_mtry_rf_tune_hfp_final, newdata = hfp_test[,1:12], type = "response" )

# Print overall accuracy
print("The Accuracy of the predictions is:",quote = FALSE)
mean(pred_rf_hfp_final == hfp_test$DEATH_EVENT)

# Print confusion matrix
print("The confusion Matrix for the predictions is:",quote = FALSE)
print("=============================================", quote= FALSE)
hfp_cm_rf_final <- confusionMatrix(data = pred_rf_hfp_final,reference =  hfp_test$DEATH_EVENT)
hfp_cm_rf_final
print("=============================================",quote = FALSE)

# Remove data that is no longer required
rm(best_mtry_rf_tune_hfp_final, pred_rf_hfp_final, imp)
```

With Random Forest, we have slightly inferior results compared to the predictions for the validation set but nothing outside of expected values.

### Artificial Neural Networks

```{r Perform Final Analysis for HFP Dataset using Neural Networks for the Holdout set, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################

# Since Neural Networks do not understand Factors, we will need to use the Original Numeric Values. As we have only Binary Factors, One-Hot-Encoding or other methods are not required. 


# We also need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation

feature_names <- colnames(hfp_train_orig) %>% setdiff("DEATH_EVENT")

train_features <- as.matrix(hfp_train_orig[feature_names])
train_targets <- as.matrix(as.numeric(hfp_train_orig$DEATH_EVENT))

val_features <- as.matrix(hfp_test_orig[feature_names])
val_targets <- as.matrix(as.numeric(hfp_test_orig$DEATH_EVENT))

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

# Let us build the Nueral Network 

model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(192, activation = "relu") |>
  layer_dense(192, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(192, activation = "relu") |>
  layer_dense(192, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(96, activation = "relu") |>
  layer_dense(96, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

# Print model
# Commented out for Report creation
# model

# Collect counts to generate initial weights
counts <- table(hfp_train_orig$DEATH_EVENT) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(hfp_test_orig$DEATH_EVENT) # Counts for Validation Set

# Setup weights. Weights are modified manually. Changes are not updated automatically 
weight_for_0 = (1 / counts["0"]) 
weight_for_1 = (1 / counts["1"])

# Train the Model 

metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-2),
  loss = "binary_crossentropy",
  metrics = metrics
)

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)


model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 512,
  epochs = 30,
  verbose = 0 # Set verbose = 2 during tuning
)


# Prepare Predictions

val_pred_final <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }

# Record and print overall accuracy
pred_correct_final <- hfp_test_orig$DEATH_EVENT == val_pred_final
print("=============================================",quote = FALSE)
print("The Accuracy of the Predictions is:",quote = FALSE)
mean(pred_correct_final)
print("=============================================",quote = FALSE)

# Collect death events
deaths_final <- hfp_test_orig$DEATH_EVENT == 1

# Prepare and print summary for death events
n_deaths_detected <- sum(deaths_final & pred_correct_final)
n_deaths_missed <- sum(deaths_final & !pred_correct_final)
n_live_flagged <- sum(!deaths_final & !pred_correct_final)
print("=============================================", quote= FALSE)
print(c("deaths detected", n_deaths_detected))
print(c("deaths missed", n_deaths_missed))
print(c("live cases flagged", n_live_flagged))
print("=============================================", quote= FALSE)

# Print confusion matrix
print("The confusion Matrix for the predictions is:",quote = FALSE)
print("=============================================", quote= FALSE)
hfp_cm_ann_final <- confusionMatrix(data = as.factor(val_pred_final), reference =  as.factor(hfp_test_orig$DEATH_EVENT))
hfp_cm_ann_final
print("=============================================", quote= FALSE)

# Remove data that is no longer required
rm(callbacks,class_weight,hfp_cv_train_orig_set, hfp_cv_test_orig_set,  metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct_final, val_pred_final, weight_for_0, weight_for_1, deaths_final)

```

The same is true for ANN too, with ANN offering slightly inferior performance compared to the predictions for the validation set.

\newpage

## Results & Inference

```{r HFP Results Summary, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Print Survival Rate for the whole dataset for reference 
print(c("The Survival Rate of patients in the HFP dataset is :", hfp_survival_rate), quote=FALSE)


# Prepare final results table

hfp_summary <- data.frame(c("Random Forest", "ANN-SequentialAPI"), c(hfp_cm_rf_final$overall["Accuracy"], hfp_cm_ann_final$overall["Accuracy"]), c(hfp_cm_rf_final$byClass["Balanced Accuracy"], hfp_cm_ann_final$byClass["Balanced Accuracy"]), c(hfp_cm_rf_final$table[2,2], hfp_cm_ann_final$table[2,2]), c(hfp_cm_rf_final$table[1,2], hfp_cm_ann_final$table[1,2]),c( hfp_cm_rf_final$table[2,1],  hfp_cm_ann_final$table[2,1]))

knitr::kable(x = hfp_summary, col.names = c("Model", "overall accuracy", "balanced accuracy", "deaths detected", "deaths missed", "live flagged"), caption = "Heart Failure Prediction - Results Summary ", digits = 4) %>% kable_styling(font_size = 10)
```

It has been rather easy so far with a very well structured and very easy to predict dataset. We have also seen that even with a dataset this simple, different Algorithms provide predictions with different accuracies.

Naive Bayes without tuning (using defaults) performs quite poorly but can be improved with tuning.

ANN are pretty good though they are a lot more intensive in terms of programming effort as well as computational needs. We can build ANN which are more accurate but again will require additional efforts.

Random Forest provides us with very good results and is quite simple to program and does not require as much computational resources as ANN. In terms of the returns obtained for the efforts spent, it is the most optimal choice for this dataset.

```{r HFP Data Cleanup, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove residual data related to the HFP dataset and run the garbage collector

rm(hfp,hfp_data, hfp_test, hfp_train, hfp_data_orig, hfp_test_orig, hfp_train_orig, hfp_cv_train_set, hfp_cv_test_set, hfp_cm_rf_final, hfp_cm_ann_final, hfp_summary, hfp_survival_rate)

gc()

##########################################################
# End Analysis of HFP Dataset
##########################################################
```

\newpage

# Myocardial Infarction Complications (MIC)

We will pick up the Myocardial Infarction Complications (MIC) or Heart Attack Prediction dataset next.

## Dataset Introduction

A very succinct description of the dataset and it contents is provided in References [3] and [4].

The description can be downloaded from

<https://figshare.le.ac.uk/ndownloader/files/22803695>

### List of Attributes/Features

The list of features is extremely long. Readers who are interested can have a look at Appendix A - MIC Dataset Features & Outcomes for more details about them.

The dataset has 1700 observations and 112 Features.

The very first feature in column 1 is the Record ID (ID).

------------------------------------------------------------------------

1\. Record ID (ID): Unique identifier. Cannot be related to participant. It can be used for reference only.

Type : Numeric/ Integer

------------------------------------------------------------------------

It can be disregarded giving us 111 features to work with. Not all observations have all features.

One of the important characteristics of this dataset is that all categorical and the only partially categorical feature can be treated as Ordinals (Ordered Factors) whose numerical value can be used as-is or can be dummy encoded using cumulative encoding.

It also has 11 "complications" for outcomes resulting from MIC. These are Non-Lethal

It also has 1 "complication" for Lethal outcome coded in column 124. The Lethal outcome is further categorised as :-

------------------------------------------------------------------------

124\. Lethal outcome (cause) (LET_IS):

Type: Category (Not Ordered)

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | unknown (alive)                      | 1429  | 84.06%   |
| 1     | cardiogenic shock                    | 110   | 6.47%    |
| 2     | pulmonary edema                      | 18    | 1.06%    |
| 3     | myocardial rupture                   | 54    | 3.18%    |
| 4     | progress of congestive heart failure | 23    | 1.35%    |
| 5     | thromboembolism                      | 12    | 0.71%    |
| 6     | asystole                             | 27    | 1.59%    |
| 7     | ventricular fibrillation             | 27    | 1.59%    |
|       | Missing                              |       | 0.0%     |

------------------------------------------------------------------------

### Problems to solve

In general columns 2-112 can be used as input data for prediction. Possible complications (outputs) are listed in columns 113-124. There are four possible time moments for complication prediction: based on the information known at

1\. the time of admission to hospital: all input columns (2-112) except 93, 94, 95, 100, 101, 102, 103, 104, 105 can be used for prediction;

2\. the end of the first day (24 hours after admission to the hospital): all input columns (2-112) except 94, 95, 101, 102, 104, 105 can be used for prediction;

3\. the end of the second day (48 hours after admission to the hospital) all input columns (2-112) except 95, 102, 105 can be used for prediction;

4\. the end of the third day (72 hours after admission to the hospital) all input columns (2-112) can be used for prediction.

## Analysis

### Feature Selection

For our Initial Analysis, we will exclude features where there is a lot of missing information. We will exclude the following :-

89\. Serum CPK content (KFK_BLOOD) (IU/L) - Which has 99.76% of Missing Values. Imputation makes little sense.

We will still include the Columns below because, imputation using Random Forest seems to work for them.

8\. Heredity on CHD (IBS_NASL) - 95.26% of Missing Values.

35\. Systolic blood pressure according to Emergency Cardiology Team (S_AD_KBRIG) (mmHg) - 63.29% of Missing Values

36\. Diastolic blood pressure according to Emergency Cardiology Team (D_AD_KBRIG) (mmHg) - 63.29% of Missing Values

**Extremely Unbalanced Predictors**

There are a few predictors which are extremely unbalanced where "Yes" values are lower than 1%of the total and in most cases are also lower than the "Missing" values.

**14**. Premature atrial contractions in the anamnesis (**nr_01**):

**No: Yes: Missing :: 1675: 4: 21**

**18**. Ventricular fibrillation in the anamnesis (**nr_07**):

**No: Yes: Missing :: 1678: 1: 21**

In it's current form, there is no way to cross check any prediction made based on the "Yes" value.

**19**. Ventricular paroxysmal tachycardia in the anamnesis (**nr_08**):

**No: Yes: Missing :: 1675: 4: 21**

In it's current form, there is very little possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**20**. First-degree AV block in the anamnesis (**np_01**):

**No: Yes: Missing :: 1680: 2: 18**

In it's current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**21**. Third-degree AV block in the anamnesis (**np_04**):

**No: Yes: Missing :: 1679: 3: 18**

In it's current form, there is very little possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**22**. LBBB (anterior branch) in the anamnesis (**np_05**):

**No: Yes: Missing :: 1671: 11: 18**

**23**. Incomplete LBBB in the anamnesis (**np_07**):

**No: Yes: Missing :: 1681: 1: 18**

In it's current form, there is no way to cross check any prediction made based on the "Yes" value.

**24**. Complete LBBB in the anamnesis (**np_08**):

**No: Yes: Missing :: 1676: 6: 18**

**25**. Incomplete RBBB in the anamnesis (**np_09**):

**No: Yes: Missing :: 1680: 2: 18**

In it's current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**26**. Complete RBBB in the anamnesis (**np_10**):

**No: Yes: Missing :: 1679: 3: 18**

In it's current form, there is very little possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**29**. Thyrotoxicosis in the anamnesis (**endocr_03**):

**No: Yes: Missing :: 1677: 13: 10**

**33**. Chronic pneumonia in the anamnesis (**zab_leg_04**):

**No: Yes: Missing :: 1684: 9: 7**

**34**. Pulmonary tuberculosis in the anamnesis (**zab_leg_06**):

**No: Yes: Missing :: 1684: 9: 7**

**42**. Paroxysms of supraventricular tachycardia at the time of admission to intensive care unit, (or at a pre-hospital stage) (**SVT_POST**):

**No: Yes: Missing :: 1680: 8: 12**

**43**. Paroxysms of ventricular tachycardia at the time of admission to intensive care unit, (or at a pre-hospital stage) (**GT_POST**):

**No: Yes: Missing :: 1680: 8: 12**

**44**. Ventricular fibrillation at the time of admission to intensive care unit, (or at a pre-hospital stage) (**FIB_G_POST**):

**No: Yes: Missing :: 1673: 15: 12**

**53**. ECG rhythm at the time of admission to hospital: idioventricular (**ritm_ecg_p_06**):

**No: Yes: Missing :: 1547: 1: 152**

In its current form, there is no way to cross check any prediction made based on the "Yes" value

**57**. Frequent premature atrial contractions on ECG at the time of admission to hospital (**n_r_ecg_p_02**):

**No: Yes: Missing :: 1577: 8: 115**

**62**. Paroxysms of supraventricular tachycardia on ECG at the time of admission to hospital (**n_r_ecg_p_08**):

**No: Yes: Missing :: 1581: 4: 115**

In its current form, there is very little possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**63**. Paroxysms of ventricular tachycardia on ECG at the time of admission to hospital (**n_r_ecg_p_09**):

**No: Yes: Missing :: 1583: 2: 115**

In its current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**64**. Ventricular fibrillation on ECG at the time of admission to hospital (**n_r_ecg_p_10**):

**No: Yes: Missing :: 1583: 2: 115**

\*Note: In its current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**65**. Sinoatrial block on ECG at the time of admission to hospital (**n_p_ecg_p_01**):

**No: Yes: Missing :: 1583: 2: 115**

\*Note: In its current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**67**. Type 1 Second-degree AV block (Mobitz I/ Wenckebach) on ECG at the time of admission to hospital (**n_p_ecg_p_04**):

**No: Yes: Missing :: 1580: 5: 115**

**68**. Type 2 Second-degree AV block (Mobitz II/Hay) on ECG at the time of admission to hospital (**n_p_ecg_p_05**):

**No: Yes: Missing :: 1583: 2: 115**

\*Note: In its current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**71**. LBBB (posterior branch) on ECG at the time of admission to hospital (**n_p_ecg_p_08**):

**No: Yes: Missing :: 1578: 7: 115**

**72**. Incomplete LBBB on ECG at the time of admission to hospital (**n_p_ecg_p_09**):

**No: Yes: Missing :: 1575: 10: 115**

**76**. Fibrinolytic therapy by Celiasum 750k IU (**fibr_ter_01**):

**No: Yes: Missing :: 1677: 13: 10**

**77**. Fibrinolytic therapy by Celiasum 1m IU (**fibr_ter_02**):

**No: Yes: Missing :: 1674: 16: 10**

**79**. Fibrinolytic therapy by Streptase (**fibr_ter_05**):

**No: Yes: Missing :: 1686: 4: 10**

\*Note: In its current form, there is very little possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

**80**. Fibrinolytic therapy by Celiasum 500k IU (**fibr_ter_06**):

**No: Yes: Missing :: 1681: 9: 10**

**81**. Fibrinolytic therapy by Celiasum 250k IU (**fibr_ter_07**):

**No: Yes: Missing :: 1684: 6: 10**

**82**. Fibrinolytic therapy by Streptodecase 1.5m IU (**fibr_ter_08**):

**No: Yes: Missing :: 1688: 2: 10**

\*Note: In its current form, there is no possibility of the "Yes" value appearing on the "Training", "Validation" and "Testing" sets.

We will track these features and zero them out if they affect the predictions negatively.

### Outcome Definition

We will only try to predict the Lethal Outcome(LET_IS) and not the other Complications. Otherwise we will need to partition our dataset for each Complication to ensure fair prediction. Predictions for each Complication will need to analysed separately and the predictions cannot be reassembled together into a common dataset easily.

For the Lethal Outcome (LET_IS), we will predict the outcome using 2 methods

1.  Binary Method:- A very simple method, where we only predict if the Patient survives or not. Lethal or Death events due to all causes are treated the same.
2.  Categorical Method:- Much more complex and error prone. Here we will try to predict the Lethal or Death event and the cause of death viz.
    -   1 - cardiogenic shock
    -   2 - pulmonary edema
    -   3 - myocardial rupture
    -   4 - progress of congestive heart failure
    -   5 - thromboembolism
    -   6 - asystole
    -   7 - ventricular fibrillation

### Dataset Preparation

**Imputation**

For Observations (Rows) where we have missing information, we will need a way to ensure that the Features (Columns) which have no missing information are still included in our predictions. In reality, we would need to have Subject Matter Expertise in CVD before we can impute any value because the Features are all related to each other and imputing values must be done taking these relationships into account.

However for the purposes of this Project, which is more academic in nature and whose results are not very relevant for the choice of treatment or outcome, we will use statistical methods to impute values.

**Though not a main topic for this report, Imputation is extremely important as inaccurately imputed values can wreak havoc and lead to inaccurate predictions causing more problems than they solve**

We will explore how to the accuracy of our predictions changes with and without Imputation. We will use Naive Bayes for exploratory analysis as it can easily work with missing values when compared to other Algorithms.

**Data download and variable set creation**

We will download the dataset from the University of Leicester's repository. As indicated in the Introduction section, the dataset is available in CSV format and can easily be imported into a data frame for further processing.

```{r Download MIC Dataset, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

##########################################################
# Begin Analysis of MIC Dataset
##########################################################

##########################################################
# Download the Raw Data from the respective repositories as the source for truth for the Datasets
##########################################################


# Myocardial infarction complications:
# https://archive.ics.uci.edu/dataset/579/myocardial+infarction+complications
# https://www.doi.org/10.25392/leicester.data.12045261.v3
# https://figshare.le.ac.uk/articles/dataset/Myocardial_infarction_complications_Database/12045261/3
# https://figshare.le.ac.uk/ndownloader/files/23581310


options(timeout = 120)

mic_data_file_csv <- "Myocardial infarction complications Database.csv"
if (!file.exists(mic_data_file_csv))
  download.file("https://figshare.le.ac.uk/ndownloader/files/23581310")

mic_data <- read.csv(mic_data_file_csv)


####### 
# Remove Variables used to hold filenames as they are not required anymore
rm(dl_mic, mic_data_file_csv)

```

We will create lists of Continuous, Ordinal (Ordered Factors), Partially Ordinal (Unordered Factors) and Nominal (Binary) features so that it is easier to process them later. Binary features are coded as Unordered Factors.

**Though using column numbers is easier for cross reference, using column numbers becomes very unwieldy soon enough.**

We will also create feature sets or groupings of features that we can use with the Keras Functional API. The feature set groupings are done with very rudimentary knowledge of the features based on their descriptions and the availability of time. In actual practice, we can greatly enhance the accuracy of the predictions with the services of a Subject Matter Expert (SME) in CVD who can help us group features that can be combined together to offer the best possible predictions.

The Reader is encouraged to experiment with the feature sets or work with an SME to build better sets that can offer more accurate predictions.

Let us have a preliminary look at the imported dataset. We will also record the survival rate of patients for the whole dataset which will serve as reference for evaluating the performance of our Algorithms

```{r Prepare MIC data for Analysis - create variable sets, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Structure of MIC data
print("================================================", quote=FALSE)
print("The structure of the MIC dataset as imported is")
print("================================================", quote = FALSE)
str(mic_data)
print("================================================", quote=FALSE)

# Compute Survival Rate which will act as the basic benchmark for all our Accuracy reports
survival_rate <- round(mean(mic_data$LET_IS == 0),digits = 4)

print("================================================", quote=FALSE)
print(c("The Survival Rate for patients affected by MIC on this datset is",survival_rate),quote = FALSE)
print("================================================", quote=FALSE)

# Create Lists of Continuous, Ordinal (Categorical) and Nominal (Binary) Features so that it is easier to process them later. Though using Column numbers is easier for cross reference,they become very unweildy soon enough. 


# Exclude ID from the list of Variables as it is not to be used for prediction

mic_continuous_variables <- c("AGE","S_AD_KBRIG","D_AD_KBRIG","S_AD_ORIT","D_AD_ORIT","K_BLOOD","NA_BLOOD", "ALT_BLOOD", "AST_BLOOD", "KFK_BLOOD","L_BLOOD", "ROE")

mic_ordinal_variables <- c("INF_ANAM","STENOK_AN","FK_STENOK","IBS_POST","GB","DLIT_AG","ant_im","lat_im","inf_im","post_im","TIME_B_S","R_AB_1_n","R_AB_2_n","R_AB_3_n","NA_R_1_n","NA_R_2_n","NA_R_3_n","NOT_NA_1_n","NOT_NA_2_n","NOT_NA_3_n" )

mic_part_ordinal_variables <- c("ZSN_A")

mic_nominal_variables <- c("SEX","IBS_NASL","SIM_GIPERT","nr_11","nr_01","nr_02","nr_03","nr_04","nr_07","nr_08","np_01","np_04","np_05","np_07","np_08","np_09","np_10", "endocr_01", "endocr_02", "endocr_03", "zab_leg_01", "zab_leg_02", "zab_leg_03", "zab_leg_04", "zab_leg_06", "O_L_POST", "K_SH_POST",     "MP_TP_POST", "SVT_POST", "GT_POST", "FIB_G_POST", "IM_PG_P", "ritm_ecg_p_01", "ritm_ecg_p_02", "ritm_ecg_p_04", "ritm_ecg_p_06", "ritm_ecg_p_07", "ritm_ecg_p_08", "n_r_ecg_p_01", "n_r_ecg_p_02",  "n_r_ecg_p_03", "n_r_ecg_p_04", "n_r_ecg_p_05", "n_r_ecg_p_06", "n_r_ecg_p_08", "n_r_ecg_p_09", "n_r_ecg_p_10", "n_p_ecg_p_01", "n_p_ecg_p_03", "n_p_ecg_p_04", "n_p_ecg_p_05", "n_p_ecg_p_06",  "n_p_ecg_p_07" , "n_p_ecg_p_08", "n_p_ecg_p_09", "n_p_ecg_p_10" , "n_p_ecg_p_11", "n_p_ecg_p_12",  "fibr_ter_01", "fibr_ter_02", "fibr_ter_03", "fibr_ter_05", "fibr_ter_06" , "fibr_ter_07", "fibr_ter_08", "GIPO_K", "GIPER_NA", "NA_KB", "NOT_NA_KB",  "LID_KB", "NITR_S", "LID_S_n", "B_BLOK_S_n", "ANT_CA_S_n", "GEPAR_S_n", "ASP_S_n", "TIKL_S_n", "TRENT_S_n")

# Exclude LET_IS from the list of complications similar to how we have excluded ID. We will handle it separately
mic_complications <- c("FIBR_PREDS", "PREDS_TAH", "JELUD_TAH", "FIBR_JELUD", "A_V_BLOK" , "OTEK_LANC" , "RAZRIV", "DRESSLER", "ZSN", "REC_IM", "P_IM_STEN")


######

# Create Feature sets for Functional API

mic_demographic_history_features <- c("AGE","SEX", "STENOK_AN","GB","SIM_GIPERT","DLIT_AG","IBS_NASL", "endocr_01", "endocr_02", "endocr_03","zab_leg_01", "zab_leg_02", "zab_leg_03", "zab_leg_04", "zab_leg_06", "O_L_POST" )


# Do not include np_09"
mic_infarction_features <- c("INF_ANAM","FK_STENOK", "IBS_POST", "IM_PG_P", "ZSN_A", "nr_11", "nr_01", "nr_02", "nr_03", "nr_04", "nr_07", "nr_08","K_SH_POST","MP_TP_POST","SVT_POST","GT_POST","FIB_G_POST", "np_01","np_04","np_05", "np_07", "np_08", "np_09", "np_10")

#Include  "TIME_B_S" here
mic_emergency_icu_features <- c("S_AD_KBRIG","D_AD_KBRIG","S_AD_ORIT","D_AD_ORIT", "O_L_POST","K_SH_POST","MP_TP_POST","SVT_POST","GT_POST","FIB_G_POST","TIME_B_S") 

mic_ecg_features <- c("ant_im","lat_im","inf_im","post_im","ritm_ecg_p_01","ritm_ecg_p_02","ritm_ecg_p_04","ritm_ecg_p_07","ritm_ecg_p_08","n_r_ecg_p_01","n_r_ecg_p_02","n_r_ecg_p_03","n_r_ecg_p_04","n_r_ecg_p_05","n_r_ecg_p_06", "n_p_ecg_p_03", "n_p_ecg_p_06","n_p_ecg_p_07","n_p_ecg_p_08","n_p_ecg_p_09","n_p_ecg_p_10","n_p_ecg_p_11","n_p_ecg_p_12")

mic_ft_features <- c("fibr_ter_01","fibr_ter_02","fibr_ter_03","fibr_ter_05","fibr_ter_06","fibr_ter_07","fibr_ter_08")

# Do not include "KFK_BLOOD"
mic_serum_features <- c("GIPO_K","K_BLOOD","GIPER_Na","Na_BLOOD","ALT_BLOOD","AST_BLOOD","L_BLOOD","ROE")

mic_relapse_features <- c("R_AB_1_n","R_AB_2_n","R_AB_3_n")

mic_medicine_features <- c("NA_KB","NOT_NA_KB","LID_KB","NITR_S","NA_R_1_n","NA_R_2_n","NA_R_3_n","NOT_NA_1_n","NOT_NA_2_n","NOT_NA_3_n","LID_S_n","B_BLOK_S_n","ANT_CA_S_n","GEPAR_S_n","ASP_S_n","TIKL_S_n","TRENT_S_n")


```

We will create Training, Testing and CV partitions of the dataset that can be used later by different algorithms with due modifications in the variable types. We will use the dataset with the original values for LET_IS for creating these partitions. We will also set KFK_BLOOD=0 for all observations.

```{r Prepare MIC data - Create Training, Testing and CV Partitions, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# We will modify the mic_data dataset
# We will configure all Ordinal and Binary Variables as Factors.
# Continuous variables are already set as integers or numeric(float) during the import operation.
# We will also set KFK_BLOOD = 0 for all observations. 
# We will retain LET_IS as it is. Do not convert to Factors as we will lose the integer values. Look at note for partition creation

mic_data_orig <- mic_data %>% 
    mutate_at(c(mic_nominal_variables, mic_complications), ~as.factor(.)) %>%
    mutate_at(c(mic_ordinal_variables), ~as.ordered(.)) %>%
    mutate_at(c(mic_part_ordinal_variables), ~as.factor(.)) %>%    
    mutate_at(c("KFK_BLOOD"), ~(. = 0)) 



# Structure of MIC data after modification of variable types
print("================================================", quote = FALSE)
print("The structure of the MIC dataset after modification of variable types and before imputation is: ", quote = FALSE)
print("================================================", quote = FALSE)
str(mic_data_orig)
print("================================================", quote = FALSE)

# Split into Training and Testing Sets. For partition creation treat LET_IS as vector of factors

set.seed(1024)

mic_test_index <- createDataPartition(y = as.factor(mic_data_orig$LET_IS), times = 1, p = 0.2, list = FALSE)
mic_orig_train <- mic_data_orig[-mic_test_index,]
mic_orig_test <- mic_data_orig[mic_test_index,]


# Create Datasets for Cross Validation, For partition creation treat LET_IS as vector of factors
# Retain Indices for the rest of the analysis as they provide consistency in partitioning
set.seed(1024)

mic_test_index_cv <- createDataPartition(y = as.factor(mic_orig_train$LET_IS), times = 1, p = 0.2, list = FALSE)
mic_orig_cv_train_set <- mic_orig_train[-mic_test_index_cv,]
mic_orig_cv_test_set <- mic_orig_train[mic_test_index_cv,]



```

\newpage

**For imputation of missing values, we will use the Multivariate Imputation by Chained Equations (mice) package. Within mice, we will use Random Forest as the imputation algorithm. The choice of Random Forest is based on experimentation and observation of the resulting imputations.**

**We will also compare the distributions before and after imputation to be sure that the imputation is not causing large changes in the shape of the distributions.**

To inform mice about our variables, we will modify our Nominal(Binary) and Ordinal(Categorical) variables as factors. Continuous variables are already modified as Integers or Numericals during the import process.

We will retain the outcome LET_IS as it is. Conversion to factors will cause us to lose its integer values.

We will also reset KFK_BLOOD=0 for all observations to avoid having to redo it for every algorithm.

**For Imputation, we will make an assumption that we will have full access to all the Features/Predictors for all data including the "Testing" or "Holdout" set and that the Features/Predictors for the Training and Holdout sets can take on similar values and they can be combined together if required for imputation. The practical aspect of such a scheme is that when we need to analyse data that has not been seen before, we can combine the unseen or unknown data with already known data to impute more plausible values. This is true in practice and hence the assumption is fair indeed.**

**As a consequence of this assumption, we will perform the imputation of missing values for the whole MIC dataset at one go.**

We can partition the dataset into Training, Testing, Training CV and Testing CV sets and perform the imputation individually for each set, However imputation performed for the partitioned datasets only has access to a smaller set of values and imputation fails for several features. This is particularly true for Testing and Testing CV partitions. We can recall that the MIC dataset has only 1700 observations overall for a very complex set of features. Based on our 80:20 partitioning scheme, this leads to 343 observations for the Testing set and 275 observations for the Testing CV set and imputation does not work well for such small numbers of observations.

\*Note: I have used 75 iterations for the imputation, using 50 causes distortion in the shape of some features, using 95 takes incrementally more time and for some strange reason causes degradation in the accuracy.

```{r Prepare MIC data - Perform Imputation , include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Imputation is CPU intensive, single threaded and takes time. Retrieve from disk if an imputed dataset is available. Else impute and save the dataset to disk for quick retrieval next time. 

################ Perform Imputation for Whole MIC dataSet ########################

# Use only Features excluding ID for imputation. 

mic_orig_imputed_rdata_file <- "mic_orig_imputed_mice.RData"

if(file.exists(mic_orig_imputed_rdata_file)){
  load(mic_orig_imputed_rdata_file)
  rm(mic_orig_imputed_rdata_file)
} else{
  mic_orig_imputed_mice <- complete(mice(mic_data_orig[,2:112], method = "rf", m=95, seed = 1024, printFlag = FALSE))
  save(mic_orig_imputed_mice, file = mic_orig_imputed_rdata_file)
  rm(mic_orig_imputed_rdata_file)
}

# Recombine Features/Predictors with unique key (ID), complications and outcome ("LET_IS)

# Broken up deliberately for ease of understanding
mic_orig_imputed_mice <- cbind(mic_data_orig[,1], mic_orig_imputed_mice, mic_data_orig[,113:123], mic_data_orig[,124] )

# ID and LET_IS specified explicitly, List of complications excluding LET_IS was created already
colnames(mic_orig_imputed_mice) = c("ID", colnames(mic_orig_imputed_mice[,2:112]), mic_complications, "LET_IS")

############## Split MIC dataset with imputed values using the same partion scheme ##############

# Split into Training and Testing Sets

mic_orig_train_imputed_mice <- mic_orig_imputed_mice[-mic_test_index,]
mic_orig_test_imputed_mice <- mic_orig_imputed_mice[mic_test_index,]


# Create Datasets for Cross Validation 

mic_orig_cv_train_imputed_mice <- mic_orig_train_imputed_mice[-mic_test_index_cv,]
mic_orig_cv_test_imputed_mice <- mic_orig_train_imputed_mice[mic_test_index_cv,]




```

\newpage

**MIC "Training" dataset - distribution of continuous variables before and after imputation**

Let us visualise the distributions of our continuous variables before and after imputation.

At the end we will also include IBS_NASL which had 95.76% missing values.

```{r Visualise MIC continuous variables before and after imputation using the mice Package, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Compare Original and Imputed values of Continuous Distributions to check if there is any major distortion

grid.arrange (histogram(mic_orig_train[,"AGE"], type="count", xlab = "AGE", ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"AGE"], type="count", xlab = "AGE", ylab = "after imputation"),	histogram(mic_orig_train[,"S_AD_KBRIG"], type="count", xlab = "S_AD_KBRIG",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"S_AD_KBRIG"], type="count", xlab="S_AD_KBRIG", ylab = "after imputation"), ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 1" )


grid.arrange (histogram(mic_orig_train[,"D_AD_KBRIG"], type="count", xlab = "D_AD_KBRIG",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"D_AD_KBRIG"], type="count", xlab="D_AD_KBRIG", ylab = "after imputation"), histogram(mic_orig_train[,"S_AD_ORIT"], type="count", xlab = "S_AD_ORIT",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"S_AD_ORIT"], type="count", xlab="S_AD_ORIT", ylab = "after imputation"),	ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 2")


grid.arrange (histogram(mic_orig_train[,"D_AD_ORIT"], type="count", xlab = "D_AD_ORIT",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"D_AD_ORIT"], type="count", xlab="D_AD_ORIT", ylab = "after imputation"),	histogram(mic_orig_train[,"K_BLOOD"], type="count", xlab = "K_BLOOD",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"K_BLOOD"], type="count", xlab="K_BLOOD", ylab = "after imputation"), ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 3"	)


grid.arrange (histogram(mic_orig_train[,"NA_BLOOD"], type="count", xlab = "NA_BLOOD",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"NA_BLOOD"], type="count", xlab="NA_BLOOD", ylab = "after imputation"), histogram(mic_orig_train[,"ALT_BLOOD"], type="count", xlab = "ALT_BLOOD",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"ALT_BLOOD"], type="count", xlab="ALT_BLOOD", ylab = "after imputation"),	ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 4"	)



grid.arrange (histogram(mic_orig_train[,"AST_BLOOD"], type="count", xlab = "AST_BLOOD",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"AST_BLOOD"], type="count", xlab="AST_BLOOD", ylab = "after imputation"),	histogram(mic_orig_train[,"L_BLOOD"], type="count", xlab = "L_BLOOD",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[,"L_BLOOD"], type="count", xlab="L_BLOOD", ylab = "after imputation"), ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 5"	)


grid.arrange (histogram(mic_orig_train[, "ROE"], type="count", xlab = "ROE",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[, "ROE"], type="count", xlab="ROE", ylab = "after imputation"),	histogram(mic_orig_train[, "IBS_NASL"], type="count", xlab = "IBS_NASL",ylab = "before imputation"), histogram(mic_orig_train_imputed_mice[, "IBS_NASL"], type="count", xlab="IBS_NASL", ylab = "after imputation"), ncol =2, nrow =2, name = "MIC - Histogram of distributions before and after imputation - 6"	)

```

We can see that the MICE package and the RF Algorithm for imputation have done a very good job in preserving the distribution of the continuous variables and the distributions do not look too distorted from the original.

IBS_NASL appears a bit distorted. D_AD_KBRIG has very minor distortion. We will go ahead with both accepting the risk.

\newpage

### Single Category for Deaths

We will try to predict the outcome using the much simpler Binary Method first.

```{r Prepare MIC data - Modify LET_IS for binary outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


# Change LET_IS to have a Single Category for Deaths rather than multiple 

mic_modified_cv_train_set <- mic_orig_cv_train_set %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0)) 

mic_modified_cv_test_set <- mic_orig_cv_test_set %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0))


```

We use Naive Bayes for performing some exploratory analysis of where we can get.

#### Naive Bayes without Imputation - Binary Outcome

First up, we will do it without any Imputation. As a reminder, an outcome of 1 represents a lethal outcome and an outcome of 0 represents survival.

For Tuning Naive Bayes, readers can refer back to the Naive Bayes section in the Heart Failure Prediction chapter.

we will configure

-   S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson

-   AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian

-   Again, KFK_BLOOD = 0 for all observations.

```{r Check Naive Bayes with Original MIC Dataset without imputation - 2, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# We will configure 
# S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson
# AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian
# KFK_BLOOD = 0 is set already for all observations

# Revert S_AD_KBRIG, S_AD_ORIT, NA_BLOOD as integers

mic_modified_cv_train_set <- mic_modified_cv_train_set %>% 
            mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.)))  %>% 
            mutate(LET_IS = as.factor(LET_IS))

mic_modified_cv_test_set <- mic_modified_cv_test_set %>% 
            mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.))) %>% 
            mutate(LET_IS = as.factor(LET_IS))


###########
# Fit Naive Bayes
fit_nb_native_LET_IS <- naive_bayes(x = mic_modified_cv_train_set[,2:112], y = mic_modified_cv_train_set$LET_IS, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
summary(fit_nb_native_LET_IS)

# Prepare Predictions
pred_nb_test_native_LET_IS <-predict(object = fit_nb_native_LET_IS, newdata = mic_modified_cv_test_set[,2:112])

# Print table of predictions
print("=====================================", quote = FALSE)
print("The table of predictions is: ", quote = FALSE)
table(as.factor(pred_nb_test_native_LET_IS))
print("=====================================", quote = FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is: ", quote = FALSE)
table(mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)

# Print overall accuracy
print("The accuracy of predictions is: ", quote = FALSE)
mean(pred_nb_test_native_LET_IS == mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)

# Print confusion matrix
print("The confusion matrix is: ")
print("=====================================", quote = FALSE)
confusionMatrix(data = pred_nb_test_native_LET_IS, reference = mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)

# Remove Data that is no longer required

rm(fit_nb_native_LET_IS, pred_nb_test_native_LET_IS, cm)

rm(mic_modified_cv_train_set, mic_modified_cv_test_set)
```

We are able to detect 28 deaths, while missing 18 and flagging 24 live cases as deaths.

The Balanced Accuracy with Naive Bayes is decent. The Sensitivity is good. Specificity is quite poor comparatively.

Though our Overall Accuracy looks like it is very good, we can see that it is not very far from the "no information rate". We can also recall that the survival rate is around 0.84 for the whole dataset.

\newpage

#### Naive Bayes with Imputation - Binary Outcome

We will redo our analysis with imputation of missing values.

We will configure

-   S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson

-   AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian

-   As usual, KFK_BLOOD = 0 for all observations

Let us now check what Naive Bayes with imputation can do.

```{r Prepare Predictions for RF imputed MIC Dataset with Naive Bayes - 1, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Modify Datasets for Cross Validation using binary outcomes 

mic_modified_cv_train_set <- mic_orig_cv_train_imputed_mice %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0))

mic_modified_cv_test_set <- mic_orig_cv_test_imputed_mice %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0))

# We will configure
# S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson
# AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian
# KFK_BLOOD = 0 is set already for all observations

# Configure S_AD_KBRIG, S_AD_ORIT, NA_BLOOD as integers

mic_modified_cv_train_set <- mic_modified_cv_train_set %>% 
            mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.)))  %>% 
            mutate(LET_IS = as.factor(LET_IS))

mic_modified_cv_test_set <- mic_modified_cv_test_set %>% 
            mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.))) %>% 
            mutate(LET_IS = as.factor(LET_IS))


###########
# Fit Naive Bayes
fit_nb_native_LET_IS <- naive_bayes(x = mic_modified_cv_train_set[,c(2:112)], y = mic_modified_cv_train_set$LET_IS, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
summary(fit_nb_native_LET_IS)

# Prepare Predictions
pred_nb_test_native_LET_IS <-predict(object = fit_nb_native_LET_IS, newdata = mic_modified_cv_test_set)

# Print table of predictions
print("=====================================", quote = FALSE)
print("The table of predictions is", quote = FALSE)
table(as.factor(pred_nb_test_native_LET_IS))
print("=====================================", quote = FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is", quote = FALSE)
table(mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)

# Print overall accuracy
print("The accuracy of predictions is", quote = FALSE)
mean(pred_nb_test_native_LET_IS == mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)

# Print confusion matrix
print("The confusion matrix is")
print("=====================================", quote = FALSE)
confusionMatrix(data = pred_nb_test_native_LET_IS, reference = mic_modified_cv_test_set$LET_IS)
print("=====================================", quote = FALSE)


# Remove Data that is no longer required

rm(fit_nb_native_LET_IS, pred_nb_test_native_LET_IS)


```

Imputation seems to have a slightly negative effect on both the Overall Accuracy and the Balanced Accuracy. There is a small reduction in the Sensitivity.

We are now able to detect 28 deaths while missing 18 and flagging 27 lives cases as deaths.

\newpage

#### Random Forest with Imputation - Binary Outcome

Random Forest cannot easily manage missing values and generates errors. Imputation is necessary for us to run the Random Forest algorithm on the dataset. Random Forest is also very sensitive to "noisy" or "irrelevant" data and can generate erroneous predictions which are far away from the target.

Similar to what we did for the HFP dataset, we will use the inbuilt function within the "randomForest" library to tune Random Forest to choose the right "$m_{try}$" values. We will also use the "matrix notation" instead of the "formula notation" as it is easier to program and quicker in terms of performance.

```{r Prepare Predictions for RF imputed MIC Dataset with Random Forest, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width= "50%"}

# Modify Datasets for Cross Validation using binary outcomes 

mic_modified_cv_train_set_rf <- mic_orig_cv_train_imputed_mice %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0))

mic_modified_cv_test_set_rf <- mic_orig_cv_test_imputed_mice %>% 
      mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0))


# Ensure NA Values are removed as Random Forest generates errors otherwise
# Use na.omit() to remove reamining values if any

mic_modified_cv_train_set_rf <- mic_modified_cv_train_set_rf %>% 
        mutate(LET_IS = as.factor(LET_IS))


mic_modified_cv_test_set_rf <- mic_modified_cv_test_set_rf %>% 
        mutate(LET_IS = as.factor(LET_IS))

# Use the inbuilt tuneRF functionality to find the best mtry value to reduce additional computation

# Use Matrix Notation rather than Formula Notation as it is a lot more efficient

print("=====================================",quote = FALSE)
print("The Random Forest mtry values and error rates are",quote = FALSE) 

set.seed(1024)

best_mtry_rf_tune <- tuneRF(x = mic_modified_cv_train_set_rf[,c(2:112)], y = mic_modified_cv_train_set_rf[,124], stepFactor = 0.5, improve = 0.000001,trace = TRUE, plot = TRUE, doBest = TRUE)

# Prepare predictions
pred_rf <- predict(best_mtry_rf_tune, newdata = mic_modified_cv_test_set_rf[,c(2:112)], type = "response")

# Print summary about Random Forest. Only print what is necessary and easy to comprehend
print("=====================================",quote = FALSE)
print("The details for Random Forest are",quote = FALSE)
print(c("Prediction Type :",best_mtry_rf_tune$type),quote = FALSE)
print(c("Number of Trees (ntree) :",best_mtry_rf_tune$ntree),quote = FALSE)
print(c("mtry value :",best_mtry_rf_tune$mtry),quote = FALSE)
print("=====================================",quote = FALSE)

# Print table of predictions
print("=====================================",quote = FALSE)
print("The table of predictions is")
table(as.factor(pred_rf))
print("=====================================",quote = FALSE)

# Print table of actual values
print("=====================================",quote = FALSE)
print("The table of actual values is")
table (mic_modified_cv_test_set_rf[,124])
print("=====================================",quote = FALSE)

# Print overall accuracy
print("The accuracy of predictions is")
mean(pred_rf == mic_modified_cv_test_set_rf[,124])
print("=====================================",quote = FALSE)

# Print confusion matrix
print("The confusion matrix is")
print("=====================================",quote = FALSE)
confusionMatrix(data = pred_rf,reference = mic_modified_cv_test_set_rf[,124])
print("=====================================",quote = FALSE)

# Extract and Sort Random Forest Variable importance in order of decreasing importance and print 10 most important values

imp <- as.data.frame(randomForest::importance(best_mtry_rf_tune))
imp <- data.frame(Importance = imp$MeanDecreaseGini,
           names   = rownames(imp))
imp <- imp[order(imp$Importance, decreasing = TRUE),]

print("The first 20 Features in order of decreasing importance in prediction are:",quote = FALSE)
print("=============================================",quote = FALSE)
knitr::kable(x = imp[1:20,], col.names = c("Col Id", "Importance", "Names"), caption = "MIC Prediction - Variable Importance (MeanDecreaseGini)")
print("=============================================",quote = FALSE)

# Remove Data that is no longer required

rm(best_mtry_rf_tune, pred_rf, imp)

```

Though our overall accuracy with Random Forest looks good, The balanced accuracy is quite poor and the number of deaths detected accurately is quite low.

The Sensitivity is extremely good and the Specificity is just as extremely poor.

We can also see the 20 most important Features that Random Forest has used in making its predictions. These provide some good insight into the relative importance of each Feature/Predictor as evaluated by Random Forest.

We can detect 11 deaths while missing 35 and flagging 1 live case inaccurately as a death.

Random Forest seems to generate poor predictions for death events (LET_IS = 1). It is not very useful in the context of our MIC dataset and we will not explore it further.

```{r MIC Initial Analysis Naive Bayes & RF Cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required and run the garbage collector
rm( mic_modified_cv_train_set, mic_modified_cv_test_set)

rm(mic_modified_cv_train_set_rf,mic_modified_cv_test_set_rf)

gc()

```

\newpage

#### XGBoost

We will look at XGBoost next for Binary Outcome Prediction.

For XGBoost, we will need to use the original MIC data without modifying categorical and binary variables as factors.

As stated earlier in the Additional Notes and Cautions section of the Introduction chapter, Though support for "categoricals" is available in R through factors from XGBoost version 3.0 onwards, the feature is considered experimental. According to the developers, only the Python package is fully supported.

<https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html>

In this project, I have only used Integer and Numerical values as inputs to XGBoost.

Though this seems incorrect at first glance, a detailed reading of the descriptions of the MIC dataset makes it clear that most of the "Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding". The only partially ordered attribute is "ZSN_A". Even this is not completely unordered.

XGBoost provides a huge number of tuning parameters. You can read more about the tuning parameters for XGBoost using the help function available within R or at

<https://xgboost.readthedocs.io/en/stable/parameter.html>

An extract is provided for immediate reference about the parameters used in our model.

------------------------------------------------------------------------

-   `verbose` [default=1]

    -   Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).If 0, xgboost will stay silent. If 1, it will print information about performance. If 2, some additional information will be printed out.

-   `nthread` [default to maximum number of threads available if not set]

    -   Number of parallel threads used to run XGBoost. When choosing it, please keep thread contention and hyperthreading in mind

-   `nrounds`

    -   max number of boosting iterations.

-   `eta` [alias: `learning_rate`]

    -   Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.

    -   range: [0,1]

    -   default value: 0.3 for tree-based boosters, 0.5 for linear booster.

-   `max_depth` [default=6, type=int32]

    -   Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. `exact` tree method requires non-zero value.

    -   range: [0,∞]

-   `objective`

    -   `binary:logistic`: logistic regression for binary classification, output probability

-   `eval_metric` [default according to objective]

    -   `error`: Binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.

-   `seed`

    -   Random number seed. If not specified, will take a random seed through R's own RNG engine.

------------------------------------------------------------------------

For tuning XGBoost, we will set max_depth = 10, eta = 0.15 and nrounds = 200. I have set nthread = 8, the readers and evaluators can change the number of threads to what is convenient to them.

```{r Perform initial analysis of MIC dataset using XGBoost for binary outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Original Dataset without Imputation

# Modify LET_IS to have a Single Category for Deaths rather than multiple> Use original dataset which has the integer values

mic_data_xgboost <- mic_data %>% mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0)))

# SINCE VARIABLES ARE ORDINAL, WE USE THEM AS-IS TO INDICATE THEIR VALUES. NO ONE-HOT CODING IS NECESSARY
# XGBOOST FOR R ONLY SUPPORTS FACTORS EXPERIMENTALLY. NOT USED HERE TO AVOID COMPLEXITY

# DO NOT CONVERT CATEGORICAL(ORDINAL) AND BINARY(NOMINAL) VARIABLES TO FACTORS
# CONVERSION USING as.matrix() CAUSES THEM TO BE COERCED TO CHARACTER VECTORS


mic_data_xgboost <- mic_data_xgboost %>% 
    mutate_at(mic_continuous_variables, ~as.numeric(.))

# Partition into Training and Testing Sets. Use index created earlier, Create only Training set. Testing is not required for now

mic_modified_train_xgboost <- mic_data_xgboost[-mic_test_index,]

######################

# Create Datasets for Cross Validation. 


mic_modified_cv_train_set_xgboost <- mic_modified_train_xgboost[-mic_test_index_cv,]
mic_modified_cv_test_set_xgboost <- mic_modified_train_xgboost[mic_test_index_cv,]

#######################

# Create a matrix that can be used by XGBoost for Training
dtrain <- xgb.DMatrix(data = as.matrix(mic_modified_cv_train_set_xgboost[,c(2:112)]), label= mic_modified_cv_train_set_xgboost$LET_IS, nthread = 8)

# Fit XGBoost
fit_xgboost_mic <- xgb.train(
    data = dtrain,
    seed = 1024,
    booster	= "gbtree",
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 200,
    objective = "binary:logistic",
    verbose = 0, # set verbose=1 during development, tuning and testing
)

# Print Summary of Parameters

print("=====================================", quote = FALSE)
print("The details for XGBoost are")
fit_xgboost_mic
print("=====================================", quote = FALSE)

# Generate probabilities for predictions
pred_xgboost_mic <-predict(object = fit_xgboost_mic, newdata = as.matrix(mic_modified_cv_test_set_xgboost[,c(2:112)]))


#######################


# Convert probabilities to binary values 0 and 1.
pred_xgboost_mic_binary <- as.numeric(pred_xgboost_mic > 0.5)

# Print table of predictions
print("=====================================", quote = FALSE)
print("The table of predictions is: ", quote = FALSE)
table(as.factor(pred_xgboost_mic_binary ))
print("=====================================", quote = FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is: ", quote = FALSE)
table(mic_modified_cv_test_set_xgboost$LET_IS)
print("=====================================", quote = FALSE)

# Print table of overall accuracy
print("The accuracy of predictions is", quote = FALSE)
mean(pred_xgboost_mic_binary == mic_modified_cv_test_set_xgboost$LET_IS)

# Print table of confusion matrix
print("The confusion matrix is", quote = FALSE)
print("=====================================", quote = FALSE)
confusionMatrix(data = as.factor(pred_xgboost_mic_binary), reference = as.factor(mic_modified_cv_test_set_xgboost$LET_IS))
print("=====================================", quote = FALSE)


rm(fit_xgboost_mic, pred_xgboost_mic ,pred_xgboost_mic_binary, dtrain)


```

We can detect 26 deaths while missing 20 and zero live cases flagged as a deaths.

XGBoost provides very good overall accuracy and balanced accuracy which are better that what Naive Bayes provides. We have perfect sensitivity but specificity is quite poor.

The number of deaths that XGBoost is able to predict accurately is also quite good. Where XGBoost really shines bright is in its the ability to avoid prediction of death events as live events. This helps increase both overall accuracy and balanced accuracy.

**\*Note: The Accuracy of XGBoost reduces quite drastically when one-hot encoding of categorical variables is used for some reason.**

XGBoost is also very robust to the presence of NA values in the observations and can ignore them while using the other predictors. It is also extremely quick. Needless to say, it an excellent choice when quick results are desired.

```{r MIC XGBoost Binary Outcome - CV Set Cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required and run the garbage collector 
rm(mic_modified_cv_test_set_xgboost, mic_modified_cv_train_set_xgboost, mic_modified_train_xgboost, mic_data_xgboost)

gc()

```

\newpage

#### Artificial Neural Networks

Let us see if ANN can make our predictions any better than Naive Bayes or XGBoost. Since ANN do not work very well with missing values, we will need to reduce the number of missing values by

1.  Imputation

2.  Making some educated choices for all observations for a particular feature.

3.  Removing observations that cannot be filled in using (1) and (2) as above.

We start exploring ANN by encoding the categorical variables using "Dummy" variables. We will use the dummyVars function from the caret Package as it can support ordinals (ordered factors).

As earlier, we first explore only binary outcomes for LET_IS. 0 indicates survival and 1 indicates a death or lethal event.

```{r Prepare MIC Dataset for expansion of Categorical Variables and binary outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# As previously we will use only a Single Category to classify all Deaths

# Let us use the imputed dataset as ANN cannot work with missing values 

# The code for ANN is sligthly more complex than for the other cases. Please follow comments carefully. More than one reading may be required to comprehend the flow

########

# Convert nominal variables from factors to integers. Since they are binary, changing them from factors to numerics does not alter their behaviour 

mic_modified_cv_train_set_ann <- mic_orig_cv_train_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 

mic_modified_cv_test_set_ann <- mic_orig_cv_test_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 


# Expand Imputed values using Dummy Variables.

##########

# Convert LET_IS to binary values

mic_modified_cv_train_set_ann  <- mic_modified_cv_train_set_ann  %>% 
        mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0))) 

mic_modified_cv_test_set_ann  <- mic_modified_cv_test_set_ann  %>% 
        mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0))) 

# Retain "LET_IS" column as it will be removed during conversion

mic_cv_train_let_is_bin <- mic_modified_cv_train_set_ann$LET_IS

mic_cv_test_let_is_bin <- mic_modified_cv_test_set_ann$LET_IS

########## ONE-HOT ENCODING USING CARET #################################

# Caret is used for one-hot encoding as it supports ordinal variables (ordered factors) 

# Use formula interface to create template for dummy vars. Create dummy vars for Ordinal and Partially Ordinal variables
dumms_vars_cv_template <- dummyVars(formula = "~.", data = mic_modified_cv_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)], sep = "_", levelsOnly = FALSE, fullRank = FALSE)

# Create dummy vars for Training CV and Testing CV datasets. 
mic_modified_cv_train_set_ann_dummy <- as.data.frame(predict(dumms_vars_cv_template, mic_modified_cv_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_cv_test_set_ann_dummy <- as.data.frame(predict(dumms_vars_cv_template, mic_modified_cv_test_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

# Collect Training CV dummy vars names. Same applies for Testing CV  
dummy_vars_names <- colnames(mic_modified_cv_train_set_ann_dummy)

# Modify Training CV and Testing CV Datasets to remove the existing columns for variables related to  "ID", Ordinal,  Partially Ordinal and Complications as they are not used in Predictions anymore.
# Remove "LET_IS" to ensure that the same logic works for both binary and categorical predictions of the outcome

mic_modified_cv_train_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

mic_modified_cv_test_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

# Collect Modified Training CV Column names. Same applies for Testing CV
mic_modified_cv_col_names <- colnames(mic_modified_cv_train_set_ann)

# Combine Collected Modified Training CV column names with Training CV dummy vars names
mic_modified_cv_col_names <- c(mic_modified_cv_col_names, dummy_vars_names)

# Create combined Training CV and Testing CV datasets by binding together the Modified CV Training and Testing datasets with their respective dummy vars daatasets
mic_modified_cv_train_set_ann <- cbind(mic_modified_cv_train_set_ann, mic_modified_cv_train_set_ann_dummy)

mic_modified_cv_test_set_ann <- cbind(mic_modified_cv_test_set_ann, mic_modified_cv_test_set_ann_dummy)

# Assign column names to the newly created combined Training CV and Testing CV datasets
colnames(mic_modified_cv_train_set_ann) <- mic_modified_cv_col_names

colnames(mic_modified_cv_test_set_ann) <- mic_modified_cv_col_names

# Remove data that is not required anymore

rm(dumms_vars_cv_template, mic_modified_cv_train_set_ann_dummy, mic_modified_cv_test_set_ann_dummy, mic_modified_cv_col_names)
```

\newpage

##### Sequential API

Let us try ANN first with Sequential API

```{r Perform Initial Analysis for MIC Dataset using Neural Networks after expansion using Sequential API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width="75%", fig.align='center'}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############

feature_names <- c(mic_nominal_variables, mic_continuous_variables, dummy_vars_names)

train_features <- as.matrix(mic_modified_cv_train_set_ann[feature_names])
train_targets <- as.matrix(mic_cv_train_let_is_bin)


val_features <- as.matrix(mic_modified_cv_test_set_ann[feature_names])
val_targets <- as.matrix(mic_cv_test_let_is_bin)

####################################
# We need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
# Remove Columns from the list that produce NA when scaled

####### Normal Scaling ############
feature_names <- colnames(train_features)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

# Let us build the Nueral Network 


model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |> 
  layer_dense(units = 256, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 256, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 192, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 192, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(1, activation = "sigmoid")


# Let us print the model for visualisation. 
# Commented out for Report Creation
#print("The Keras Sequential API model is",quote = FALSE)
#print("=============================================",quote = FALSE)
#model
#print("=============================================",quote = FALSE)

# Collect counts for configuration of initial weights
counts <- table(mic_cv_train_let_is_bin) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_cv_test_let_is_bin) # Counts for Validation Set

# Setup weights. Weights are modified manually. Changes are not updated automatically 
weight_for_0 = as.numeric(1 / counts["0"]) 
weight_for_1 = as.numeric(1 / counts["1"]) *0.98

# Train the Model 

metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-3),
  loss = "binary_crossentropy",
  metrics = metrics
)
# Add callbacks if required

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)



plot_sequential_api_model <- model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  verbose = 0 # Set verbose = 2 during development and tuning
)

# Let us plot the metrics 
print("Sequential API - Metrics and Trends during Training", quote = FALSE)
print("=============================================", quote = FALSE)
plot(plot_sequential_api_model,smooth = TRUE)
print("=============================================", quote = FALSE)

# Prepare Predictions

val_pred <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }

pred_correct <- mic_cv_test_let_is_bin == val_pred

# Print table of predicted values
print("================================", quote = FALSE)
print("The Table of Predicted values is", quote = FALSE)
table(as.numeric(!pred_correct))
print("================================", quote = FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is")
table(as.factor(mic_cv_test_let_is_bin))
print("=====================================", quote = FALSE)

# Print overall accuracy. Method is slightly different from previous Algorithms 
print("================================", quote = FALSE)
print(c("Validation accuracy is : ", round(mean(pred_correct), digits = 4)))
print("================================", quote = FALSE)

# Collect death events
deaths <-mic_cv_test_let_is_bin == 1

# Prepare and print summary for death events
n_deaths_detected <- sum(deaths & pred_correct)
n_deaths_missed <- sum(deaths & !pred_correct)
n_live_flagged <- sum(!deaths & !pred_correct)

print(c("deaths detected",n_deaths_detected))
print(c("deaths missed",n_deaths_missed))
print(c("survival cases flagged as deaths",n_live_flagged))

# Print confusion matrix
print("================================", quote = FALSE)
print("The Confusion Matrix is")
confusionMatrix(data = as.factor(val_pred), reference = as.factor(mic_cv_test_let_is_bin))
print("================================", quote = FALSE)

# Remove data that is no longer required

rm(class_weight, metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct, val_pred, weight_for_0, weight_for_1, plot_sequential_api_model)

rm(index_features_scaling, feature_names_for_scaling)


```

We can now detect 31 deaths while missing 15 and flagging 25 live cases as deaths.

With ANN built using Keras sequential API, we are able to better our predictions for deaths. Though the number of live events flagged as deaths is higher than XGBoost or Naive Bayes, our balanced accuracy has also improved. We are seeing some incremental improvements in the Specificity.

We can further tune the ANN to achieve much higher overall and balanced accuracies, but the flip side is that the ANN perform very poorly when run against other validation or test sets. Just like over-fitting or over-training, we suffer from over-optimising the ANN to the validation set.

The dataset is also very complex and finding holdout and validation subsets that are truly reflective of the whole dataset is a real challenge. Creation of the holdout and validation sets with different seeds and running the ANN against them causes variations in the predictions. So we need to judiciously choose ANN whose predictions do not vary wildly between different sets.

\newpage

##### Functional API

We will try ANN with Functional API next. Functional API are extremely versatile and flexible when compared to Sequential API. Functional API allow us to build hierarchies of layers and group features together to provide very good predictions. As indicated earlier, such grouping would require the services of an SME in CVD who can advise on the features that work best when grouped together and can also advice on the hierarchy to be built to get better predictions. We will build something that is quite basic to demonstrate the utility of Functional API.

```{r Prepare MIC Dataset for expansion of Categorical Variables and binary outcome using Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Code for Functional API is more complex and will require following the code and comments carefully

# Initialise Empty Vectors to collect and store respective column indices for all feature sets.
mic_demographic_history_col_indices <- c() 
mic_infarction_col_indices <- c()
mic_emergency_icu_col_indices <- c()
mic_ecg_col_indices <- c()
mic_ft_col_indices <- c()
mic_serum_col_indices <- c()
mic_relapse_col_indices <- c()
mic_medicine_col_indices <- c() 

# Use string detection to identify and extract the respective column indices for each feature set
# Extract the Column indices and store them in respective vectors. 

##### 
for (i in 1:(length(mic_demographic_history_features))) {
mic_demographic_history_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_demographic_history_features[i])
 mic_demographic_history_col_indices <- c(mic_demographic_history_col_indices, mic_demographic_history_col)
}
rm(mic_demographic_history_col,i)


##### 
for (i in 1:(length(mic_infarction_features))) {
mic_infarction_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_infarction_features[i])
 mic_infarction_col_indices <- c(mic_infarction_col_indices, mic_infarction_col)
}
rm(mic_infarction_col,i)


##### 
for (i in 1:(length(mic_emergency_icu_features))) {
mic_emergency_icu_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_emergency_icu_features[i])
 mic_emergency_icu_col_indices <- c(mic_emergency_icu_col_indices, mic_emergency_icu_col)
}
rm(mic_emergency_icu_col,i)

##### 
for (i in 1:(length(mic_ecg_features))) {
mic_ecg_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_ecg_features[i])
 mic_ecg_col_indices <- c(mic_ecg_col_indices, mic_ecg_col)
}
rm(mic_ecg_col,i)

##### 
for (i in 1:(length(mic_ft_features))) {
mic_ft_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_ft_features[i])
 mic_ft_col_indices <- c(mic_ft_col_indices, mic_ft_col)
}
rm(mic_ft_col,i)

##### 
for (i in 1:(length(mic_serum_features))) {
mic_serum_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_serum_features[i])
 mic_serum_col_indices <- c(mic_serum_col_indices, mic_serum_col)
}
rm(mic_serum_col,i)

##### 
for (i in 1:(length(mic_relapse_features))) {
mic_relapse_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_relapse_features[i])
 mic_relapse_col_indices <- c(mic_relapse_col_indices, mic_relapse_col)
}
rm(mic_relapse_col,i)

##### 
for (i in 1:(length(mic_medicine_features))) {
mic_medicine_col <- str_which( string = colnames(mic_modified_cv_train_set_ann), pattern = mic_medicine_features[i])
 mic_medicine_col_indices <- c(mic_medicine_col_indices, mic_medicine_col)
}
rm(mic_medicine_col,i)


```

```{r Perform Initial Analysis of MIC Dataset after expansion of Categorical Variables for binary outcome using Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width="75%", fig.align='center'}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


############# Create the Input Data #########################################


########## Demographic & History Features ###########################

train_features_demographic_history <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_demographic_history_col_indices)])
val_features_demographic_history <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_demographic_history_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic_history <- colnames(train_features_demographic_history)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic_history) == 0)

index_features_scaling <- which(!feature_names_demographic_history  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic_history[,index_features_scaling])

train_features_demographic_history <- train_features_demographic_history[,c(feature_names_for_scaling)]
val_features_demographic_history <- val_features_demographic_history[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic_history %<>% scale()
val_features_demographic_history %<>% 
        scale(center = attr(train_features_demographic_history, "scaled:center"),
        scale = attr(train_features_demographic_history, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Infarction Features ###########################

train_features_infarction <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_infarction_col_indices)])
val_features_infarction <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_infarction_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_infarction <- colnames(train_features_infarction)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_infarction) == 0)

index_features_scaling <- which(!feature_names_infarction  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_infarction[,index_features_scaling])

train_features_infarction <- train_features_infarction[,c(feature_names_for_scaling)]
val_features_infarction <- val_features_infarction[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_infarction %<>% scale()
val_features_infarction %<>% 
        scale(center = attr(train_features_infarction, "scaled:center"),
        scale = attr(train_features_infarction, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Emergency ICU Features ###########################

train_features_emergency_icu <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_emergency_icu_col_indices)])
val_features_emergency_icu <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_emergency_icu_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_emergency_icu <- colnames(train_features_emergency_icu)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_emergency_icu) == 0)

index_features_scaling <- which(!feature_names_emergency_icu  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_emergency_icu[,index_features_scaling])

train_features_emergency_icu <- train_features_emergency_icu[,c(feature_names_for_scaling)]
val_features_emergency_icu <- val_features_emergency_icu[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_emergency_icu %<>% scale()
val_features_emergency_icu %<>% 
        scale(center = attr(train_features_emergency_icu, "scaled:center"),
        scale = attr(train_features_emergency_icu, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## ECG Features ###########################

train_features_ecg <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_ecg_col_indices)])
val_features_ecg <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_ecg_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ecg <- colnames(train_features_ecg)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ecg) == 0)

index_features_scaling <- which(!feature_names_ecg  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ecg[,index_features_scaling])

train_features_ecg <- train_features_ecg[,c(feature_names_for_scaling)]
val_features_ecg <- val_features_ecg[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ecg %<>% scale()
val_features_ecg %<>% 
        scale(center = attr(train_features_ecg, "scaled:center"),
        scale = attr(train_features_ecg, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## FT Features ###########################

train_features_ft <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_ft_col_indices)])
val_features_ft <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_ft_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ft <- colnames(train_features_ft)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ft) == 0)

index_features_scaling <- which(!feature_names_ft  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ft[,index_features_scaling])

train_features_ft <- train_features_ft[,c(feature_names_for_scaling)]
val_features_ft <- val_features_ft[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ft %<>% scale()
val_features_ft %<>% 
        scale(center = attr(train_features_ft, "scaled:center"),
        scale = attr(train_features_ft, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Serum Features ###########################

train_features_serum <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_serum_col_indices)])
val_features_serum <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_serum_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_serum <- colnames(train_features_serum)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_serum) == 0)

index_features_scaling <- which(!feature_names_serum  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_serum[,index_features_scaling])

train_features_serum <- train_features_serum[,c(feature_names_for_scaling)]
val_features_serum <- val_features_serum[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_serum %<>% scale()
val_features_serum %<>% 
        scale(center = attr(train_features_serum, "scaled:center"),
        scale = attr(train_features_serum, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Relpase Features ###########################

train_features_relapse <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_relapse_col_indices)])
val_features_relapse <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_relapse_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_relapse <- colnames(train_features_relapse)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_relapse) == 0)

index_features_scaling <- which(!feature_names_relapse  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_relapse[,index_features_scaling])

train_features_relapse <- train_features_relapse[,c(feature_names_for_scaling)]
val_features_relapse <- val_features_relapse[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_relapse %<>% scale()
val_features_relapse %<>% 
        scale(center = attr(train_features_relapse, "scaled:center"),
        scale = attr(train_features_relapse, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Medicine Features ###########################

train_features_medicine <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_medicine_col_indices)])
val_features_medicine <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_medicine_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_medicine <- colnames(train_features_medicine)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


###################################################################################

####### Create Training & Validation Targets #######################

train_targets <- as.matrix(mic_cv_train_let_is_bin)
val_targets <- as.matrix(mic_cv_test_let_is_bin)


###################################################################################

# Let us define the input shapes. 
input_shape_demographic_history  <- ncol(train_features_demographic_history)
input_shape_infarction <- ncol(train_features_infarction)
input_shape_emergency_icu <- ncol(train_features_emergency_icu)
input_shape_ecg <- ncol(train_features_ecg)
input_shape_ft <- ncol(train_features_ft)
input_shape_serum <- ncol(train_features_serum)
input_shape_relapse <- ncol(train_features_relapse)
input_shape_medicine <- ncol(train_features_medicine)


# Let us build the Keras Inputs & Feature sets
input_demographic_history <- keras_input(shape(input_shape_demographic_history), name = "demographic_history")
input_infarction <- keras_input(shape(input_shape_infarction), name = "infarction")
input_emergency_icu <- keras_input(shape(input_shape_emergency_icu), name = "emergency_icu")
input_ecg <- keras_input(shape(input_shape_ecg), name = "ecg")
input_ft <- keras_input(shape(input_shape_ft), name = "ft")
input_serum <- keras_input(shape(input_shape_serum), name = "serum")
input_relapse <- keras_input(shape(input_shape_relapse), name = "relapse")
input_medicine <- keras_input(shape(input_shape_medicine), name = "medicine")


demographic_history_features <- 
    layer_dense(object = input_demographic_history, units = 256) |> 
    layer_dropout(rate = 0.3, seed = 1024) |>
    layer_dense(units = 128, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

infarction_features <- 
    layer_dense(object = input_infarction, units = 1536, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 


emergency_icu_features <- 
    layer_dense(object = input_emergency_icu, units = 1216, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

ecg_features <- 
    layer_dense(object = input_ecg, units = 576, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

ft_features <- 
    layer_dense(object = input_ft, units = 64, activation = "relu") |>  
    layer_dropout(rate = 0.3, seed = 1024)

serum_features <- 
    layer_dense(object = input_serum, units = 64, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

relapse_features <- 
    layer_dense(object = input_relapse, units = 72, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024)

medicine_features <- 
    layer_dense(object = input_medicine, units = 240, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) |>
    layer_dense(units = 120, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024)

# Let us combine the Feature Layers together

combined_features <- layer_concatenate(list(demographic_history_features,infarction_features, emergency_icu_features, ecg_features, ft_features, serum_features, relapse_features, medicine_features))


pred_functional_api <- layer_dense(object = combined_features, units = 1, activation = "sigmoid")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_demographic_history, input_infarction, input_emergency_icu, input_ecg, input_ft, input_serum, input_relapse, input_medicine),
  outputs = list(pred_functional_api)
)

# Let us Plot the model for Visualisation
# Commented Out for Report Creation
#print("The Plot of the Keras Functional API model is",quote = FALSE)
#print("=============================================",quote = FALSE)
#plot(functional_api_model, show_shapes = TRUE)
#print("=============================================",quote = FALSE)

#print("The summary of the Keras Functional API model is",quote = FALSE)
#print("=============================================",quote = FALSE)
#summary(functional_api_model)
#print("=============================================",quote = FALSE)

# Collect counts for initial weight generation
counts <- table(mic_cv_train_let_is_bin) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_cv_test_let_is_bin) # Counts for Validation Set

# Configure weights. Weights are updated manually. 
weight_for_0 = as.numeric(1 / counts["0"])
weight_for_1 = as.numeric(1 / counts["1"]) 

# Compile Model

metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
functional_api_model |> compile(
  optimizer = optimizer_adam(1e-3),
  loss = "binary_crossentropy",
  metrics = metrics
)


class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)


######  Fit model ##############


plot_functional_api_model <- functional_api_model |> 
  fit(
  x = list(demographic_history = train_features_demographic_history, infarction = train_features_infarction, emergency_icu = train_features_emergency_icu, ecg = train_features_ecg, ft = train_features_ft, serum = train_features_serum, relapse = train_features_relapse, medicine = train_features_medicine),
  y = train_targets,
  validation_data = list(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  verbose = 0 #set verbose=2 during development, tuning and testing
)

# Let us plot the metrics 
print("Functional API - Metrics and Trends during Training",quote = FALSE)
print("=============================================",quote = FALSE)
plot(plot_functional_api_model,smooth = TRUE)
print("=============================================",quote = FALSE)

# Prepare Predictions

val_pred_functional_api <- functional_api_model %>%
  predict(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine)) %>%
  { as.integer(. > 0.5) }

pred_correct_functional_api <- mic_cv_test_let_is_bin == val_pred_functional_api

# Print table of predictions
print("================================",quote=FALSE)
print("The Table of Predicted values is: ",quote = FALSE)
table(as.numeric(!pred_correct_functional_api))
print("================================",quote=FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is: ", quote = FALSE)
table(as.factor(mic_cv_test_let_is_bin))
print("=====================================", quote = FALSE)

# Print overall accuracy. 
print("================================", quote = FALSE)
print(c("Validation accuracy is: ", round(mean(pred_correct_functional_api), digits = 4)),quote = FALSE)
print("================================", quote = FALSE)

# Collect death events
deaths_functional_api <- mic_cv_test_let_is_bin == 1

# Prepare and print summary of death events
n_deaths_detected <- sum(deaths_functional_api & pred_correct_functional_api)
n_deaths_missed <- sum(deaths_functional_api & !pred_correct_functional_api)
n_live_flagged <- sum(!deaths_functional_api & !pred_correct_functional_api)

print(c("deaths detected",n_deaths_detected))
print(c("deaths missed",n_deaths_missed))
print(c("survival cases flagged as deaths",n_live_flagged))

# Print confusion matrix
print("================================", quote=FALSE)
print("The Confusion Matrix is",quote=FALSE)
confusionMatrix(data = as.factor(val_pred_functional_api), reference = as.factor(mic_cv_test_let_is_bin))
print("================================", quote=FALSE)



######## Remove Variables that are not required anymore ############

rm(metrics, counts, deaths_functional_api, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct_functional_api, val_pred_functional_api, weight_for_0, weight_for_1)

rm( demographic_history_features, ecg_features, emergency_icu_features,  ft_features,   infarction_features, medicine_features, relapse_features, serum_features, combined_features)

rm( input_demographic_history, input_ecg, input_emergency_icu,  input_ft,  input_infarction, input_medicine, input_relapse, input_serum)

rm( train_features_demographic_history, train_features_ecg, train_features_emergency_icu, train_features_ft,  train_features_infarction, train_features_medicine, train_features_relapse, train_features_serum, train_features_all)

rm( val_features_demographic_history, val_features_ecg, val_features_emergency_icu, val_features_ft,  val_features_infarction, val_features_medicine, val_features_relapse, val_features_serum, val_features_all)

rm( feature_names_demographic_history, feature_names_ecg, feature_names_emergency_icu,  feature_names_ft,  feature_names_infarction, feature_names_medicine, feature_names_relapse, feature_names_serum)

rm(input_shape_demographic_history, input_shape_ecg, input_shape_emergency_icu,  input_shape_ft,  input_shape_infarction, input_shape_medicine, input_shape_relapse, input_shape_serum)

rm(functional_api_model, pred_functional_api,train_targets, val_targets, class_weight, plot_functional_api_model)

rm(mic_cv_train_let_is_bin, mic_cv_test_let_is_bin)
```

We are now able to detect 29 deaths while missing 17 and flagging 23 live cases as deaths.

With Keras Functional API, The number of deaths detected has reduced. However, we are able to match results in terms of the overall accuracy as well as reducing the live events marked as deaths. Though not as good as with the Sequential API, we are seeing some incremental improvements in the Specificity.

However, I must reiterate that Functional API work best when the features/predictors are put together to provide the best possible predictions and I must be honest in admitting that this project is the first time that I have tried my hand at ANN and my knowledge of ANN is very basic at best. It is very much possible to build ANN with Functional API that will provide higher overall and balanced accuracies while increasing the number of deaths detected.

```{r MIC Data Clean up Cross Validation Sets Keras ANN Binary outcome, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required and run the garbage collector
rm(mic_modified_cv_train_set_ann, mic_modified_cv_test_set_ann, mic_modified_train_set_ann) 

gc()

```

### Multiple Categories for Deaths

We will now pick up our biggest challenge as yet which is to predict the lethal event (LET_IS) along with the category. With all our algorithms, we are really struggling even with the much simpler binary outcome for predicting the lethal event so our chances of being able to achieve decent categorical accuracy and predictions for the individual categories in the lethal event are likely to be bleak. We will only use XGBoost and ANN to evaluate the same. Readers are encouraged to check with the other algorithms as they deem fit.

For multiple categories for deaths, our goals will also be slightly different. While it is really important to predict outcomes at an individual level accurately, here we will strive to:-

1.  Predict outcomes at the level of the category - Though this seems a bit misleading, we must remember that such predictions are still useful for people who are looking at summary statistics at a higher level of abstraction. This should be quite possible.
    1.  We will use the term "Shape" (of predictions) to mean the overall number of predictions for each class at the level of the class. This will give us a summary view of the predictions at the class level.
2.  Predict individual outcomes within each category - This will require predictions within each category to be accurate, this is going to be very difficult and we are most likely to fail on this aspect, we will see where we can get to.

#### XGBoost

Let us start with XGBoost.

```{r Perform initial analysis of MIC dataset using XGBoost for categorical outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Original Dataset without Imputation

# Retain LET_IS with multiple categories for deaths. It is already set as an integer during the import process

# DO NOT CONVERT CATEGORICAL(ORDINAL) AND BINARY(NOMINAL) VARIABLES TO FACTORS
# CONVERSION USING as.matrix() CAUSES THEM TO BE COERCED TO CHARACTER VECTORS

mic_data_xgboost <- mic_data %>% 
    mutate_at(mic_continuous_variables, ~as.numeric(.))


# Split into Training and Testing Sets. Use index created earlier. Create only Training Set. Testing Set is not required for now

mic_modified_train_xgboost <- mic_data_xgboost[-mic_test_index,]

######################

# Create Datasets for Cross Validation. 

mic_modified_cv_train_set_xgboost <- mic_modified_train_xgboost[-mic_test_index_cv,]
mic_modified_cv_test_set_xgboost <- mic_modified_train_xgboost[mic_test_index_cv,]

###########


###########

# Create a matrix that can be used by XGBoost for Training
dtrain_categorical <- xgb.DMatrix(data = as.matrix(mic_modified_cv_train_set_xgboost[,c(2:112)]), label= mic_modified_cv_train_set_xgboost$LET_IS, nthread = 8)

# Fit XGBoost
fit_xgboost_mic_categorical <- xgb.train(
    data = dtrain_categorical,
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 200,
    objective = "multi:softmax", 
    params = list("num_class" = 8, "booster" = "gbtree"),
    verbose = 0 # set verbose=2 during development, tuning and testing
)

# Print Summary of Parameters

print("=====================================", quote = FALSE)
print("The details for XGBoost are: ", quote = FALSE)
fit_xgboost_mic_categorical
print("=====================================")

# Generate probabilities for predictions
pred_xgboost_mic_categorical <-predict(object = fit_xgboost_mic_categorical, newdata = as.matrix(mic_modified_cv_test_set_xgboost[,c(2:112)]))

# Print table of predictions
print("=====================================", quote = FALSE)
print("The table of predictions is: ", quote = FALSE)
table(as.factor(pred_xgboost_mic_categorical ))
print("=====================================", quote = FALSE)

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is: ", quote = FALSE)
table(mic_modified_cv_test_set_xgboost$LET_IS)
print("=====================================", quote = FALSE)

# Print overall accuracy
print("The accuracy of predictions is: ", quote = FALSE)
mean(pred_xgboost_mic_categorical == mic_modified_cv_test_set_xgboost$LET_IS)
print("=====================================", quote = FALSE)

print("The confusion matrix is")
print("=====================================", quote = FALSE)
cm_xgboost_multi <- confusionMatrix(data = as.factor(pred_xgboost_mic_categorical), reference = as.factor(mic_modified_cv_test_set_xgboost$LET_IS))
cm_xgboost_multi
print("=====================================", quote = FALSE)

# Extract and Print summary of death events

print("================================",quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(cm_xgboost_multi$table[2,2], cm_xgboost_multi$table[3,3], cm_xgboost_multi$table[4,4], cm_xgboost_multi$table[5,5], cm_xgboost_multi$table[6,6], cm_xgboost_multi$table[7,7], cm_xgboost_multi$table[8,8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(cm_xgboost_multi$table[2:8,2:8])
print("================================",quote=FALSE)

# Remove data that is no longer required
rm(fit_xgboost_mic_categorical, pred_xgboost_mic_categorical , dtrain_categorical)

rm(cm_xgboost_multi)
```

With XGBoost, we are able to predict and classify 11 deaths accurately. We are able to detect 19 deaths while missing 27 deaths.

As expected, XGBoost is not as accurate with predictions for multiple categories of outcomes as it was for the binary outcome.

As usual, it is extremely quick.

```{r MIC XGBoost Categorical Outcome - CV Set Cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required and run the garbage collector

rm(mic_modified_cv_test_set_xgboost, mic_modified_cv_train_set_xgboost, dtrain, mic_modified_train_xgboost, mic_data_xgboost)

rm(mic_modified_cv_test_set, mic_modified_cv_train_set, mic_data_modified, mic_data_xgboost)

gc()

```

\newpage

#### Artificial Neural Networks

```{r Prepare MIC Dataset for expansion of Categorical Variables and multiple outcomes, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use prviously created Lists of Continuous, Ordinal (Categorical) and Nominal (Binary) Features to avoid duplication and errors. 


########

# Convert nominal variables from factors to integers. Since they are binary, changing them from factors to integers does not alter their behaviour 

mic_modified_cv_train_set_ann <- mic_orig_cv_train_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 

mic_modified_cv_test_set_ann <- mic_orig_cv_test_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 


# Expand Imputed values using Dummy Variables.

##########

# Retain LET_IS values as they will be lost during conversion

mic_cv_train_let_is_multi <- mic_modified_cv_train_set_ann$LET_IS

mic_cv_test_let_is_multi <- mic_modified_cv_test_set_ann$LET_IS

# Convert LET_IS as factor for one-hot encoding

mic_modified_cv_train_set_ann %<>% mutate(LET_IS = as.factor(LET_IS))

mic_modified_cv_test_set_ann %<>% mutate(LET_IS = as.factor(LET_IS))

########## ONE-HOT ENCODING USING CARET FOR PREDICTORS AND OUTCOME ###################

# We will use CARET for creating the dummy variables for all the predictors and the outcome as it supports both ordered and unordered factors 

# We will use two seperate instances of dummyVars for creations of the dummy variables for the features and the outcome respectively. We can do it using a single instance but the code is very confusing and difficult to follow.

################## Use formula interface to create template for dummy vars. ################## 

########### Features ###########
dummy_vars_cv_features <- dummyVars(formula = "~.", data = mic_modified_cv_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)], sep = "_", levelsOnly = FALSE, fullRank = FALSE)
########### Outcome ###########
dummy_vars_cv_outcome <- dummyVars(formula = "~.", data = mic_modified_cv_train_set_ann["LET_IS"],  sep = "_", levelsOnly = FALSE, fullRank = FALSE)

################## Create dummy vars for Training CV and Testing CV datasets. ##################
mic_modified_cv_train_set_ann_dummy <- as.data.frame(predict(dummy_vars_cv_features, mic_modified_cv_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_cv_test_set_ann_dummy <- as.data.frame(predict(dummy_vars_cv_features, mic_modified_cv_test_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_cv_train_set_ann_outcome <-  as.data.frame(predict(dummy_vars_cv_outcome, mic_modified_cv_train_set_ann["LET_IS"]))

mic_modified_cv_test_set_ann_outcome <- as.data.frame(predict(dummy_vars_cv_outcome, mic_modified_cv_test_set_ann["LET_IS"]))

##############################################################################################

# Collect Training CV dummy vars names. Same applies for Testing CV. 
dummy_vars_names <- colnames(mic_modified_cv_train_set_ann_dummy)

# Modify Training CV and Testing CV Datasets to remove the existing columns for variables related to  "ID", Ordinal,  Partially Ordinal, Complications and "LET_IS".

mic_modified_cv_train_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

mic_modified_cv_test_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))


# Collect Modified Training CV Column names. Same applies for Testing CV
mic_modified_cv_col_names <- colnames(mic_modified_cv_train_set_ann)

# Combine Collected Modified Training CV column names with Training CV dummy vars names for predictors.
mic_modified_cv_col_names <- c(mic_modified_cv_col_names, dummy_vars_names)

# Create combined Training CV and Testing CV datasets by binding together the Modified CV Training and Testing datasets with the respective dummy vars daatasets
mic_modified_cv_train_set_ann <- cbind(mic_modified_cv_train_set_ann, mic_modified_cv_train_set_ann_dummy)

mic_modified_cv_test_set_ann <- cbind(mic_modified_cv_test_set_ann, mic_modified_cv_test_set_ann_dummy)

# Assign column names to the newly created combined Training CV, Testing CV datasets and the outcome datasets.

colnames(mic_modified_cv_train_set_ann) <- mic_modified_cv_col_names

colnames(mic_modified_cv_test_set_ann) <- mic_modified_cv_col_names

# Remove data that is not required anymore

rm(dummy_vars_cv_features, dummy_vars_cv_outcome, mic_modified_cv_train_set_ann_dummy, mic_modified_cv_test_set_ann_dummy, mic_modified_cv_col_names)
```

##### Sequential API

Let us see what ANN built using the Sequential API can do

```{r Perform Initial Analysis for MIC Dataset using Neural Networks with Categories for outcome LET_IS using Sequential API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment

set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############


feature_names <- c(mic_nominal_variables, mic_continuous_variables, dummy_vars_names)


train_features <- as.matrix(mic_modified_cv_train_set_ann[feature_names])
train_targets <- as.matrix(mic_modified_cv_train_set_ann_outcome)

val_features <- as.matrix(mic_modified_cv_test_set_ann[feature_names])
val_targets <- as.matrix(mic_modified_cv_test_set_ann_outcome)

####################################
# As earlier, We need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
# As earlier, we also need to remove Columns from the list that produce NA when scaled

####### Normal Scaling ############
feature_names <- colnames(train_features)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

####### Normal Scaling ############
train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))




model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(units = 3072) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 3072) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 2304) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 8, activation = 'softmax')

# Print model for visualisation
# Commented out for report creation
# summary(model)

# Collect counts for generation of initial weights
counts <- table(mic_cv_train_let_is_multi) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_cv_test_let_is_multi) # Counts for Validation Set

# Configure Weights. Weights updated manually with multipliers. 

weight_for_0 = as.numeric(1 / counts["0"])*2.6
weight_for_1 = as.numeric(1 / counts["1"])*0.225
weight_for_2 = as.numeric(1 / counts["2"])*0.05
weight_for_3 = as.numeric(1 / counts["3"])*0.25
weight_for_4 = as.numeric(1 / counts["4"])*0.05
weight_for_5 = as.numeric(1 / counts["5"])*0.0750
weight_for_6 = as.numeric(1 / counts["6"])*0.0875
weight_for_7 = as.numeric(1 / counts["7"])*0.06125

# Train the Model 

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy','categorical_accuracy')
)


class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1,
                     "2" = weight_for_2,
                     "3" = weight_for_3,
                     "4" = weight_for_4,
                     "5" = weight_for_5,
                     "6" = weight_for_6,
                     "7" = weight_for_7)



model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  batch_size = 2048,
  epochs = 30,
  class_weight = class_weight,
  verbose = 0 # set verbose=2 for development and testing
)



# Evaluate Model
# Commented out for Report creation
# model |> evaluate(val_features, val_targets)

# Prepare Predictions. Method is different from those used earlier for Binary outcomes

probs <- model |> predict(val_features)

pred_ann <- max.col(probs) - 1L

# Print table of predictions 
print("================================", quote = FALSE)
print("The Table of Predicted values is", quote= FALSE)
table(as.factor(pred_ann))

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is: ", quote = FALSE)
table(as.factor(mic_cv_test_let_is_multi))

# Print overall accuracy
print("================================", quote = FALSE)
print("Validation accuracy is : ",quote = FALSE)
mean(mic_cv_test_let_is_multi == pred_ann)
print("================================", quote = FALSE)

# Print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is",quote=FALSE)
cm_ann_seq_api_multi <-confusionMatrix(data = as.factor(pred_ann), reference = as.factor(mic_cv_test_let_is_multi))
cm_ann_seq_api_multi 
print("================================",quote=FALSE)

# Extract and print summary of death events 
print("================================",quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(cm_ann_seq_api_multi$table[2,2], cm_ann_seq_api_multi$table[3,3], cm_ann_seq_api_multi$table[4,4], cm_ann_seq_api_multi$table[5,5], cm_ann_seq_api_multi$table[6,6], cm_ann_seq_api_multi$table[7,7], cm_ann_seq_api_multi$table[8,8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(cm_ann_seq_api_multi$table[2:8,2:8])
print("================================",quote=FALSE)

# Remove unwanted variables

rm(class_weight, metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct, val_pred, weight_for_0, weight_for_1, weight_for_2, weight_for_3, weight_for_4, weight_for_5, weight_for_6, weight_for_7)

rm(index_features_scaling, feature_names_for_scaling)

rm(cm_ann_seq_api_multi)

```

When we need to predict categorical outcomes with ANN, the need for modifying the class weights becomes apparent. It is a bit of a paradoxical situation because by modifying the class weights we can get almost any ANN to predict the shape or categorical distributions a lot more accurately than individual predictions within each category.

Modifying the class weights is not a very difficult task and can be easily done with a little bit of iterative tuning.

However, it is better to build the ANN first to be as accurate as possible and then use the class weights only for further tuning.

We will try to balance the accuracy of predictions with maintaining the shape of the predictions as much as possible.

With our ANN and the adjustment to the class weights, we are able to predict and categorise about 12 Death events accurately and detect 27 Deaths accurately.

As indicated earlier, another thing to watch out for with ANN is that Just like over-fitting or over-training, we can suffer from over-optimising the ANN to the validation set. It is pretty easy to achieve 80% accuracy on the validation set only for the accuracy to reduce to 73%-74% when tried against another set.

\newpage

##### Functional API

Let us see what we can achieve with Functional API next

```{r Perform Initial Analysis of MIC Dataset after expansion of Categorical Variables for Multiple Category outcome using Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, fig.align='center'}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################
########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############



############# Create the Input Data #########################################

########## Demographic & History Features ###########################

train_features_demographic_history <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_demographic_history_col_indices)])
val_features_demographic_history <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_demographic_history_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic_history <- colnames(train_features_demographic_history)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic_history) == 0)

index_features_scaling <- which(!feature_names_demographic_history  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic_history[,index_features_scaling])

train_features_demographic_history <- train_features_demographic_history[,c(feature_names_for_scaling)]
val_features_demographic_history <- val_features_demographic_history[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic_history %<>% scale()
val_features_demographic_history %<>% 
        scale(center = attr(train_features_demographic_history, "scaled:center"),
        scale = attr(train_features_demographic_history, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )



########## Infarction Features ###########################

train_features_infarction <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_infarction_col_indices)])
val_features_infarction <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_infarction_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_infarction <- colnames(train_features_infarction)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_infarction) == 0)

index_features_scaling <- which(!feature_names_infarction  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_infarction[,index_features_scaling])

train_features_infarction <- train_features_infarction[,c(feature_names_for_scaling)]
val_features_infarction <- val_features_infarction[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_infarction %<>% scale()
val_features_infarction %<>% 
        scale(center = attr(train_features_infarction, "scaled:center"),
        scale = attr(train_features_infarction, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Emergency ICU Features ###########################

train_features_emergency_icu <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_emergency_icu_col_indices)])
val_features_emergency_icu <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_emergency_icu_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_emergency_icu <- colnames(train_features_emergency_icu)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_emergency_icu) == 0)

index_features_scaling <- which(!feature_names_emergency_icu  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_emergency_icu[,index_features_scaling])

train_features_emergency_icu <- train_features_emergency_icu[,c(feature_names_for_scaling)]
val_features_emergency_icu <- val_features_emergency_icu[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_emergency_icu %<>% scale()
val_features_emergency_icu %<>% 
        scale(center = attr(train_features_emergency_icu, "scaled:center"),
        scale = attr(train_features_emergency_icu, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## ECG Features ###########################

train_features_ecg <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_ecg_col_indices)])
val_features_ecg <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_ecg_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ecg <- colnames(train_features_ecg)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ecg) == 0)

index_features_scaling <- which(!feature_names_ecg  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ecg[,index_features_scaling])

train_features_ecg <- train_features_ecg[,c(feature_names_for_scaling)]
val_features_ecg <- val_features_ecg[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ecg %<>% scale()
val_features_ecg %<>% 
        scale(center = attr(train_features_ecg, "scaled:center"),
        scale = attr(train_features_ecg, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## FT Features ###########################

train_features_ft <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_ft_col_indices)])
val_features_ft <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_ft_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ft <- colnames(train_features_ft)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ft) == 0)

index_features_scaling <- which(!feature_names_ft  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ft[,index_features_scaling])

train_features_ft <- train_features_ft[,c(feature_names_for_scaling)]
val_features_ft <- val_features_ft[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ft %<>% scale()
val_features_ft %<>% 
        scale(center = attr(train_features_ft, "scaled:center"),
        scale = attr(train_features_ft, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Serum Features ###########################

train_features_serum <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_serum_col_indices)])
val_features_serum <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_serum_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_serum <- colnames(train_features_serum)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_serum) == 0)

index_features_scaling <- which(!feature_names_serum  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_serum[,index_features_scaling])

train_features_serum <- train_features_serum[,c(feature_names_for_scaling)]
val_features_serum <- val_features_serum[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_serum %<>% scale()
val_features_serum %<>% 
        scale(center = attr(train_features_serum, "scaled:center"),
        scale = attr(train_features_serum, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Relapse Features ###########################

train_features_relapse <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_relapse_col_indices)])
val_features_relapse <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_relapse_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_relapse <- colnames(train_features_relapse)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_relapse) == 0)

index_features_scaling <- which(!feature_names_relapse  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_relapse[,index_features_scaling])

train_features_relapse <- train_features_relapse[,c(feature_names_for_scaling)]
val_features_relapse <- val_features_relapse[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_relapse %<>% scale()
val_features_relapse %<>% 
        scale(center = attr(train_features_relapse, "scaled:center"),
        scale = attr(train_features_relapse, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Medicine Features ###########################

train_features_medicine <- as.matrix(mic_modified_cv_train_set_ann[,c(mic_medicine_col_indices)])
val_features_medicine <- as.matrix(mic_modified_cv_test_set_ann[,c(mic_medicine_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_medicine <- colnames(train_features_medicine)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )



############## CREATE TRAINING AND VALIDATION TARGETS ##############################
train_targets <- as.matrix(mic_modified_cv_train_set_ann_outcome)
val_targets <- as.matrix(mic_modified_cv_test_set_ann_outcome)

###################################################################################

# Let us define the input shapes. 
input_shape_demographic_history  <- ncol(train_features_demographic_history)
input_shape_infarction <- ncol(train_features_infarction)
input_shape_emergency_icu <- ncol(train_features_emergency_icu)
input_shape_ecg <- ncol(train_features_ecg)
input_shape_ft <- ncol(train_features_ft)
input_shape_serum <- ncol(train_features_serum)
input_shape_relapse <- ncol(train_features_relapse)
input_shape_medicine <- ncol(train_features_medicine)



# Let us build the Keras Inputs & Features
input_demographic_history <- keras_input(shape(input_shape_demographic_history), name = "demographic_history")
input_infarction <- keras_input(shape(input_shape_infarction), name = "infarction")
input_emergency_icu <- keras_input(shape(input_shape_emergency_icu), name = "emergency_icu")
input_ecg <- keras_input(shape(input_shape_ecg), name = "ecg")
input_ft <- keras_input(shape(input_shape_ft), name = "ft")
input_serum <- keras_input(shape(input_shape_serum), name = "serum")
input_relapse <- keras_input(shape(input_shape_relapse), name = "relapse")
input_medicine <- keras_input(shape(input_shape_medicine), name = "medicine")

########################################

demographic_history_features <- 
    layer_dense(object = input_demographic_history, units = 1024) |> 
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dropout(rate = 0.3, seed = 1024) 

infarction_features <- 
    layer_dense(object = input_infarction, units = 2048) |> 
    layer_dense(units = 2048) |>
    layer_dropout(rate = 0.3, seed = 1024) 

emergency_icu_features <- 
    layer_dense(object = input_emergency_icu, units = 1280) |> 
    layer_dense(units = 1280) |>
    layer_dense(units = 1280) |>
    layer_dense(units = 1280) |>
    layer_dropout(rate = 0.3, seed = 1024)

ecg_features <- 
    layer_dense(object = input_ecg, units = 1024) |> 
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dropout(rate = 0.3, seed = 1024)
      
ft_features <- 
    layer_dense(object = input_ft, units = 64) |> 
    layer_dropout(rate = 0.3, seed = 1024)    

serum_features <- 
    layer_dense(object = input_serum, units = 64) |> 
    layer_dropout(rate = 0.3, seed = 1024)

relapse_features <- 
    layer_dense(object = input_relapse, units = 96) |>
    layer_dropout(rate = 0.3, seed = 1024) 

medicine_features <- 
    layer_dense(object = input_medicine, units = 960) |> 
    layer_dense(units = 960) |>
    layer_dense(units = 960) |>
    layer_dense(units = 960) |>
    layer_dropout(rate = 0.3, seed = 1024)


########################################

# Let us combine the Feature Layers together

combined_features <- layer_concatenate(list(demographic_history_features, infarction_features, emergency_icu_features, ecg_features, ft_features, serum_features, relapse_features, medicine_features))

pred_functional_api <- layer_dense(object = combined_features, units = 8, activation = "softmax")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_demographic_history, input_infarction, input_emergency_icu, input_ecg, input_ft, input_serum, input_relapse, input_medicine),
  outputs = list(pred_functional_api)
)

# Let us Plot the model for Visualisation
# Commented out for Report Creation
#print("The Plot of the Keras Functional API model is",quote = FALSE)
#print("=============================================",quote = FALSE)
#plot(functional_api_model, show_shapes = TRUE)
#print("=============================================",quote = FALSE)

#print("The summary of the Keras Functional API model is",quote = FALSE)
#print("=============================================",quote = FALSE)
#summary(functional_api_model)
#print("=============================================",quote = FALSE)

# Collect counts for generation of initial weights
counts <- table(mic_cv_train_let_is_multi) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_cv_test_let_is_multi) # Counts for Validation Set

# Configure weights. Weights are updated manually with multipliers
weight_for_0 = as.numeric(1 / counts["0"])*3.5
weight_for_1 = as.numeric(1 / counts["1"])*1.1
weight_for_2 = as.numeric(1 / counts["2"])*0.2
weight_for_3 = as.numeric(1 / counts["3"])*0.7
weight_for_4 = as.numeric(1 / counts["4"])*0.15
weight_for_5 = as.numeric(1 / counts["5"])*0.3
weight_for_6 = as.numeric(1 / counts["6"])*0.3
weight_for_7 = as.numeric(1 / counts["7"])*0.4


# Train the Model 

functional_api_model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy','categorical_accuracy')
)


##############
class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1,
                     "2" = weight_for_2,
                     "3" = weight_for_3,
                     "4" = weight_for_4,
                     "5" = weight_for_5,
                     "6" = weight_for_6,
                     "7" = weight_for_7)

######  Fit model ##############


functional_api_model |> 
  fit(
  x = list(demographic_history = train_features_demographic_history, infarction = train_features_infarction, emergency_icu = train_features_emergency_icu, ecg = train_features_ecg, ft = train_features_ft, serum = train_features_serum, relapse = train_features_relapse, medicine = train_features_medicine ),
  y = train_targets,
  validation_data = list(list(val_features_demographic_history,  val_features_infarction,  val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets),
  batch_size = 2048,
  epochs = 30,
  class_weight = class_weight,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)


# Evaluate Model
# Commented out for Report creation
# functional_api_model |> evaluate(list(val_features_demographic_history,  val_features_infarction,   val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets)

# Prepare Predictions. Method is different from those used earlier for Binary outcomes

probs <- functional_api_model |> predict(list(val_features_demographic_history,  val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine))

pred_ann <- max.col(probs) - 1L 

# Print table of predictions 
print("================================", quote = FALSE)
print("The Table of Predicted values is: ", quote= FALSE)
table(as.factor(pred_ann))

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is", quote = FALSE)
table(as.factor(mic_cv_test_let_is_multi))

# Print overall accuracy
print("================================", quote = FALSE)
print("Validation accuracy is : ",quote = FALSE)
mean(mic_cv_test_let_is_multi  == pred_ann)

# Print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is", quote=FALSE)
cm_ann_seq_func_multi <- confusionMatrix(data = as.factor(pred_ann), reference = as.factor(mic_cv_test_let_is_multi ))
cm_ann_seq_func_multi
print("================================",quote=FALSE)

# Extract and Print summary about death events
print("================================", quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(cm_ann_seq_func_multi$table[2,2], cm_ann_seq_func_multi$table[3,3], cm_ann_seq_func_multi$table[4,4], cm_ann_seq_func_multi$table[5,5], cm_ann_seq_func_multi$table[6,6], cm_ann_seq_func_multi$table[7,7], cm_ann_seq_func_multi$table[8,8])
print("================================", quote=FALSE)

print("================================", quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(cm_ann_seq_func_multi$table[2:8,2:8])
print("================================", quote=FALSE)

# Remove data that is no longer required

rm(metrics, counts, deaths_functional_api, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct_functional_api, val_pred_functional_api)

rm( demographic_history_features, ecg_features, emergency_icu_features, ft_features,  infarction_features, medicine_features, relapse_features, serum_features, combined_features)

rm(input_demographic_history, input_ecg, input_emergency_icu, input_ft,  input_infarction, input_medicine, input_relapse, input_serum)

rm( train_features_demographic_history, train_features_ecg, train_features_emergency_icu,  train_features_ft,   train_features_infarction, train_features_medicine, train_features_relapse, train_features_serum)

rm( val_features_demographic_history, val_features_ecg, val_features_emergency_icu, val_features_ft,  val_features_infarction, val_features_medicine, val_features_relapse, val_features_serum)

rm( feature_names_demographic_history, feature_names_ecg, feature_names_emergency_icu,  feature_names_ft,   feature_names_infarction, feature_names_medicine, feature_names_relapse, feature_names_serum)

rm( input_shape_demographic_history, input_shape_ecg, input_shape_emergency_icu, input_shape_ft,  input_shape_infarction, input_shape_medicine, input_shape_relapse, input_shape_serum)

rm(mic_blockage_col_indices, mic_demographic_history_col_indices, mic_ecg_col_indices, mic_emergency_icu_col_indices, mic_endocrine_col_indices, mic_ft_col_indices, mic_hf_rhythm_col_indices, mic_hypertension_col_indices, mic_infarction_col_indices, mic_medicine_col_indices, mic_pulmonary_col_indices, mic_relapse_col_indices, mic_serum_col_indices, mic_all_col_indices)

rm(functional_api_model, pred_functional_api,probs, pred_ann, mic_train_let_is, mic_cv_test_let_is_multi, mic_cv_train_let_is_multi)

rm(weight_for_0, weight_for_1, weight_for_2, weight_for_3, weight_for_4, weight_for_5, weight_for_6, weight_for_7, class_weight)

rm(train_features, train_targets, val_features, val_targets)

rm(cm_ann_seq_func_multi)
```

With Functional API, we are able to accurately predict and categorise 13 deaths. We are able to detect 28 deaths accurately. This is just about marginally better than what we could do with Sequential API.

Where we have some real progress with Functional API is in the shape of the predictions. They are a lot more closer to the actual shape of predictions than XGBoost or ANN built with Sequential API.

However, we need to evaluate the performance of all against the holdout set before drawing further conclusions.

```{r MIC Data Clean up Cross Validation Sets Keras ANN Categorical outcome, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required 
rm(mic_modified_cv_train_set_ann, mic_modified_cv_test_set_ann, mic_modified_train_set_ann) 

rm(mic_modified_cv_train_set_ann_outcome, mic_modified_cv_test_set_ann_outcome, dummy_vars_names)

gc()

```

```{r MIC Data Clean up Cross Validation Sets, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Clean up Data and run the garbage collector to free up memory

rm(mic_orig_cv_train_set, mic_orig_cv_test_set, mic_orig_cv_train_imputed_mice, mic_orig_cv_test_imputed_mice)

rm(mic_test_index_cv)

gc()
```

\newpage

## Predictions for Holdout Set

We will now perform our predictions for the Holdout sets using the same models as we did for the Cross Validation sets. With Naive Bayes and XGBoost , we just reuse the parameters that we have used in the Analysis section of this chapter.

### Single Category for Deaths

#### Naive Bayes without Imputation - Binary Outcome

```{r Predictions using Naive Bayes for MIC Holdout Dataset without imputation - 2, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Change LET_IS to have a Single Category for Deaths rather than multiple 

mic_modified_train <- mic_orig_train %>% 
    mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0)) %>%
    mutate(LET_IS = as.factor(LET_IS))

mic_modified_test <- mic_orig_test %>% 
    mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0)) %>%
    mutate(LET_IS = as.factor(LET_IS))


# We will configure 
# S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson
# AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian
# KFK_BLOOD = 0 is set already for all observations


mic_modified_train <- mic_modified_train %>% 
      mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD", "D_AD_KBRIG", "D_AD_ORIT","ROE"), ~(.= as.integer(.))) %>%
      mutate(AGE = as.numeric(AGE))

mic_modified_test <- mic_modified_test %>% 
      mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD", "D_AD_KBRIG", "D_AD_ORIT","ROE"), ~(.= as.integer(.)))  %>%
      mutate(AGE = as.numeric(AGE))



###########
# Fit Naive Bayes
fit_nb_native_LET_IS_final <- naive_bayes(x = mic_modified_train[,2:112], y = mic_modified_train$LET_IS, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
summary(fit_nb_native_LET_IS_final)

# Make Predictions
pred_nb_test_native_LET_IS_final <-predict(object = fit_nb_native_LET_IS_final, newdata = mic_modified_test[,2:112])

# Print table of predictions
print("================================", quote=FALSE)
print("The table of predictions is", quote = FALSE)
table(as.factor(pred_nb_test_native_LET_IS_final))
print("================================", quote=FALSE)

# Print table of actual values
print("================================", quote=FALSE)
print("The table of actual values is", quote = FALSE)
table(mic_modified_test$LET_IS)
print("================================", quote=FALSE)

# Print overall accuracy
print("The accuracy of predictions is", quote = FALSE)
mean(pred_nb_test_native_LET_IS_final == mic_modified_test$LET_IS)

# Record and Print confusion matrix
print("The confusion matrix is", quote = FALSE)
print("================================", quote=FALSE)
mic_nb_cm_bin_2 <- confusionMatrix(data = pred_nb_test_native_LET_IS_final, reference = mic_modified_test$LET_IS)
mic_nb_cm_bin_2
print("================================", quote=FALSE)


# Remove Data that is no longer required

rm(fit_nb_native_LET_IS_final, pred_nb_test_native_LET_IS_final, mic_modified_train, mic_modified_test)


```

Naive Bayes without imputation performs lower for the holdout set than it does with the cross validation set.

The Number of deaths predicted has increased only slightly. The number of deaths missed and the number of live cases reported as deaths has increased a lot causing a much larger drop in both overall accuracy and the balanced accuracy. The Specificity has also degraded when compared to the initial Analysis.

#### Naive Bayes with Imputation - Binary Outcome

```{r MIC Holdout Set Predictions Naive Bayes - Imputation using the Mice Package and Random Forest as the Imputation Method - 1, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.


# Set up new Training set with imputed values. Configure LET_IS for binary prediction

mic_modified_train <- mic_orig_train_imputed_mice %>% 
    mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0)) %>%
    mutate(LET_IS = as.factor(LET_IS))

mic_modified_test <- mic_orig_test_imputed_mice %>% 
    mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, 1, 0)) %>%
    mutate(LET_IS = as.factor(LET_IS))

# Configure variables 

# We will configure 
# S_AD_KBRIG, S_AD_ORIT, NA_BLOOD, D_AD_KBRIG, D_AD_ORIT and ROE as Poisson
# AGE, K_BLOOD, ALT_BLOOD, AST_BLOOD and L_BLOOD as Gaussian
# KFK_BLOOD = 0 is set already for all observations

mic_modified_train <- mic_modified_train %>% 
      mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.))) 

mic_modified_test <- mic_modified_test %>% 
      mutate_at(c("S_AD_KBRIG", "S_AD_ORIT", "NA_BLOOD"), ~(.= as.integer(.))) 


###########
# Fit Naive Bayes
fit_nb_native_LET_IS_final <- naive_bayes(x = mic_modified_train[,2:112], y = mic_modified_train$LET_IS, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
summary(fit_nb_native_LET_IS_final)

# Prepare Predictions
pred_nb_test_native_LET_IS_final <-predict(object = fit_nb_native_LET_IS_final, newdata = mic_modified_test)

# Print table of predictions
print("================================", quote=FALSE)
print("The table of predictions is: ", quote = FALSE)
table(as.factor(pred_nb_test_native_LET_IS_final))
print("================================", quote=FALSE)

# Print table of actual values
print("================================", quote=FALSE)
print("The table of actual values is: ", quote = FALSE)
table(mic_modified_test$LET_IS)
print("================================", quote=FALSE)

# Print overall accuracy
print("The accuracy of predictions is: ", quote = FALSE)
mean(pred_nb_test_native_LET_IS_final == mic_modified_test$LET_IS)

# Record and Print confusion matrix
print("The confusion matrix is: ", quote = FALSE)
print("================================",quote=FALSE)
mic_nb_wi_cm_bin_1 <- confusionMatrix(data = pred_nb_test_native_LET_IS_final, reference = mic_modified_test$LET_IS)
mic_nb_wi_cm_bin_1
print("================================",quote=FALSE)



# Remove Data that is no longer required

rm(fit_nb_native_LET_IS_final, pred_nb_test_native_LET_IS_final)


```

We can see the same behaviour with the imputed dataset. The number of deaths missed and the number of live cases reported as deaths has increased dragging down the overall accuracy and the balanced accuracy. Again the Specificity has also degraded when compared to the initial Analysis.

We can see a pattern start to emerge that is an indication of the complexity of this dataset and the difficulty of making accurate predictions.

\newpage

#### XGBoost

Let us see if XGBoost can fare any better

```{r Perform Final analysis of MIC dataset using XGBoost for binary outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently. 

# Use Original Dataset without Imputation

# Change LET_IS to have a Single Category for Deaths rather than multiple. Set up outcomes as Integers specifically or they could get coerced to character vectors otherwise. 

mic_data_modified_xgboost <- mic_data %>% mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0)))


# DO NOT CONVERT CATEGORICAL(ORDINAL) AND BINARY(NOMINAL) VARIABLES TO FACTORS
# CONVERSION USING as.matrix() CAUSES THEM TO BE COERCED TO CHARACTER VECTORS

mic_data_modified_xgboost <- mic_data_modified_xgboost %>% mutate_at(mic_continuous_variables, ~as.numeric(.))

mic_modified_train_xgboost <- mic_data_modified_xgboost[-mic_test_index,]
mic_modified_test_xgboost <- mic_data_modified_xgboost[mic_test_index,]



###########
# Prepare Dmatrix for XGBoost
dtrain_final <- xgb.DMatrix(data = as.matrix(mic_modified_train_xgboost[,c(2:112)]), label= mic_modified_train_xgboost$LET_IS, nthread = 8)

# Fit XGBoost
fit_xgboost_mic_final <- xgb.train(
    data = dtrain_final,
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 200,
    objective = "binary:logistic",
    verbose = 0 # Set verbose=2 during development and testing
)

# Print Summary
print("================================",quote=FALSE)
print("The Details for XGBoost are", quote = FALSE)
fit_xgboost_mic_final
print("================================",quote=FALSE)

# Prepare Predictions
pred_xgboost_mic_final <-predict(object = fit_xgboost_mic_final, newdata = as.matrix(mic_modified_test_xgboost[,c(2:112)]))

pred_xgboost_mic_binary_final <- as.numeric(pred_xgboost_mic_final > 0.5)

# Print table of predictions
print("================================",quote=FALSE)
print("The table of predictions is", quote = FALSE)
table(as.factor(pred_xgboost_mic_binary_final ))
print("================================",quote=FALSE)

# Print table of actual values
print("================================",quote=FALSE)
print("The table of actual values is", quote = FALSE)
table(mic_modified_test_xgboost$LET_IS)
print("================================",quote=FALSE)

# Print overall accuracy
print("================================",quote=FALSE)
mean(pred_xgboost_mic_binary_final == mic_modified_test_xgboost$LET_IS)

# Record and print confusion matrix
print("The confusion matrix is", quote = FALSE)
print("================================",quote=FALSE)
mic_xgb_cm_bin <- confusionMatrix(data = as.factor(pred_xgboost_mic_binary_final), reference = as.factor(mic_modified_test_xgboost$LET_IS))
mic_xgb_cm_bin
print("================================",quote=FALSE)

# Remove data that is no longer required

rm(fit_xgboost_mic_final, pred_xgboost_mic_final, pred_xgboost_mic_binary_final)

rm(mic_modified_test, mic_modified_train, mic_data_modified, dtrain_final)


```

XGBoost is a lot more consistent in its behaviour for predictions for the Holdout set. The overall accuracy for predictions is slightly better and the balanced accuracy is slightly lower than for the Cross Validation set. We can see some reduction in the Specificity but the results are largely consistent.

#### Artificial Neural Networks

```{r Prepare MIC Holdout Test Dataset for expansion of Categorical Variables and binary outcome,, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}



# As previously we will use only a Single Category to classify all Deaths

# Let us use the imputed dataset as ANN cannot work with missing values 


########

# Convert nominal variables from factors to integers. Since they are binary, changing them from factors to numerics does not alter their behaviour 

mic_modified_train_set_ann <- mic_orig_train_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 

mic_modified_test_set_ann <- mic_orig_test_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 


# Expand Imputed values using Dummy Variables.

##########

# Convert LET_IS to binary values

mic_modified_train_set_ann  <- mic_modified_train_set_ann  %>% 
        mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0))) 

mic_modified_test_set_ann  <- mic_modified_test_set_ann  %>% 
        mutate( LET_IS = ifelse(LET_IS==1 | LET_IS==2 | LET_IS==3 | LET_IS==4 | LET_IS==5 | LET_IS==6 | LET_IS==7, as.integer(1), as.integer(0))) 



# Retain "LET_IS" column as we will remove it during conversion

mic_train_let_is_final_bin <- mic_modified_train_set_ann$LET_IS
mic_test_let_is_final_bin <- mic_modified_test_set_ann$LET_IS


########## ONE-HOT ENCODING USING CARET #################################

# Use formula interface to create template for dummy vars. Create dummy vars for Ordinal and Partially Ordinal variables
dumms_vars_template <- dummyVars(formula = "~.", data = mic_modified_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)], sep = "_", levelsOnly = FALSE, fullRank = TRUE)

# Create dummy vars for Training  and Testing datasets. 
mic_modified_train_set_ann_dummy <- as.data.frame(predict(dumms_vars_template, mic_modified_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_test_set_ann_dummy <- as.data.frame(predict(dumms_vars_template, mic_modified_test_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

# Collect Training dummy vars names. Same applies for Testing  
dummy_vars_names <- colnames(mic_modified_train_set_ann_dummy)

# Modify Training  and Testing  Datasets to remove the existing columns for variables related to  "ID", Ordinal,  Partially Ordinal and Complications.
mic_modified_train_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

mic_modified_test_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

# Collect Modified Training Column names. Same applies for Testing 
mic_modified_col_names <- colnames(mic_modified_train_set_ann)

# Combine Collected Modified Training column names with Training dummy vars names
mic_modified_col_names <- c(mic_modified_col_names, dummy_vars_names)

# Create combined Training  and Testing  datasets by binding together the Modified Training and Testing datasets with their respective dummy vars daatasets
mic_modified_train_set_ann <- cbind(mic_modified_train_set_ann, mic_modified_train_set_ann_dummy)

mic_modified_test_set_ann <- cbind(mic_modified_test_set_ann, mic_modified_test_set_ann_dummy)

# Assign column names to the newly created combined Training and Testing datasets
colnames(mic_modified_train_set_ann) <- mic_modified_col_names

colnames(mic_modified_test_set_ann) <- mic_modified_col_names

# Remove data that is not required anymore

rm(dumms_vars_template, mic_modified_train_set_ann_dummy, mic_modified_test_set_ann_dummy, mic_modified_col_names)
```

##### Sequential API

Let us evaluate Sequential API now

```{r Perform Final Analysis for MIC Dataset using Neural Networks after expansion using Sequential API and binary outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


########## Create ANN for Predictions #######################################

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############


feature_names <- c(mic_nominal_variables, mic_continuous_variables, dummy_vars_names)

train_features <- as.matrix(mic_modified_train_set_ann[feature_names])
train_targets <- as.matrix(mic_train_let_is_final_bin)

val_features <- as.matrix(mic_modified_test_set_ann[feature_names])
val_targets <- as.matrix(mic_test_let_is_final_bin)

####################################
# We need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
# Remove Columns from the list that produce NA when scaled

####### Normal Scaling ############
feature_names <- colnames(train_features)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

# Let us build the Nueral Network 


model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |> 
  layer_dense(units = 256, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 256, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 192, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 192, activation = "relu") |> 
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(1, activation = "sigmoid")

# Collect counts for initial weight generation
counts <- table(mic_train_let_is_final_bin) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_test_let_is_final_bin) # Counts for Validation Set

# Configure Weights. Weights are manually updated with multipliers
weight_for_0 = as.numeric(1 / counts["0"]) 
weight_for_1 = as.numeric(1 / counts["1"]) * 0.98 


# Train the Model 

metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-3),
  loss = "binary_crossentropy",
  metrics = metrics
)

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)

model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)

# Prepare Predictions

val_pred_final <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }

pred_correct_final <- mic_test_let_is_final_bin == val_pred_final

# Print table of predictions
print("================================",quote=FALSE)
print("The Table of Predicted values is", quote = FALSE)
table(as.numeric(!pred_correct_final))
print("================================",quote=FALSE)

# Print table of actual values
print("================================",quote=FALSE)
print("The table of actual values is", quote = FALSE)
table(as.factor(mic_test_let_is_final_bin))
print("================================",quote=FALSE)

# Print overall accuracy
print("================================",quote=FALSE)
print(c("The overall accuracy is : ", round(mean(pred_correct_final), digits = 4)))
print("================================",quote=FALSE)

# Record and Print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is", quote = FALSE)
mic_ann_seq_cm_bin <- confusionMatrix(data = as.factor(val_pred_final), reference = as.factor(mic_test_let_is_final_bin))
mic_ann_seq_cm_bin
print("================================",quote=FALSE)

# Collect death events
deaths_final <- mic_test_let_is_final_bin == 1

# Prepare and print summary of death events
n_deaths_detected <- sum(deaths_final & pred_correct_final)
n_deaths_missed <- sum(deaths_final & !pred_correct_final)
n_live_flagged <- sum(!deaths_final & !pred_correct_final)

print(c("deaths detected: ",n_deaths_detected), quote = FALSE)
print(c("deaths missed: ",n_deaths_missed), quote = FALSE)
print(c("survival cases flagged as deaths: ",n_live_flagged), quote = FALSE)


# Remove Data that is no longer required

rm(class_weight, metrics, train_features, train_targets, val_features, val_targets, counts, deaths_final, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct_final, val_pred_final, mic_binary_seq_api_class_weight)

rm(index_features_scaling, feature_names_for_scaling)

rm(weight_for_0, weight_for_1)

rm(mic_binary_seq_api_class_weight)
```

With ANN built using the Keras Sequential API, we are able to achieve a decent balanced accuracy and predict a higher number of deaths more accurately.

However, our balanced accuracy is getting dragged down due to the large number of survival events incorrectly predicted as lethal events. Though at first glance it may seem that we are doing worse than XGBoost, we must remind ourselves that the biggest challenge with the MIC dataset is to accurately predict the deaths or lethal events more than the survival events and we have made some incremental progress towards predicting more deaths accurately.

\newpage

##### Functional API

We evaluate Functional API next

```{r Prepare Final MIC Dataset with Holdout Set for expansion of Categorical Variables and binary outcome using Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


# Initialise Empty Vectors to collect and store respective column indices
mic_demographic_history_col_indices <- c() 
mic_infarction_col_indices <- c()
mic_emergency_icu_col_indices <- c()
mic_ecg_col_indices <- c()
mic_ft_col_indices <- c()
mic_serum_col_indices <- c()
mic_relapse_col_indices <- c()
mic_medicine_col_indices <- c() 


# Extract the Column indices and store them in respective vectors for each feature set. 
##### 
for (i in 1:(length(mic_demographic_history_features))) {
mic_demographic_history_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_demographic_history_features[i])
 mic_demographic_history_col_indices <- c(mic_demographic_history_col_indices, mic_demographic_history_col)
}
rm(mic_demographic_history_col,i)


##### 
for (i in 1:(length(mic_infarction_features))) {
mic_infarction_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_infarction_features[i])
 mic_infarction_col_indices <- c(mic_infarction_col_indices, mic_infarction_col)
}
rm(mic_infarction_col,i)


##### 
for (i in 1:(length(mic_emergency_icu_features))) {
mic_emergency_icu_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_emergency_icu_features[i])
 mic_emergency_icu_col_indices <- c(mic_emergency_icu_col_indices, mic_emergency_icu_col)
}
rm(mic_emergency_icu_col,i)

##### 
for (i in 1:(length(mic_ecg_features))) {
mic_ecg_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_ecg_features[i])
 mic_ecg_col_indices <- c(mic_ecg_col_indices, mic_ecg_col)
}
rm(mic_ecg_col,i)

##### 
for (i in 1:(length(mic_ft_features))) {
mic_ft_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_ft_features[i])
 mic_ft_col_indices <- c(mic_ft_col_indices, mic_ft_col)
}
rm(mic_ft_col,i)

##### 
for (i in 1:(length(mic_serum_features))) {
mic_serum_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_serum_features[i])
 mic_serum_col_indices <- c(mic_serum_col_indices, mic_serum_col)
}
rm(mic_serum_col,i)

##### 
for (i in 1:(length(mic_relapse_features))) {
mic_relapse_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_relapse_features[i])
 mic_relapse_col_indices <- c(mic_relapse_col_indices, mic_relapse_col)
}
rm(mic_relapse_col,i)

##### 
for (i in 1:(length(mic_medicine_features))) {
mic_medicine_col <- str_which( string = colnames(mic_modified_train_set_ann), pattern = mic_medicine_features[i])
 mic_medicine_col_indices <- c(mic_medicine_col_indices, mic_medicine_col)
}
rm(mic_medicine_col,i)


```

```{r Perform Final Analysis of MIC Dataset after expansion of Categorical Variables for binary outcome using Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


############# Create the Input Data #########################################


########## Demographic & History Features ###########################

train_features_demographic_history <- as.matrix(mic_modified_train_set_ann[,c(mic_demographic_history_col_indices)])
val_features_demographic_history <- as.matrix(mic_modified_test_set_ann[,c(mic_demographic_history_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic_history <- colnames(train_features_demographic_history)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic_history) == 0)

index_features_scaling <- which(!feature_names_demographic_history  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic_history[,index_features_scaling])

train_features_demographic_history <- train_features_demographic_history[,c(feature_names_for_scaling)]
val_features_demographic_history <- val_features_demographic_history[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic_history %<>% scale()
val_features_demographic_history %<>% 
        scale(center = attr(train_features_demographic_history, "scaled:center"),
        scale = attr(train_features_demographic_history, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Infarction Features ###########################

train_features_infarction <- as.matrix(mic_modified_train_set_ann[,c(mic_infarction_col_indices)])
val_features_infarction <- as.matrix(mic_modified_test_set_ann[,c(mic_infarction_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_infarction <- colnames(train_features_infarction)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_infarction) == 0)

index_features_scaling <- which(!feature_names_infarction  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_infarction[,index_features_scaling])

train_features_infarction <- train_features_infarction[,c(feature_names_for_scaling)]
val_features_infarction <- val_features_infarction[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_infarction %<>% scale()
val_features_infarction %<>% 
        scale(center = attr(train_features_infarction, "scaled:center"),
        scale = attr(train_features_infarction, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Emergency ICU Features ###########################

train_features_emergency_icu <- as.matrix(mic_modified_train_set_ann[,c(mic_emergency_icu_col_indices)])
val_features_emergency_icu <- as.matrix(mic_modified_test_set_ann[,c(mic_emergency_icu_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_emergency_icu <- colnames(train_features_emergency_icu)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_emergency_icu) == 0)

index_features_scaling <- which(!feature_names_emergency_icu  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_emergency_icu[,index_features_scaling])

train_features_emergency_icu <- train_features_emergency_icu[,c(feature_names_for_scaling)]
val_features_emergency_icu <- val_features_emergency_icu[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_emergency_icu %<>% scale()
val_features_emergency_icu %<>% 
        scale(center = attr(train_features_emergency_icu, "scaled:center"),
        scale = attr(train_features_emergency_icu, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## ECG Features ###########################

train_features_ecg <- as.matrix(mic_modified_train_set_ann[,c(mic_ecg_col_indices)])
val_features_ecg <- as.matrix(mic_modified_test_set_ann[,c(mic_ecg_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ecg <- colnames(train_features_ecg)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ecg) == 0)

index_features_scaling <- which(!feature_names_ecg  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ecg[,index_features_scaling])

train_features_ecg <- train_features_ecg[,c(feature_names_for_scaling)]
val_features_ecg <- val_features_ecg[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ecg %<>% scale()
val_features_ecg %<>% 
        scale(center = attr(train_features_ecg, "scaled:center"),
        scale = attr(train_features_ecg, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## FT Features ###########################

train_features_ft <- as.matrix(mic_modified_train_set_ann[,c(mic_ft_col_indices)])
val_features_ft <- as.matrix(mic_modified_test_set_ann[,c(mic_ft_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ft <- colnames(train_features_ft)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ft) == 0)

index_features_scaling <- which(!feature_names_ft  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ft[,index_features_scaling])

train_features_ft <- train_features_ft[,c(feature_names_for_scaling)]
val_features_ft <- val_features_ft[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ft %<>% scale()
val_features_ft %<>% 
        scale(center = attr(train_features_ft, "scaled:center"),
        scale = attr(train_features_ft, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Serum Features ###########################

train_features_serum <- as.matrix(mic_modified_train_set_ann[,c(mic_serum_col_indices)])
val_features_serum <- as.matrix(mic_modified_test_set_ann[,c(mic_serum_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_serum <- colnames(train_features_serum)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_serum) == 0)

index_features_scaling <- which(!feature_names_serum  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_serum[,index_features_scaling])

train_features_serum <- train_features_serum[,c(feature_names_for_scaling)]
val_features_serum <- val_features_serum[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_serum %<>% scale()
val_features_serum %<>% 
        scale(center = attr(train_features_serum, "scaled:center"),
        scale = attr(train_features_serum, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Relpase Features ###########################

train_features_relapse <- as.matrix(mic_modified_train_set_ann[,c(mic_relapse_col_indices)])
val_features_relapse <- as.matrix(mic_modified_test_set_ann[,c(mic_relapse_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_relapse <- colnames(train_features_relapse)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_relapse) == 0)

index_features_scaling <- which(!feature_names_relapse  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_relapse[,index_features_scaling])

train_features_relapse <- train_features_relapse[,c(feature_names_for_scaling)]
val_features_relapse <- val_features_relapse[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_relapse %<>% scale()
val_features_relapse %<>% 
        scale(center = attr(train_features_relapse, "scaled:center"),
        scale = attr(train_features_relapse, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Medicine Features ###########################

train_features_medicine <- as.matrix(mic_modified_train_set_ann[,c(mic_medicine_col_indices)])
val_features_medicine <- as.matrix(mic_modified_test_set_ann[,c(mic_medicine_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_medicine <- colnames(train_features_medicine)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


####### Create Training & Validation Targets #######################

train_targets <- as.matrix(mic_train_let_is_final_bin)
val_targets <- as.matrix(mic_test_let_is_final_bin)


###################################################################################

# Let us define the input shapes. 
input_shape_demographic_history  <- ncol(train_features_demographic_history)
input_shape_infarction <- ncol(train_features_infarction)
input_shape_emergency_icu <- ncol(train_features_emergency_icu)
input_shape_ecg <- ncol(train_features_ecg)
input_shape_ft <- ncol(train_features_ft)
input_shape_serum <- ncol(train_features_serum)
input_shape_relapse <- ncol(train_features_relapse)
input_shape_medicine <- ncol(train_features_medicine)


# Let us build the Keras Inputs & Features
input_demographic_history <- keras_input(shape(input_shape_demographic_history), name = "demographic_history")
input_infarction <- keras_input(shape(input_shape_infarction), name = "infarction")
input_emergency_icu <- keras_input(shape(input_shape_emergency_icu), name = "emergency_icu")
input_ecg <- keras_input(shape(input_shape_ecg), name = "ecg")
input_ft <- keras_input(shape(input_shape_ft), name = "ft")
input_serum <- keras_input(shape(input_shape_serum), name = "serum")
input_relapse <- keras_input(shape(input_shape_relapse), name = "relapse")
input_medicine <- keras_input(shape(input_shape_medicine), name = "medicine")


# Let us build the ANN Feature Layers
demographic_history_features <- 
    layer_dense(object = input_demographic_history, units = 256) |> 
    layer_dropout(rate = 0.3, seed = 1024) |>
    layer_dense(units = 128, activation = "relu") |> #128
    layer_dropout(rate = 0.3, seed = 1024) 

infarction_features <- 
    layer_dense(object = input_infarction, units = 1536, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 


emergency_icu_features <- 
    layer_dense(object = input_emergency_icu, units = 1216, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

ecg_features <- 
    layer_dense(object = input_ecg, units = 576, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

ft_features <- 
    layer_dense(object = input_ft, units = 64, activation = "relu") |>  
    layer_dropout(rate = 0.3, seed = 1024)

serum_features <- 
    layer_dense(object = input_serum, units = 64, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

relapse_features <- 
    layer_dense(object = input_relapse, units = 72, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024)

medicine_features <- 
    layer_dense(object = input_medicine, units = 240, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) |>
    layer_dense(units = 120, activation = "relu") |> #320 #512
    layer_dropout(rate = 0.3, seed = 1024)

# Let us combine the Feature Layers together

combined_features <- layer_concatenate(list(demographic_history_features,infarction_features, emergency_icu_features, ecg_features, ft_features, serum_features, relapse_features, medicine_features))

pred_functional_api <- layer_dense(object = combined_features, units = 1, activation = "sigmoid")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_demographic_history, input_infarction, input_emergency_icu, input_ecg, input_ft, input_serum, input_relapse, input_medicine),
  outputs = list(pred_functional_api)
)


# Collect counts for initial weight generation
counts <- table(mic_test_let_is_final_bin) # Counts for Training Set
# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_test_let_is_final_bin) # Counts for Validation Set

# Configure Weights. Weights are updated manually with multipliers

weight_for_0 = as.numeric(1 / counts["0"]) 
weight_for_1 = as.numeric(1 / counts["1"]) 


# Compile Model

metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
functional_api_model |> compile(
  optimizer = optimizer_adam(1e-3),
  loss = "binary_crossentropy",
  metrics = metrics
)


class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)



######  Fit model ##############


functional_api_model |> 
  fit(
  x = list(demographic_history = train_features_demographic_history, infarction = train_features_infarction, emergency_icu = train_features_emergency_icu, ecg = train_features_ecg, ft = train_features_ft, serum = train_features_serum, relapse = train_features_relapse, medicine = train_features_medicine),
  y = train_targets,
  validation_data = list(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)


# Prepare Predictions

val_pred_functional_api_final <- functional_api_model %>%
  predict(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine)) %>%
  { as.integer(. > 0.5) }

pred_correct_functional_api_final <- mic_test_let_is_final_bin == val_pred_functional_api_final

# Print table of predictions
print("================================",quote=FALSE)
print("The Table of Predicted values is", quote = FALSE)
table(as.numeric(!pred_correct_functional_api_final))
print("================================",quote=FALSE)

# Print table of actual values
print("================================",quote=FALSE)
print("The table of actual values is", quote = FALSE)
table(as.factor(mic_test_let_is_final_bin))
print("================================",quote=FALSE)

# Print overall accuracy
print("================================",quote=FALSE)
print(c("The overall accuracy is : ", round(mean(pred_correct_functional_api_final), digits = 4)), quote = FALSE)
print("================================",quote=FALSE)

# Record and print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is", quote = FALSE)
mic_ann_func_cm_bin <- confusionMatrix(data = as.factor(val_pred_functional_api_final), reference = as.factor(mic_test_let_is_final_bin))
mic_ann_func_cm_bin
print("================================",quote=FALSE)

# Collect death events 
deaths_functional_api_final <- mic_test_let_is_final_bin == 1

# Prepare and print summary of death events
n_deaths_detected <- sum(deaths_functional_api_final & pred_correct_functional_api_final)
n_deaths_missed <- sum(deaths_functional_api_final & !pred_correct_functional_api_final)
n_live_flagged <- sum(!deaths_functional_api_final & !pred_correct_functional_api_final)

print(c("deaths detected: ",n_deaths_detected), quote = FALSE)
print(c("deaths missed: ",n_deaths_missed), quote = FALSE)
print(c("survival cases flagged as deaths: ",n_live_flagged), quote = FALSE)


# Remove data that is no longer required

rm(callbacks,class_weight, metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct, val_pred, weight_for_0, weight_for_1)


rm(deaths_functional_api_final)

rm(weight_for_0, weight_for_1)
rm(mic_binary_funcational_api_class_weight)

rm(mic_train_let_is_final_bin,mic_test_let_is_final_bin)

```

With ANN built using Keras Functional API, we are getting a bit better in being able to predict lethal or death events. At the same time the number of live cases being flagged as deaths has increased quite dramatically. The Overall Accuracy and the Balanced Accuracy are quite good. We have the best Specificity as yet. Sensitivity has suffered a bit.

As I have said earlier, this ANN model is not a true reflection of the capabilities of ANN. This is more a reflection of my own capabilities in building ANN. This project is the first for me to try ANN and I have had to learn on my feet over the last month or so by looking at the Keras documentation. I strongly believe that ANN models can be built to predict with much better overall and balanced accuracies.

```{r MIC Final Analysis ANN Cleanup - Binary , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required and run garbage collector to free up memory

rm(mic_modified_cv_test_set_ann, mic_modified_cv_train_set_ann, dtrain, mic_modified_cv_test_set_ann_outcome, mic_modified_cv_train_set_ann_outcome )

gc()

```

\newpage

### Multiple Categories for Deaths

Let us now look at the predictions for our holdout set for Multiple categories of deaths. We start with XGBoost

#### XGBoost

```{r Perform final analysis of MIC dataset using XGBoost for categorical outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently. 

# Use Original Dataset without Imputation

# DO NOT CONVERT CATEGORICAL(ORDINAL) AND BINARY(NOMINAL) VARIABLES TO FACTORS
# CONVERSION USING as.matrix() CAUSES THEM TO BE COERCED TO CHARACTER VECTORS

mic_data_xgboost <- mic_data %>% 
    mutate_at(mic_continuous_variables, ~as.numeric(.))

# Create Training and Testing datasets

mic_modified_train_xgboost <- mic_data_xgboost[-mic_test_index,]
mic_modified_test_xgboost <- mic_data_xgboost[mic_test_index,]


###########

# Create a matrix that can be used by XGBoost for Training
dtrain_categorical_final <- xgb.DMatrix(data = as.matrix(mic_modified_train_xgboost[,c(2:112)]), label= mic_modified_train_xgboost$LET_IS, nthread = 8)

# Fit XGBoost
fit_xgboost_mic_categorical_final <- xgb.train(
    data = dtrain_categorical_final,
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 200,
    objective = "multi:softmax", 
    params = list("num_class" = 8, "booster" = "gbtree"),
    verbose = 0 # set verbose=2 during development, tuning and testing
)

# Print Summary of Parameters

print("=====================================", quote = FALSE)
print("The details for XGBoost are", quote = FALSE)
fit_xgboost_mic_categorical_final
print("=====================================", quote = FALSE)

# Prepare predictions
pred_xgboost_mic_categorical_final <-predict(object = fit_xgboost_mic_categorical_final, newdata = as.matrix(mic_modified_test_xgboost[,c(2:112)]))

# Print table of predictions
print("=====================================", quote = FALSE)
print("The table of predictions is", quote = FALSE)
table(as.factor(pred_xgboost_mic_categorical_final ))
print("=====================================")

# Print table of actual values
print("=====================================", quote = FALSE)
print("The table of actual values is", quote = FALSE)
table(mic_modified_test_xgboost$LET_IS)
print("=====================================", quote = FALSE)

# Print overall accuracy
print("The accuracy of predictions is", quote = FALSE)
mean(pred_xgboost_mic_categorical_final == mic_modified_test_xgboost$LET_IS)
print("=====================================", quote = FALSE)

# Record and print confusion matrix
print("The confusion matrix is", quote = FALSE)
print("=====================================", quote = FALSE)
mic_xgb_cm_multi_final <- confusionMatrix(data = as.factor(pred_xgboost_mic_categorical_final), reference = as.factor(mic_modified_test_xgboost$LET_IS))
mic_xgb_cm_multi_final
print("=====================================", quote = FALSE)

# Extract and Print Summary of Death Events
print("================================",quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(mic_xgb_cm_multi_final$table[2,2], mic_xgb_cm_multi_final$table[3,3], mic_xgb_cm_multi_final$table[4,4], mic_xgb_cm_multi_final$table[5,5], mic_xgb_cm_multi_final$table[6,6], mic_xgb_cm_multi_final$table[7,7], mic_xgb_cm_multi_final$table[8,8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(mic_xgb_cm_multi_final$table[2:8,2:8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Live cases flagged as Deaths is :",quote=FALSE)
sum(mic_xgb_cm_multi_final$table[2:8,1])
print("================================",quote=FALSE)

# Remove Data that is no longer required
rm(fit_xgboost_mic_categorical_final, dtrain_categorical_final)

rm(mic_data_modified, dtrain, mic_modified_test_xgboost, mic_modified_train_xgboost)

rm(mic_data_modified_xgboost, mic_data_xgboost)
```

As usual XGBoost is a consistent performer. XGboost is able to correctly detect and categorise 18 deaths while being able to correctly detect 27 deaths. The overall accuracy is extremely good but comes at the huge cost of predicting more than half of the deaths as live cases, which is not what we want in this particular case.

However, the shape of our predictions has improved quite a bit from the initial analysis.

\newpage

#### Artificial Neural Networks

We now turn our attention to ANN built using Sequential API

```{r Prepare MIC Dataset for Final Analysis and multiple outcomes, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Use prviously created Lists of Continuous, Ordinal (Categorical) and Nominal (Binary) Features to avoid duplication and errors. 


########

# Convert nominal variables from factors to integers. Since they are binary, changing them from factors to numerics does not alter their behaviour 

mic_modified_train_set_ann <- mic_orig_train_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 

mic_modified_test_set_ann <- mic_orig_test_imputed_mice %>% 
          mutate_at(c(mic_nominal_variables),~(.=as.integer(.))) 


#############
# Expand Imputed values using Dummy Variables.

# Retain "LET_IS" column as it will be lost during conversion

mic_train_let_is_final_multi <- mic_modified_train_set_ann$LET_IS
mic_test_let_is_final_multi <- mic_modified_test_set_ann$LET_IS

# Set up Training and Testing sets for one hot encoding. Convert LET_IS to a factor

mic_modified_train_set_ann %<>% mutate(LET_IS = as.factor(LET_IS))

mic_modified_test_set_ann %<>% mutate(LET_IS = as.factor(LET_IS))


########## ONE-HOT ENCODING USING CARET FOR PREDICTORS AND OUTCOME ###################

# We will use CARET for creating the dummy variables for all the predictors and the outcome as it support both ordered and unordered factors 

# We will two seperate instances of dummyVars for creations of the dummy variables for the features and the outcome. We can do it using a single instance but the code is very confusing and difficult to follow.

# Use formula interface to create template for dummy vars. 

dummy_vars_features <- dummyVars(formula = "~.", data = mic_modified_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)], sep = "_", levelsOnly = FALSE, fullRank = FALSE)

dummy_vars_outcome <- dummyVars(formula = "~.", data = mic_modified_train_set_ann["LET_IS"], sep = "_", levelsOnly = FALSE, fullRank = FALSE)

# Create dummy vars for Training and Testing datasets. 
mic_modified_train_set_ann_dummy <- as.data.frame(predict(dummy_vars_features, mic_modified_train_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_test_set_ann_dummy <- as.data.frame(predict(dummy_vars_features, mic_modified_test_set_ann[,c(mic_ordinal_variables, mic_part_ordinal_variables)]))

mic_modified_train_set_ann_outcome <-  as.data.frame(predict(dummy_vars_outcome, mic_modified_train_set_ann["LET_IS"]))

mic_modified_test_set_ann_outcome <- as.data.frame(predict(dummy_vars_outcome, mic_modified_test_set_ann["LET_IS"]))

# Collect Training dummy vars names. Same applies for Testing CV. 
dummy_vars_names <- colnames(mic_modified_train_set_ann_dummy)

# Modify Training CV and Testing CV Datasets to remove the existing columns for variables related to  "ID", Ordinal,  Partially Ordinal, Complications and "LET_IS".

mic_modified_train_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))

mic_modified_test_set_ann %<>% select(-c("ID",mic_ordinal_variables,mic_part_ordinal_variables, mic_complications,"LET_IS"))



# Collect Modified Training Column names. Same applies for Testing dataset
mic_modified_col_names <- colnames(mic_modified_train_set_ann)

# Combine Collected Modified Training column names with Training dummy vars names for predictors

mic_modified_col_names <- c(mic_modified_col_names, dummy_vars_names)

# Create combined Training and Testing datasets by binding together the Modified Training and Testing datasets with the respective dummy vars daatasets
mic_modified_train_set_ann <- cbind(mic_modified_train_set_ann, mic_modified_train_set_ann_dummy)

mic_modified_test_set_ann <- cbind(mic_modified_test_set_ann, mic_modified_test_set_ann_dummy)

# Assign column names to the newly created combined Training and Testing datasets
colnames(mic_modified_train_set_ann) <- mic_modified_col_names

colnames(mic_modified_test_set_ann) <- mic_modified_col_names


# Remove data that is not required anymore

rm(dummy_vars_features, dummy_vars_outcome, mic_modified_train_set_ann_dummy, mic_modified_test_set_ann_dummy, mic_modified_col_names)
```

##### Sequential API

```{r Perform Final Analysis for MIC Dataset using Neural Networks with Categories for outcome LET_IS using Sequential API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############
feature_names <- c(mic_nominal_variables, mic_continuous_variables, dummy_vars_names)

train_features <- as.matrix(mic_modified_train_set_ann[feature_names])
train_targets <- as.matrix(mic_modified_train_set_ann_outcome)

val_features <- as.matrix(mic_modified_test_set_ann[feature_names])
val_targets <- as.matrix(mic_modified_test_set_ann_outcome)

####################################
# As earlier, We need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
# As earlier, we also need to remove Columns from the list that produce NA when scaled

####### Normal Scaling ############
feature_names <- colnames(train_features)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

####### Normal Scaling ############
train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

# Let us build the ANN

model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(units = 3072) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 3072) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 2304) |>
  layer_dropout(rate = 0.3, seed = 1024) |>
  layer_dense(units = 8, activation = 'softmax')


# Collect counts for initial weight generation
counts <- table(mic_train_let_is_final_multi) # Counts for Training Set
# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_test_let_is_final_multi) # Counts for Validation Set

# Configure Weights. Weights are updated manually with multipliers
weight_for_0 = as.numeric(1 / counts["0"])*2.6
weight_for_1 = as.numeric(1 / counts["1"])*0.225
weight_for_2 = as.numeric(1 / counts["2"])*0.05
weight_for_3 = as.numeric(1 / counts["3"])*0.25
weight_for_4 = as.numeric(1 / counts["4"])*0.05
weight_for_5 = as.numeric(1 / counts["5"])*0.0750
weight_for_6 = as.numeric(1 / counts["6"])*0.0875
weight_for_7 = as.numeric(1 / counts["7"])*0.06125

# Train the Model 

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy','categorical_accuracy')
)

##############

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1,
                     "2" = weight_for_2,
                     "3" = weight_for_3,
                     "4" = weight_for_4,
                     "5" = weight_for_5,
                     "6" = weight_for_6,
                     "7" = weight_for_7)




model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  batch_size = 2048,
  class_weight = class_weight,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)


# Evaluate Model 
# Commented out for Report creation
# model |> evaluate(val_features, val_targets)

# Prepare Predictions

probs <- model |> predict(val_features)

pred_ann_seq_api_multi <- max.col(probs) - 1L

# Print table of predictions
print("================================",quote=FALSE)
print("The Table of Predicted values is",quote=FALSE)
table(as.factor(pred_ann_seq_api_multi))
print("================================",quote=FALSE)

# Print table of actual values
print("=====================================",quote=FALSE)
print("The table of actual values is",quote=FALSE)
table(as.factor(mic_test_let_is_final_multi))
print("================================",quote=FALSE)

# Print overall accuracy
print("================================",quote=FALSE)
print("The overall accuracy is : ",quote=FALSE)
mean(mic_test_let_is_final_multi == pred_ann_seq_api_multi)

# Record and print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is",quote=FALSE)
mic_ann_seq_cm_multi_final <- confusionMatrix(data = as.factor(pred_ann_seq_api_multi), reference = as.factor(mic_test_let_is_final_multi))
mic_ann_seq_cm_multi_final
print("================================")

# Extract and print summary of death events
print("================================",quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(mic_ann_seq_cm_multi_final$table[2,2], mic_ann_seq_cm_multi_final$table[3,3], mic_ann_seq_cm_multi_final$table[4,4], mic_ann_seq_cm_multi_final$table[5,5], mic_ann_seq_cm_multi_final$table[6,6], mic_ann_seq_cm_multi_final$table[7,7], mic_ann_seq_cm_multi_final$table[8,8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(mic_ann_seq_cm_multi_final$table[2:8,2:8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Live cases flagged as Deaths is :",quote=FALSE)
sum(mic_ann_seq_cm_multi_final$table[2:8,1])
print("================================",quote=FALSE)

# Remove data that is no longer required
rm(class_weight, metrics, train_features, train_targets, val_features, val_targets, counts, deaths, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct, val_pred)

rm(index_features_scaling, feature_names_for_scaling, mic_seq_api_class_weight)

rm(probs)

rm(weight_for_0, weight_for_1, weight_for_2, weight_for_3, weight_for_4, weight_for_5, weight_for_6, weight_for_7)

```

The Results are largely as expected. The results are a lot worse than they are for binary predictions. We are not able to predict across the whole range too. The prediction accuracy for the Holdout set is again a lot worse than that for the cross validation set reflecting the complexity of the dataset.

However, we have made some progress in the number of deaths correctly detected and categorised as well as the number of deaths detected. This has come at the cost of flagging many live cases as deaths.

##### Functional API

Let us now check how ANN built using Functional API perform

```{r Perform Initial Analysis for MIC Dataset using Neural Networks with Categories for outcome LET_IS with Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, , fig.align='center'}


########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################



############# Create the Input Data #########################################


########## Demographic & History Features ###########################

train_features_demographic_history <- as.matrix(mic_modified_train_set_ann[,c(mic_demographic_history_col_indices)])
val_features_demographic_history <- as.matrix(mic_modified_test_set_ann[,c(mic_demographic_history_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic_history <- colnames(train_features_demographic_history)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic_history) == 0)

index_features_scaling <- which(!feature_names_demographic_history  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic_history[,index_features_scaling])

train_features_demographic_history <- train_features_demographic_history[,c(feature_names_for_scaling)]
val_features_demographic_history <- val_features_demographic_history[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic_history %<>% scale()
val_features_demographic_history %<>% 
        scale(center = attr(train_features_demographic_history, "scaled:center"),
        scale = attr(train_features_demographic_history, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )



########## Infarction Features ###########################

train_features_infarction <- as.matrix(mic_modified_train_set_ann[,c(mic_infarction_col_indices)])
val_features_infarction <- as.matrix(mic_modified_test_set_ann[,c(mic_infarction_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_infarction <- colnames(train_features_infarction)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_infarction) == 0)

index_features_scaling <- which(!feature_names_infarction  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_infarction[,index_features_scaling])

train_features_infarction <- train_features_infarction[,c(feature_names_for_scaling)]
val_features_infarction <- val_features_infarction[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_infarction %<>% scale()
val_features_infarction %<>% 
        scale(center = attr(train_features_infarction, "scaled:center"),
        scale = attr(train_features_infarction, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )



########## Emergency ICU Features ###########################

train_features_emergency_icu <- as.matrix(mic_modified_train_set_ann[,c(mic_emergency_icu_col_indices)])
val_features_emergency_icu <- as.matrix(mic_modified_test_set_ann[,c(mic_emergency_icu_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_emergency_icu <- colnames(train_features_emergency_icu)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_emergency_icu) == 0)

index_features_scaling <- which(!feature_names_emergency_icu  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_emergency_icu[,index_features_scaling])

train_features_emergency_icu <- train_features_emergency_icu[,c(feature_names_for_scaling)]
val_features_emergency_icu <- val_features_emergency_icu[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_emergency_icu %<>% scale()
val_features_emergency_icu %<>% 
        scale(center = attr(train_features_emergency_icu, "scaled:center"),
        scale = attr(train_features_emergency_icu, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## ECG Features ###########################

train_features_ecg <- as.matrix(mic_modified_train_set_ann[,c(mic_ecg_col_indices)])
val_features_ecg <- as.matrix(mic_modified_test_set_ann[,c(mic_ecg_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ecg <- colnames(train_features_ecg)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ecg) == 0)

index_features_scaling <- which(!feature_names_ecg  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ecg[,index_features_scaling])

train_features_ecg <- train_features_ecg[,c(feature_names_for_scaling)]
val_features_ecg <- val_features_ecg[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ecg %<>% scale()
val_features_ecg %<>% 
        scale(center = attr(train_features_ecg, "scaled:center"),
        scale = attr(train_features_ecg, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## FT Features ###########################

train_features_ft <- as.matrix(mic_modified_train_set_ann[,c(mic_ft_col_indices)])
val_features_ft <- as.matrix(mic_modified_test_set_ann[,c(mic_ft_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_ft <- colnames(train_features_ft)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_ft) == 0)

index_features_scaling <- which(!feature_names_ft  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_ft[,index_features_scaling])

train_features_ft <- train_features_ft[,c(feature_names_for_scaling)]
val_features_ft <- val_features_ft[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_ft %<>% scale()
val_features_ft %<>% 
        scale(center = attr(train_features_ft, "scaled:center"),
        scale = attr(train_features_ft, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Serum Features ###########################

train_features_serum <- as.matrix(mic_modified_train_set_ann[,c(mic_serum_col_indices)])
val_features_serum <- as.matrix(mic_modified_test_set_ann[,c(mic_serum_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_serum <- colnames(train_features_serum)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_serum) == 0)

index_features_scaling <- which(!feature_names_serum  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_serum[,index_features_scaling])

train_features_serum <- train_features_serum[,c(feature_names_for_scaling)]
val_features_serum <- val_features_serum[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_serum %<>% scale()
val_features_serum %<>% 
        scale(center = attr(train_features_serum, "scaled:center"),
        scale = attr(train_features_serum, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Relapse Features ###########################

train_features_relapse <- as.matrix(mic_modified_train_set_ann[,c(mic_relapse_col_indices)])
val_features_relapse <- as.matrix(mic_modified_test_set_ann[,c(mic_relapse_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_relapse <- colnames(train_features_relapse)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_relapse) == 0)

index_features_scaling <- which(!feature_names_relapse  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_relapse[,index_features_scaling])

train_features_relapse <- train_features_relapse[,c(feature_names_for_scaling)]
val_features_relapse <- val_features_relapse[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_relapse %<>% scale()
val_features_relapse %<>% 
        scale(center = attr(train_features_relapse, "scaled:center"),
        scale = attr(train_features_relapse, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Medicine Features ###########################

train_features_medicine <- as.matrix(mic_modified_train_set_ann[,c(mic_medicine_col_indices)])
val_features_medicine <- as.matrix(mic_modified_test_set_ann[,c(mic_medicine_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_medicine <- colnames(train_features_medicine)

mic_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(mic_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(mic_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )



################# CREATE TRAINING AND VALIDATION TARGETS ########################

train_targets <- as.matrix(mic_modified_train_set_ann_outcome)
val_targets <- as.matrix(mic_modified_test_set_ann_outcome)

###################################################################################

# Let us define the input shapes. 
input_shape_demographic_history  <- ncol(train_features_demographic_history)
input_shape_infarction <- ncol(train_features_infarction)
input_shape_emergency_icu <- ncol(train_features_emergency_icu)
input_shape_ecg <- ncol(train_features_ecg)
input_shape_ft <- ncol(train_features_ft)
input_shape_serum <- ncol(train_features_serum)
input_shape_relapse <- ncol(train_features_relapse)
input_shape_medicine <- ncol(train_features_medicine)



# Let us build the Keras Inputs & Features
input_demographic_history <- keras_input(shape(input_shape_demographic_history), name = "demographic_history")
input_infarction <- keras_input(shape(input_shape_infarction), name = "infarction")
input_emergency_icu <- keras_input(shape(input_shape_emergency_icu), name = "emergency_icu")
input_ecg <- keras_input(shape(input_shape_ecg), name = "ecg")
input_ft <- keras_input(shape(input_shape_ft), name = "ft")
input_serum <- keras_input(shape(input_shape_serum), name = "serum")
input_relapse <- keras_input(shape(input_shape_relapse), name = "relapse")
input_medicine <- keras_input(shape(input_shape_medicine), name = "medicine")

# Let us build the ANN
########################################

demographic_history_features <- 
    layer_dense(object = input_demographic_history, units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dropout(rate = 0.3, seed = 1024) 

infarction_features <- 
    layer_dense(object = input_infarction, units = 2048) |>
    layer_dense(units = 2048) |>
    layer_dropout(rate = 0.3, seed = 1024) 

emergency_icu_features <- 
    layer_dense(object = input_emergency_icu, units = 1280) |>
    layer_dense(units = 1280) |>
    layer_dense(units = 1280) |>
    layer_dense(units = 1280) |>
    layer_dropout(rate = 0.3, seed = 1024)

ecg_features <- 
    layer_dense(object = input_ecg, units = 1024) |> 
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dense(units = 1024) |>
    layer_dropout(rate = 0.3, seed = 1024)
      
ft_features <- 
    layer_dense(object = input_ft, units = 64) |>  
    layer_dropout(rate = 0.3, seed = 1024)    

serum_features <- 
    layer_dense(object = input_serum, units = 64) |> 
    layer_dropout(rate = 0.3, seed = 1024)

relapse_features <- 
    layer_dense(object = input_relapse, units = 96) |> 
    layer_dropout(rate = 0.3, seed = 1024) 

medicine_features <- 
    layer_dense(object = input_medicine, units = 960) |> 
    layer_dense(units = 960) |>
    layer_dense(units = 960) |>
    layer_dense(units = 960) |>
    layer_dropout(rate = 0.3, seed = 1024)


########################################


# Let us combine the Feature Layers together

combined_features <- layer_concatenate(list(demographic_history_features, infarction_features, emergency_icu_features, ecg_features, ft_features, serum_features, relapse_features, medicine_features))



pred_functional_api <- layer_dense(object = combined_features, units = 8, activation = "softmax")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_demographic_history, input_infarction, input_emergency_icu, input_ecg, input_ft, input_serum, input_relapse, input_medicine),
  outputs = list(pred_functional_api)
)



# Collect counts for initial weight generation
counts <- table(mic_train_let_is_final_multi) # Counts for Training Set
# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(mic_test_let_is_final_multi) # Counts for Validation Set

# Configure weights. Weights are updated manually with multipliers
weight_for_0 = as.numeric(1 / counts["0"])*3.5
weight_for_1 = as.numeric(1 / counts["1"])*1.1
weight_for_2 = as.numeric(1 / counts["2"])*0.2
weight_for_3 = as.numeric(1 / counts["3"])*0.7
weight_for_4 = as.numeric(1 / counts["4"])*0.15
weight_for_5 = as.numeric(1 / counts["5"])*0.3
weight_for_6 = as.numeric(1 / counts["6"])*0.3
weight_for_7 = as.numeric(1 / counts["7"])*0.4



# Train the Model 

functional_api_model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy','categorical_accuracy')
)

##############
class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1,
                     "2" = weight_for_2,
                     "3" = weight_for_3,
                     "4" = weight_for_4,
                     "5" = weight_for_5,
                     "6" = weight_for_6,
                     "7" = weight_for_7)

######  Fit model ##############

functional_api_model |> 
  fit(
  x = list(demographic_history = train_features_demographic_history, infarction = train_features_infarction, emergency_icu = train_features_emergency_icu, ecg = train_features_ecg, ft = train_features_ft, serum = train_features_serum, relapse = train_features_relapse, medicine = train_features_medicine ),
  y = train_targets,
  validation_data = list(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets),
  batch_size = 2048,
  epochs = 30,
  class_weight = class_weight, 
  verbose = 0 # Set verbose=2 during development, tuning and testing
)


# Evaluate Model
# Commented out for report creation
# functional_api_model |> evaluate(list(val_features_demographic_history, val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine),val_targets)

# Prepare Predictions

probs <- functional_api_model |> predict(list(val_features_demographic_history,  val_features_infarction, val_features_emergency_icu, val_features_ecg, val_features_ft, val_features_serum, val_features_relapse, val_features_medicine))

pred_ann_func_api_multi <- max.col(probs) - 1L

# Print table of predicted values
print("================================",quote=FALSE)
print("The Table of Predicted values is", quote = FALSE)
table(as.factor(pred_ann_func_api_multi))
print("================================",quote=FALSE)

# Print table of actual values
print("================================",quote=FALSE)
print("The Table of actual values is", quote = FALSE)
table(as.factor(mic_test_let_is_final_multi))

# Print overall accuracy
print("================================",quote=FALSE)
print("The overall accuracy is", quote = FALSE)
mean(mic_test_let_is_final_multi  == pred_ann_func_api_multi)

# Record and print confusion matrix
print("================================",quote=FALSE)
print("The Confusion Matrix is", quote = FALSE)
mic_ann_func_cm_multi_final <- confusionMatrix(data = as.factor(pred_ann_func_api_multi), reference = as.factor(mic_test_let_is_final_multi))
mic_ann_func_cm_multi_final
print("================================",quote=FALSE)

# Extract and print summary of death events
print("================================",quote=FALSE)
print("The Number of Deaths correctly categorised is :",quote=FALSE)
sum(mic_ann_func_cm_multi_final$table[2,2], mic_ann_func_cm_multi_final$table[3,3], mic_ann_func_cm_multi_final$table[4,4], mic_ann_func_cm_multi_final$table[5,5], mic_ann_func_cm_multi_final$table[6,6], mic_ann_func_cm_multi_final$table[7,7], mic_ann_func_cm_multi_final$table[8,8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Deaths correctly detected is :",quote=FALSE)
sum(mic_ann_func_cm_multi_final$table[2:8,2:8])
print("================================",quote=FALSE)

print("================================",quote=FALSE)
print("The Number of Live cases flagged as Deaths is :",quote=FALSE)
sum(mic_ann_func_cm_multi_final$table[2:8,1])
print("================================",quote=FALSE)

# Remove data that is no longer required

rm(metrics, counts, deaths_functional_api_final, feature_names, model, n_deaths_detected, n_deaths_missed, n_live_flagged, pred_correct_functional_api_final, val_pred_functional_api_final)

rm( demographic_history_features, ecg_features, emergency_icu_features, ft_features,   infarction_features, medicine_features, relapse_features, serum_features, combined_features)

rm( input_demographic_history, input_ecg, input_emergency_icu,  input_ft,  input_infarction, input_medicine, input_relapse, input_serum)

rm( train_features_demographic_history, train_features_ecg, train_features_emergency_icu,  train_features_ft,   train_features_infarction, train_features_medicine, train_features_relapse, train_features_serum)

rm( val_features_demographic_history, val_features_ecg, val_features_emergency_icu, val_features_ft,  val_features_infarction, val_features_medicine, val_features_relapse, val_features_serum)

rm( feature_names_demographic_history, feature_names_ecg, feature_names_emergency_icu,  feature_names_ft, feature_names_infarction, feature_names_medicine, feature_names_relapse, feature_names_serum)

rm( input_shape_demographic_history, input_shape_ecg, input_shape_emergency_icu, input_shape_ft,   input_shape_infarction, input_shape_medicine, input_shape_relapse, input_shape_serum)

rm( mic_demographic_history_col_indices, mic_ecg_col_indices, mic_emergency_icu_col_indices,  mic_ft_col_indices,   mic_infarction_col_indices, mic_medicine_col_indices, mic_relapse_col_indices, mic_serum_col_indices)

rm(functional_api_model, probs)

rm(class_weight)

rm(train_features, train_targets, val_features, val_targets)

rm(mic_functional_api_class_weight)

rm(weight_for_0, weight_for_1, weight_for_2, weight_for_3, weight_for_4, weight_for_5, weight_for_6, weight_for_7)


```

The ANN built using the Functional API offer a middle-ground between the predictions from XGBoost and ANN built with the Sequential API. They offer better accuracies than XGBoost in the number of deaths detected. They also offer lower number of number of lives cases flagged as deaths when compared to ANN built with Sequential API.

Where ANN using Functional API really provide us with some advantage is in the shape of the predictions, they offer much better shape to the predictions that XGBoost or ANN built using Sequential API.

\newpage

## Results & Inference

The MIC dataset is a lot more complex and has more features when compared to the HFP dataset. It also has a much smaller number of observations to work with, making it more difficult to extract patterns for predictions.

We have also seen that the predictions for the Validation set and the Holdout sets also vary by much.

### Additional Observations

Let us record some additional observations that apply to both the binary outcome and the multi category outcome.

1.  The choice of the validation and holdout sets is more providential than intentional as I have used the same Random Number Generator seed (1024) throughout for keeping the code and scheme simple and consistent. However, the holdout set that results from using this seed is actually quite a difficult one to make predictions against.
2.  **It is easy to modify the seed and choose different validation and holdout sets.** **For those who are curious.** **I can recommend setting the seed to 4096 for partitioning the MIC dataset between Training and Testing sets. You can see a dramatic improvement in the Predictions offered by some of the Algorithms like ANN and Naive Bayes.**
3.  With ANN, depending on the way the ANN is built, the predictions can vary a lot when trained using different sets. Another situation to watch out for with ANN is that when they are very well optimised for the Validation set, they often perform poorly against the Test sets. The behaviour is similar to what we can see with over-training or over-fitting.
4.  When ANN are built using unordered factors as compared to using Ordinals (ordered factors), they can be built to offer very high accuracies but unfortunately they lack consistency in their predictions, and the predictions between the validation and holdout sets vary a lot. Using Ordinals makes the predictions a lot more consistent.
5.  XGBoost and ANN have a lot of tuning parameters that can greatly enhance their accuracy. Whatever I have built are just the most basic for purposes of comparison. With the right amount of skill and time, they can be tuned to offer far more accurate predictions.

Let us summarise and review our analysis so far

### Single Category for Deaths

```{r MIC Results Summary - Binary Outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Print Survival Rate for reference

print(c("The Survival Rate for patients on this dataset is: ", survival_rate),quote = FALSE)


# Prepare final results table for Binary outcome

mic_summary_bin <- data.frame(c("Naive Bayes without Imputation","Naive Bayes with Imputation", "XGBoost", "ANN Sequential API", "ANN Functional API"), c( mic_nb_cm_bin_2$overall["Accuracy"], mic_nb_wi_cm_bin_1$overall["Accuracy"], mic_xgb_cm_bin$overall["Accuracy"], mic_ann_seq_cm_bin$overall["Accuracy"], mic_ann_func_cm_bin$overall["Accuracy"]), c( mic_nb_cm_bin_2$byClass["Balanced Accuracy"], mic_nb_wi_cm_bin_1$byClass["Balanced Accuracy"], mic_xgb_cm_bin$byClass["Balanced Accuracy"], mic_ann_seq_cm_bin$byClass["Balanced Accuracy"], mic_ann_func_cm_bin$byClass["Balanced Accuracy"]), c( mic_nb_cm_bin_2$table[2,2], mic_nb_wi_cm_bin_1$table[2,2],  mic_xgb_cm_bin$table[2,2], mic_ann_seq_cm_bin$table[2,2], mic_ann_func_cm_bin$table[2,2]), c( mic_nb_cm_bin_2$table[1,2], mic_nb_wi_cm_bin_1$table[1,2], mic_xgb_cm_bin$table[1,2], mic_ann_seq_cm_bin$table[1,2], mic_ann_func_cm_bin$table[1,2]),c( mic_nb_cm_bin_2$table[2,1], mic_nb_wi_cm_bin_1$table[2,1], mic_xgb_cm_bin$table[2,1], mic_ann_seq_cm_bin$table[2,1], mic_ann_func_cm_bin$table[2,1]))

# Print results 
knitr::kable(x = mic_summary_bin, col.names = c("Model", "overall accuracy", "balanced accuracy", "deaths detected", "deaths missed", "live flagged"), caption = "Myocardial Infarction Complications - Results Summary - Binary Outcome ", digits = 4) %>% kable_styling(font_size = 8)

```

When it comes to predictions with a simple binary outcome, we can see that XGBoost does a grand job in terms of overall accuracy and also has very good balanced accuracy. It does a pretty good job in terms of the deaths detected. For anybody who is looking for a solution that is quick and requires minimal effort to configure and run, XGBoost is the right choice.

ANN do a better job in terms of balanced accuracy. As stated before many times, the ANN that I have built are just very basic and rudimentary. With the right knowledge and skills, ANN can be built to be far more accurate.

XGBoost also has many tuning parameters and can provide further improvements in accuracy. as well

### Multiple Categories for Deaths

```{r MIC Results Summary - Categorical Outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Prepare final results table for categorical outcome
mic_summary_multi <- data.frame(c("XGBoost", "ANN Sequential API", "ANN Functional API"), c(mic_xgb_cm_multi_final$overall["Accuracy"], mic_ann_seq_cm_multi_final$overall["Accuracy"], mic_ann_func_cm_multi_final$overall["Accuracy"]), c(mic_xgb_cm_multi_final$table[2,2] + mic_xgb_cm_multi_final$table[3,3] + mic_xgb_cm_multi_final$table[4,4] + mic_xgb_cm_multi_final$table[5,5] + mic_xgb_cm_multi_final$table[6,6] + mic_xgb_cm_multi_final$table[7,7] + mic_xgb_cm_multi_final$table[8,8] , mic_ann_seq_cm_multi_final$table[2,2] + mic_ann_seq_cm_multi_final$table[3,3] + mic_ann_seq_cm_multi_final$table[4,4] + mic_ann_seq_cm_multi_final$table[5,5] + mic_ann_seq_cm_multi_final$table[6,6] + mic_ann_seq_cm_multi_final$table[7,7] + mic_ann_seq_cm_multi_final$table[8,8], mic_ann_func_cm_multi_final$table[2,2] + mic_ann_func_cm_multi_final$table[3,3] + mic_ann_func_cm_multi_final$table[4,4] + mic_ann_func_cm_multi_final$table[5,5] + mic_ann_func_cm_multi_final$table[6,6] + mic_ann_func_cm_multi_final$table[7,7] + mic_ann_func_cm_multi_final$table[8,8]) , c(sum(mic_xgb_cm_multi_final$table[2:8,2:8]), sum(mic_ann_seq_cm_multi_final$table[2:8,2:8]), sum(mic_ann_func_cm_multi_final$table[2:8,2:8])), c(sum(mic_xgb_cm_multi_final$table[1,2:8]), sum(mic_ann_seq_cm_multi_final$table[1,2:8]), sum(mic_ann_func_cm_multi_final$table[1,2:8])), c(sum(mic_xgb_cm_multi_final$table[2:8,1]), sum(mic_ann_seq_cm_multi_final$table[2:8,1]), sum(mic_ann_func_cm_multi_final$table[2:8,1])))

# Print results 
knitr::kable(x = mic_summary_multi, col.names = c("Model", "Accuracy", "Deaths Accurately Categorised", "Deaths Detected", "Deaths Missed", "Live Cases Flagged"), caption = "MIC - Results Summary - Categorical Outcome ", digits = 4) %>% kable_styling(font_size = 8)


```

#### Distributions of Predictions

Let us look at the distribution of predictions between the various algorithms

```{r MIC Results Summary - Categorical Outcome - Distributions of predictions, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Prepare Tables of Actual and Predicted values for easy visualisation

mic_test_let_is_final_multi_table <- data.frame(table(mic_test_let_is_final_multi))
colnames(mic_test_let_is_final_multi_table) <- c("LET_IS","Actual Values")

pred_xgboost_mic_categorical_final_table <- data.frame(table(pred_xgboost_mic_categorical_final))
colnames(pred_xgboost_mic_categorical_final_table) <- c("LET_IS","XGBoost")

pred_ann_seq_api_multi_table <- data.frame(table(pred_ann_seq_api_multi))
colnames(pred_ann_seq_api_multi_table) <- c("LET_IS","ANN Sequential API")

pred_ann_func_api_multi_table <- data.frame(table(pred_ann_func_api_multi))
colnames(pred_ann_func_api_multi_table) <- c("LET_IS","ANN Functional API")

mic_distributions_multi <- mic_test_let_is_final_multi_table %>% 
    left_join(pred_xgboost_mic_categorical_final_table, by = "LET_IS") %>% 
    left_join(pred_ann_seq_api_multi_table, by = "LET_IS") %>% 
    left_join(pred_ann_func_api_multi_table, by = "LET_IS") %>%
    replace_na(repl = 0)


# Print distributions

knitr::kable(x = mic_distributions_multi, col.names = c("LET_IS","Actual Values", "XGBoost", "ANN Sequential API", "ANN Functional API"), caption = "MIC - Table of distributions") %>% kable_styling(font_size = 8)

# Compile summary of actual values and predictions

mic_multi_accuracy_final <- data.frame(c(mic_test_let_is_final_multi_table[1,1], mic_test_let_is_final_multi_table[2,1], mic_test_let_is_final_multi_table [3,1], mic_test_let_is_final_multi_table [4,1], mic_test_let_is_final_multi_table [5,1], mic_test_let_is_final_multi_table [6,1], mic_test_let_is_final_multi_table [7,1], mic_test_let_is_final_multi_table [8,1]), c(mic_test_let_is_final_multi_table[1,2], mic_test_let_is_final_multi_table[2,2], mic_test_let_is_final_multi_table [3,2], mic_test_let_is_final_multi_table [4,2], mic_test_let_is_final_multi_table [5,2], mic_test_let_is_final_multi_table [6,2], mic_test_let_is_final_multi_table [7,2], mic_test_let_is_final_multi_table [8,2]), c(mic_xgb_cm_multi_final$table[1,1], mic_xgb_cm_multi_final$table[2,2] , mic_xgb_cm_multi_final$table[3,3], mic_xgb_cm_multi_final$table[4,4], mic_xgb_cm_multi_final$table[5,5],  mic_xgb_cm_multi_final$table[6,6], mic_xgb_cm_multi_final$table[7,7], mic_xgb_cm_multi_final$table[8,8]), c(mic_ann_seq_cm_multi_final$table[1,1], mic_ann_seq_cm_multi_final$table[2,2], mic_ann_seq_cm_multi_final$table[3,3], mic_ann_seq_cm_multi_final$table[4,4], mic_ann_seq_cm_multi_final$table[5,5], mic_ann_seq_cm_multi_final$table[6,6],  mic_ann_seq_cm_multi_final$table[7,7], mic_ann_seq_cm_multi_final$table[8,8]), c(mic_ann_func_cm_multi_final$table[1,1], mic_ann_func_cm_multi_final$table[2,2], mic_ann_func_cm_multi_final$table[3,3], mic_ann_func_cm_multi_final$table[4,4], mic_ann_func_cm_multi_final$table[5,5], mic_ann_func_cm_multi_final$table[6,6],  mic_ann_func_cm_multi_final$table[7,7], mic_ann_func_cm_multi_final$table[8,8]))

# Print summary

knitr::kable(x = mic_multi_accuracy_final, col.names = c("LET_IS", "Actual Values", "XGBoost", "ANN Sequential API", "ANN Functional API"),caption = "MIC - Table of Accurate Predictions") %>% kable_styling(font_size = 8)

# Remove data that is no longer required

rm(mic_test_let_is_final_multi_table, pred_xgboost_mic_categorical_final_table,pred_ann_seq_api_multi_table, pred_ann_func_api_multi_table, mic_multi_accuracy_final)
```

With Multiple Categories for Deaths too, the inference is somewhat similar to the Single Category. XGBoost is a good choice for somebody who is looking for a quick solution that is easy on the effort required. However the shape of predictions is poor and XGBoost is more biased towards predicting more deaths as live cases which is counter productive to our efforts to predict more deaths accurately. However, we are using XGBoost only with some basic tuning. With more advanced tuning XGBoost can do quite well

ANN require a lot more programming skill and effort, but can provide better accuracy if built and tuned properly.

```{r MIC Final Analysis ANN Cleanup - Categorical , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove data that is no longer required and run garbage collector to free up memory
rm(mic_modified_train_set_ann, mic_modified_test_set_ann)

rm( mic_modified_train_set_ann_outcome, mic_modified_test_set_ann_outcome, dummy_vars_names)

rm(mic_nb_cm_bin_1, mic_nb_cm_bin_2, mic_nb_wi_cm_bin_1, mic_nb_wi_cm_bin_2, mic_xgb_cm_bin, mic_ann_seq_cm_bin, mic_ann_func_cm_bin)

rm ( mic_test_let_is_final_multi, mic_train_let_is_final_multi)

rm(pred_xgboost_mic_categorical_final, pred_ann_seq_api_multi, pred_ann_func_api_multi )

rm(mic_xgb_cm_multi_final, mic_ann_seq_cm_multi_final, mic_ann_func_cm_multi_final)

rm(mic_summary_bin, mic_summary_multi, mic_distributions_multi)

rm(survival_rate)


gc()


```

```{r MIC Data Clean up Training and Holdout Sets, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Perform final cleanup of dataset and run garbage collector

rm(mic_continuous_variables, mic_ordinal_variables, mic_part_ordinal_variables,mic_nominal_variables, mic_complications, survival_rate, dummy_vars_names)

rm( mic_demographic_history_features, mic_ecg_features, mic_emergency_icu_features, mic_ft_features,  mic_infarction_features, mic_medicine_features, mic_relapse_features, mic_serum_features)

rm(mic_orig_train, mic_orig_test,  mic_data_orig, mic_data, mic_orig_imputed_mice, mic_orig_train_imputed_mice, mic_orig_test_imputed_mice)

rm(mic_test_index)

gc()

##########################################################
# End Analysis of MIC Dataset
##########################################################

```

\newpage

# Diabetes 130-US Hospitals for Years 1999-2008

We will pick up the largest dataset at the end.

## Dataset Introduction

### List of Attributes/Features

The list of Attributes/Features is very well described in the Research Article available at <https://doi.org/10.1155/2014/781670>.

We present a slightly modified version of the table available in the article below, which has updates to the missing values to be aligned with the dataset under analysis. Another important feature of the raw data is that the missing values are all represented using a "?" mark, which makes it very easy to identify them.

+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| **Col** | **Feature name**         | **Type**          | **Description and values**                                           | **% missing** |
|         |                          |                   |                                                                      |               |
| **ID**  |                          |                   |                                                                      |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 1       | encounter_id             | Numeric / Integer | Unique identifier of an encounter                                    | 0             |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 2       | patient_nbr              | Numeric / Integer | Unique identifier of a patient                                       | 0             |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 3       | race                     | Categorical       | Values:                                                              | 2.33          |
|         |                          |                   |                                                                      |               |
|         |                          |                   | Caucasian, Asian, African American,                                  |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | Hispanic, and other                                                  |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 4       | gender                   | Categorical       | Values:                                                              | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | male, female, and unknown/invalid                                    |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 5       | age                      | Categorical       | Grouped in 10-year intervals:                                        | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | [0, 10), [10, 20),..., [90, 100)                                     |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 6       | weight                   | Numeric           | Weight in pounds.                                                    | 96.86         |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 7       | admission_type_id        | Categorical       | Integer identifier corresponding                                     | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | to 9 distinct values,                                                |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | for example, emergency, urgent,                                      |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | elective, newborn, and not available                                 |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 8       | discharge_disposition_id | Categorical       | Integer identifier corresponding                                     | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | to 29 distinct values,                                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | for example, discharged to home, expired, and not available          |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 9       | admission_source_id      | Categorical       | Integer identifier corresponding                                     | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | to 21 distinct values,                                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | for example, physician referral, emergency room, and transfer        |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | from a hospital                                                      |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 10      | time_in_hospital         | Integer           | Integer number of days between admission and discharge               | 0             |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 11      | payer_code               | Categorical       | Integer identifier corresponding                                     | 39.56         |
|         |                          |                   |                                                                      |               |
|         |                          |                   | to 23 distinct values,                                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | for example, Blue Cross/Blue Shield, Medicare, and self-pay          |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 12      | medical_specialty        | Categorical       | Integer identifier of a specialty                                    | 49.08         |
|         |                          |                   |                                                                      |               |
|         |                          |                   | of the admitting physician, corresponding to 84 distinct values,     |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | for example, cardiology, internal medicine, family/general practice, |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | and surgeon                                                          |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 13      | num_lab_procedures       | Integer           | Number of lab tests performed                                        | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | during the encounter                                                 |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 14      | num_procedures           | Integer           | Number of procedures                                                 | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | (other than lab tests) performed                                     |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | during the encounter                                                 |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 15      | num_medications          | Integer           | Number of distinct generic names                                     | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | administered during the encounter                                    |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 16      | number_outpatient        | Integer           | Number of outpatient visits of the                                   | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | patient in the year preceding the                                    |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | encounter                                                            |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 17      | number_emergency         | Integer           | Number of emergency visits of the                                    | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | patient in the year preceding the                                    |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | encounter                                                            |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 18      | number_inpatient         | Integer           | Number of inpatient visits of the                                    | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | patient in the year preceding the                                    |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | encounter                                                            |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 19      | diag_1                   | Categorical       | The primary diagnosis                                                | 0.02          |
|         |                          |                   |                                                                      |               |
|         |                          |                   | (coded as first three digits of ICD9);                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | 717 distinct values                                                  |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 20      | diag_2                   | Categorical       | Secondary diagnosis                                                  | 0.35          |
|         |                          |                   |                                                                      |               |
|         |                          |                   | (coded as first three digits of ICD9);                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | 749 distinct values                                                  |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 21      | diag_3                   | Categorical       | Additional secondary diagnosis                                       | 1.4           |
|         |                          |                   |                                                                      |               |
|         |                          |                   | (coded as first three digits of ICD9);                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | 790 distinct values                                                  |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 22      | number_diagnoses         | Integer           | Number of diagnoses                                                  | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | entered to the system                                                |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 23      | max_glu_serum            | Categorical       | Indicates the range of the                                           | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | result or if the test was not taken.                                 |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | Values: \>200, \>300, normal,                                        |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | and none if not measured                                             |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+
| 24      | A1Cresult                | Categorical       | Indicates the range of the                                           | 0             |
|         |                          |                   |                                                                      |               |
|         |                          |                   | result or if the test was not taken.                                 |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | Values:                                                              |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | \>8 if the result was greater than 8%,                               |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | \>7 if the result was greater than                                   |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | 7% but less than 8%,                                                 |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | normal if the result was less than                                   |               |
|         |                          |                   |                                                                      |               |
|         |                          |                   | 7%, and none if not measured.                                        |               |
+---------+--------------------------+-------------------+----------------------------------------------------------------------+---------------+

: D130 Attributes/Features - Columns 1-24

\newpage

+--------------------------------------------------------------------------------------------+
| Columns 25-47                                                                              |
|                                                                                            |
| **Description and values**                                                                 |
|                                                                                            |
| 23 features for medications -                                                              |
|                                                                                            |
| The feature indicates whether the drug was prescribed or there was a change in the dosage. |
|                                                                                            |
| Values: “up” if the dosage was increased during the encounter,                             |
|                                                                                            |
| “down” if the dosage was decreased,                                                        |
|                                                                                            |
| “steady” if the dosage did not change,                                                     |
|                                                                                            |
| and “no” if the drug was not prescribed                                                    |
+--------------------------------------------------------------------------------------------+

+------------+--------------------------+-------------+----------------------------+---------------+
| **Col**    | **Feature name**         | **Type**    | **Description and values** | **% missing** |
|            |                          |             |                            |               |
| **ID**     |                          |             |                            |               |
+============+==========================+=============+============================+===============+
| 25         | metformin                | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 26         | repaglinide              | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 27         | nateglinide              | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 28         | chlorpropamide           | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 29         | glimepiride              | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 30         | acetohexamide            | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 31         | glipizide                | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 32         | glyburide                | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 33         | tolbutamide              | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 34         | pioglitazone             | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 35         | rosiglitazone            | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 36         | acarbose                 | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 37         | miglitol                 | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 38         | troglitazone             | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 39         | tolazamide               | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 40         | examide                  | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 41         | citoglipton              | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 42         | insulin                  | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 43         | glyburide-metformin      | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 44         | glipizide-metformin      | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 45         | glimepiride-pioglitazone | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 46         | metformin-rosiglitazone  | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+
| 47         | metformin-pioglitazone   | Categorical | See note above             | 0             |
+------------+--------------------------+-------------+----------------------------+---------------+

: D130 Attributes/Features - Columns 25-47

\newpage

+-----------+------------------+-------------+----------------------------------+---------------+
| **Col**   | **Feature name** | **Type**    | **Description and values**       | **% missing** |
|           |                  |             |                                  |               |
| **ID**    |                  |             |                                  |               |
+===========+==================+=============+==================================+===============+
| 48        | change           | Categorical | Indicates if there was a change  | 0             |
|           |                  |             |                                  |               |
|           |                  |             | in diabetic medications          |               |
|           |                  |             |                                  |               |
|           |                  |             | (either dosage or generic name). |               |
|           |                  |             |                                  |               |
|           |                  |             | Values: change and no change     |               |
+-----------+------------------+-------------+----------------------------------+---------------+
| 49        | diabetesMed      | Categorical | Indicates if there was any       | 0             |
|           |                  |             |                                  |               |
|           |                  |             | diabetic medication prescribed.  |               |
|           |                  |             |                                  |               |
|           |                  |             | Values: yes and no               |               |
+-----------+------------------+-------------+----------------------------------+---------------+

: D130 Attributes/Features - Columns 48-49

+----------+------------+-------------+--------------------------------------------+---------------+
| **Col**  | **Target** | **Type**    | **Description and values**                 | **% missing** |
|          |            |             |                                            |               |
| **ID**   |            |             |                                            |               |
+==========+============+=============+============================================+===============+
| 50       | readmitted | Categorical | Days to inpatient readmission.             | 0             |
|          |            |             |                                            |               |
|          |            |             | Values: \<30 if the patient was readmitted |               |
|          |            |             |                                            |               |
|          |            |             | in less than 30 days,                      |               |
|          |            |             |                                            |               |
|          |            |             | \>30 if the patient was                    |               |
|          |            |             |                                            |               |
|          |            |             | readmitted in more than 30 days,           |               |
|          |            |             |                                            |               |
|          |            |             | and No for no record of readmission.       |               |
+----------+------------+-------------+--------------------------------------------+---------------+

: D130 Outcome/Prediction - Column 50

Let us have a quick look at some of the Attributes and see what we can make of them at first glance.

1.  encounter_id - It is a unique value for every interaction and is the unique key for each observation in the Dataset. There are as many unique "encounter_id" values as there are rows in the dataset i.e 101766.
2.  patient_nbr- It is a unique value for each Patient and can help us track the number of encounter_id associated with each patient. The total number of unique "patient_nbr" values is 71518 which is about 70.28% of the total number of rows. We shall treat the "patient_nbr" as a factor type variable.
3.  race - It is a categorical variable with values Caucasian, Asian, African American, Hispanic, and other. About 2.33% of the values are missing. It does not make much sense to try to impute values into the missing rows based on the data from the other columns. We can choose to either include the missing values under the "other" category or leave it as it is. We will choose the latter option so not to taint the original data. Remember that in this dataset, unknown values are represented by a "?".
4.  Weight - Though being overweight is a very important indicator for developing Diabetes and progression of the disease, we will also need some indication of whether the person is overweight. Though Race, Gender and Age can provide some indications, these are too broad to be able to guess whether a person is overweight or not when missing almost 97% of the values. Too much of the data is missing and it is unlikely that we can impute something based on the data from the other columns that is sensible.
5.  payer_code - About 39.56% of the values are missing. It is likely that this column actually codes some important data about when the patient decides to seek medical attention, Again imputation using the other columns would possibly be erroneous.
6.  medical_specialty - About 49.08% of the values are missing. Again it is very likely that this column codes important data about the initial diagnosis and progression of the disease before the patients (or their caregivers) sought specific medical attention for diabetes. It will be unlikely to impute missing data.
7.  diag_1, diag_2 and diag_3 - There are a very small numbers of missing values, though there are 717 distinct possibilities for diag_1, 749 for diag_2 and 790 for diag_3 respectively, Since these are all based on ICD9 codes, any imputation using the other columns would possibly be erroneous.

### Problem to Solve

We need to be able to predict the days to inpatient readmission. The values are

1.  "\<30" if the patient was readmitted in less than 30 days
2.  "\>30" if the patient was readmitted in more than 30 days, and
3.  "No" for no record of readmission.

### Important Details

Next, we explore some very important details about the dataset that are not available from the UCI Website but can be understood by reading through the exemplary work done by the researchers, which is available in their publication[2].

Here is an extract from the paper with the authors' comments reproduced verbatim but organised as a ordered list for ease of reading.

------------------------------------------------------------------------

1.  Criteria for Inclusion.

    (1) It is an inpatient encounter (a hospital admission).
    (2) It is a “diabetic” encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.
    (3) The length of stay was at least 1 day and at most 14 days.
    (4) Laboratory tests were performed during the encounter.
    (5) Medications were administered during the encounter.

2.  Criteria 3-4 were applied to remove admissions for procedures and so forth, which were of less than 23 hours of duration and in which changes in diabetes management were less likely to have occurred.

3.  **It should be noted that the diabetic encounters are not all encounters of diabetic patients but rather only these encounters where diabetes was coded as an existing health condition.**

4.  101,766 encounters were identified to fulfil all of the above five inclusion criteria and were used in further analysis. Attribute/feature selection was performed by our clinical experts and only attributes that were potentially associated with the diabetic condition or management were retained.

5.  Since we are primarily interested in factors that lead to early readmission, we defined the readmission attribute (out- come) as having two values: “readmitted,” if the patient was readmitted within 30 days of discharge or “otherwise,” which covers both readmission after 30 days and no readmission at all.

6.  The values of the readmission attribute were determined by examination of all patient records in the database to determine the first inpatient visit after discharge. Note that 30 days was chosen based on criteria often used by funding agencies.

7.  Haemoglobin A1c (HbA1c) is an important measure of glucose control, which is widely applied to measure performance of diabetes care.

    1.  The measurement of HbA1c at the time of hospital admission offers a unique opportunity to assess the efficacy of current therapy and to make changes in that therapy if indicated (e.g., HbA1c \> 8.0% on current regimen). We considered the possibility that if an HbA1c test result was available from a measurement (outpatient or inpatient) done within three months prior to the sentinel admission, the test might not be repeated. In these cases (0.1% of the total), we used the measurement available from the previous visit. In all other cases, measurement of HbA1c was performed at the time of hospital admission.

    2.  We examined both the frequency of HbA1c test ordering and the response to its result, which we defined as a change in diabetic medications. By a “change of medication” we understand any dosage change (increase or reduction) as well as change to a drug with a different generic name, for example, a change of the type of insulin or an introduction of a new drug. The database contains detailed information about dosage but is restricted only to medications administered during the encounter. It was not possible to track any preadmission and discharge medications.

    3.  We considered four groups of encounters: (1) no HbA1c test performed, (2) HbA1c performed and in normal range, (3) HbA1c performed and the result is greater than 8% with no change in diabetic medications, and (4) HbA1c performed, result is greater than 8%, and diabetic medication was changed.

------------------------------------------------------------------------

As a reading of the Research Article would indicate, the primary emphasis of the article is on the efficacy of using HbA1c measurements for managing diabetes and its progression, through changes in treatment.

\newpage

## Analysis

### Dataset preparation

We will download the dataset from the UCI ML repository. As indicated in the Introduction chapter, the dataset is available in CSV format enclosed in a zip file. It can easily be unzipped and imported into a data frame for further processing.

We will create Lists of Continuous, Discrete, Categorical and Binary features so that it is easier to process them later.

We will also create feature sets or groupings of features that we can use with the Keras Functional API. The feature set groupings are done with very rudimentary knowledge of the features based on their descriptions and the availability of time. As indicated already, we can greatly enhance the accuracy of the predictions with the services of a Subject Matter Expert (SME) in CVD who can help us group features that can be combined together to offer the best possible predictions.

```{r Download d130 dataset, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

##########################################################
# Begin Analysis of d130 Dataset
##########################################################

##########################################################
# Download the Raw Data from the respective repositories as the source for truth for the Datasets
##########################################################


options(timeout = 120)


# Diabetes 130-US Hospitals for Years 1999-2008
# https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008
# https://archive.ics.uci.edu/static/public/296/diabetes+130-us+hospitals+for+years+1999-2008.zip

dl_d130 <- "diabetes+130-us+hospitals+for+years+1999-2008.zip"
if(!file.exists(dl_d130))
  download.file("https://archive.ics.uci.edu/static/public/296/diabetes+130-us+hospitals+for+years+1999-2008.zip", dl_d130)

d130_data_file <- "diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv"
  if(!file.exists(d130_data_file))
  unzip(dl_d130, "diabetic_data.csv")

d130_ids_file <- "diabetes+130-us+hospitals+for+years+1999-2008/IDS_mapping.csv"
  if(!file.exists(d130_ids_file))
  unzip(dl_d130, "IDS_mapping.csv")

d130_data <- read.csv(d130_data_file)
d130_ids_mapping <- read.csv(d130_ids_file)


####### 
# Remove Variables used to hold filenames as they are not required anymore
rm(d130_data_file,d130_ids_file,dl_d130)



```

We will assign the right types to each variable. We will configure Binary and Discrete variables as factors, Discrete variables as Integers and Continuous variables as Numerics (Float). There is only one Continuous variable which is "weight" and it is not relevant to our analysis. we will document it for the sake of completeness.

Let us get a view of the ratios of the outcomes in the dataset.

```{r Prepare d130 data for Analysis, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Create a new modified dataset for analysis

# Let us create the list of variables and their types so that it is easier to process them later

################ Classification of Variables ############

# Weight is only documented here for the sake of completeness. It is not used for any other purpose as 97% of the values are missing. 
d130_continuous_variables <- c("weight") 

# We do not include encounter_id as it is unique for each observation and has no value in Prediction

d130_discrete_variables <- c("time_in_hospital","num_lab_procedures","num_procedures","num_medications","number_outpatient","number_emergency","number_inpatient","number_diagnoses")

d130_categorical_variables <- c("patient_nbr","race","age","admission_type_id","discharge_disposition_id","admission_source_id","payer_code","medical_specialty","diag_1","diag_2","diag_3","max_glu_serum","A1Cresult","metformin","repaglinide","nateglinide","chlorpropamide","glimepiride","acetohexamide","glipizide","glyburide","tolbutamide","pioglitazone","rosiglitazone","acarbose","miglitol","troglitazone","tolazamide","examide","citoglipton","insulin","glyburide.metformin","glipizide.metformin","glimepiride.pioglitazone","metformin.rosiglitazone","metformin.pioglitazone")

d130_binary_variables <- c("gender", "change", "diabetesMed")

# Print structure of imported dataset
print("The structure of our d130 as imported is :",quote = FALSE)
print("================================",quote = FALSE)
str(d130_data)
print("================================",quote = FALSE)

# Split of Responses among the different categories

print("================================",quote=FALSE)
print("The Table of Outcomes is",quote=FALSE)
table(d130_data$readmitted)

print(c("Ratio of NO :",round(sum(d130_data$readmitted == "NO")/nrow(d130_data), digits = 4)), quote=FALSE)

print(c("Ratio of >30 :", round(sum(d130_data$readmitted == ">30")/nrow(d130_data), digits = 4)), quote=FALSE)

print(c("Ratio of <30 ", round(sum(d130_data$readmitted == "<30")/nrow(d130_data), digits = 4)), quote=FALSE)
print("================================",quote=FALSE)
```

For each Algorithm, the variable types required for processing are different and we need to be mindful of the same.

Before we begin with the work of analysing the dataset using various ML Algorithms, we will partition the dataset to create sets for Training and Testing. We will further partition the Training set into Training CV and Testing CV sets.

For input into Naive Bayes, we will configure categorical and binary variables as factors and discrete variables as integers.

```{r Prepare d130 data for Analysis - create indices and partitions, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Create a new modified dataset for analysis
# Convert Binary and Categorical Variables to Factors. Replace weight with 0
d130_data_modified <- d130_data %>% 
    mutate_at(c(2:49), ~(ifelse(. == "?", NA, .))) %>%
    mutate_at(c(d130_categorical_variables, d130_binary_variables), ~as.factor(.)) %>% 
    mutate_at(c(d130_discrete_variables), ~as.integer(.)) %>% 
    mutate_at(c("weight"), ~(. = 0)) 


# Split into Training and Testing Sets

set.seed(1024)

d130_test_index <- createDataPartition(y = d130_data_modified$readmitted, times = 1, p = 0.2, list = FALSE)
d130_modified_train <- d130_data_modified[-d130_test_index,]
d130_modified_test <- d130_data_modified[d130_test_index,]


# Create Datasets for Cross Validation 

set.seed(1024)

d130_test_index_cv <- createDataPartition(y = d130_modified_train$readmitted, times = 1, p = 0.2, list = FALSE)
d130_modified_cv_train_set <- d130_modified_train[-d130_test_index_cv,]
d130_modified_cv_test_set <- d130_modified_train[d130_test_index_cv,]




```

\newpage

### Naive Bayes

Let us try with Naive Bayes to get a sense of where we can get with our predictions. As with the MIC dataset, the choice of Naive Bayes is because :-

1.  It can handle missing values.
2.  It can handle large numbers of factors.

Before we process the data using Naive Bayes, we will replace all missing values represented by "?" in the imported dataset with NA which is the internal representation for missing values in R.

Naive Bayes is not very complex to tune and has a few parameters that can be tuned for optimal predictions between and within the classes. Please have a look at the Naive Bayes section from the Heart Failure Prediction (HFP) chapter for more details about tuning Naive Bayes.

We will use Poisson distribution for all Discrete variables. The only Continuous variable is "weight" which we can disregard. We will disable Laplace smoothing so that we can get greater accurate predictions in the "\<30" class.

#### Categorical outcome

```{r Check Naive Bayes with Modified d130 Dataset, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Set all entries in the weight column to 0 so that we can tune the naive_bayes() function without errors
d130_modified_cv_train_set <- d130_modified_cv_train_set %>% 
          mutate(readmitted = as.factor(readmitted))

d130_modified_cv_test_set <- d130_modified_cv_test_set %>% 
          mutate(readmitted = as.factor(readmitted))

# Fit Model
# Use Column names in the formula to Predict Complications for easy tracking. 
###########
# Fit Naive Bayes
fit_nb_native_readmitted <- naive_bayes(x = d130_modified_cv_train_set[,2:47], y = d130_modified_cv_train_set$readmitted, laplace = 0, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
print("The Naive Bayes Summary is :",quote = FALSE)
print("================================",quote = FALSE)
summary(fit_nb_native_readmitted)
print("================================",quote = FALSE)

# Prepare Predictions
pred_nb_test_native_readmitted <-predict(object = fit_nb_native_readmitted, newdata = d130_modified_cv_test_set[,2:47])

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_nb_test_native_readmitted))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_modified_cv_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_nb_test_native_readmitted == d130_modified_cv_test_set$readmitted)
print("================================",quote = FALSE)

# Print confusion matrix
print("The confusion matrix is :",quote = FALSE)
print("================================",quote = FALSE)
confusionMatrix(data = as.factor(pred_nb_test_native_readmitted), reference = as.factor( d130_modified_cv_test_set$readmitted))
print("================================",quote = FALSE)


rm(fit_nb_native_readmitted, pred_nb_test_native_readmitted)


```

We have some decent shape to the Table of Predictions with our chosen parameters. Our predictions within classes are still quite poor. Again by shape we mean the overall number of predictions for each class at the level of the class.

\*Note: We can achieve much higher overall accuracy by configuring the discrete variables as being Gaussian. However this leads to much poorer predictions for the "\<30" and "\>30" classes.

By changing the amount of Laplace smoothing by 1 and 2, we can see changes in the distributions with more number of accurate predictions being made for the "\>30" and "NO" classes

Another quirk that we can observe is that by retaining the "?" as the representation for the missing values, Naive Bayes can produce slightly better results. The behaviour is possibly anomalous but the gains are very small.

**The other peculiarity that we can observe with the dataset is that the categorical features provide very little increments in increasing the overall accuracy. However, they do serve the important purpose of making more accurate predictions within the "\<30" and "\>30" classes.**

\newpage

#### Binary outcome

Let us see if we can do better with a Binary Classifier, For the "readmitted" field, we will retain the "NO" value as-is. We will consolidate the "\>30" and "\<30" values into a much simpler "YES".

**It is worth noting at this point that the original research article consolidates the outcome for readmission in \>30 days and NO readmission into a single category. This is different from our scheme for distinguishing between Readmission and NO Readmission.**

For Binary predictions, we will enable Laplace smoothing with a value of 1. This is to keep the shape of the predictions from becoming too distorted.

```{r Check Naive Bayes with Modified d130 Dataset with Binary Classification, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.


# Create Datasets for Cross Validation 

d130_modified_cv_train_set_binary <- d130_modified_cv_train_set %>% 
        mutate(readmitted = ifelse(readmitted == "NO", "NO", "YES")) %>% 
        mutate(readmitted = as.factor(readmitted))

d130_modified_cv_test_set_binary <- d130_modified_cv_test_set %>% 
        mutate(readmitted = ifelse(readmitted == "NO", "NO", "YES")) %>% 
        mutate(readmitted = as.factor(readmitted))

###########
# Fit Naive Bayes
fit_nb_native_readmitted_binary <- naive_bayes(x = d130_modified_cv_train_set_binary[,2:49], y = d130_modified_cv_train_set_binary$readmitted, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
print("The Naive Bayes Summary is :",quote = FALSE)
print("================================",quote = FALSE)
summary(fit_nb_native_readmitted_binary)
print("================================",quote = FALSE)

# Prepare Predictions
pred_nb_test_native_readmitted_binary <-predict(object = fit_nb_native_readmitted_binary, newdata = d130_modified_cv_test_set_binary)

# Print table of predictions 
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_nb_test_native_readmitted_binary))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_modified_cv_test_set_binary$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_nb_test_native_readmitted_binary == d130_modified_cv_test_set_binary$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = pred_nb_test_native_readmitted_binary, reference = d130_modified_cv_test_set_binary$readmitted)
print("================================",quote = FALSE)


rm(fit_nb_native_readmitted_binary, pred_nb_test_native_readmitted_binary)

```

As Expected, our Overall Accuracy has gotten a little better. Tuning the parameters helps us predict "readmitted" = "YES" more frequently. The Shape of predictions is quite good, however both Sensitivity and Specificity are quite poor.

```{r d130 Initial Analysis - Naive Bayes data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

rm (d130_modified_cv_train_set, d130_modified_cv_test_set)

rm(d130_data_modified, d130_modified_cv_train_set_binary, d130_modified_cv_test_set_binary, d130_modified_train, d130_modified_test)

gc()

```

\newpage

### Random Forest

Let us explore Random Forest next. Since the Random Forest implementation in R cannot handle a lot of factors, we will retain the type of Categorical and Binary variables as Characters from the original dataset.

As Random Forest cannot manage missing values like Naive Bayes can, we will instead

1.  Zero out the column related to weight

2.  We will retain the "?" to represent missing values in payer_code, medical_specialty and other columns

```{r Prepare Predictions with Random Forest for d130, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width= "50%"}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Original Dataset as Random Forest struggles with Large Numbers of Factors
# Mutate readmitted field as factors as we want Random Forest to perform classification
# Set weight=0 for all observations

d130_data_orig <- d130_data %>% 
              mutate_at(c("weight"), ~(. = 0)) %>%
              mutate_at(c("readmitted"), ~as.factor(.))

# Use previously created indices for partitioning

d130_orig_train <- d130_data_orig[-d130_test_index,]
d130_orig_test <- d130_data_orig[d130_test_index,]  

d130_modified_cv_train_set <- d130_orig_train[-d130_test_index_cv,]
d130_modified_cv_test_set <- d130_orig_train[d130_test_index_cv,] 


# Print Random Forest mtry and error rates
print("=====================================",quote = FALSE)
print("The Random Forest mtry values and error rates are",quote = FALSE) 

# Use tuneRF to choose best mtry value
set.seed(1024)
best_mtry_rf_tune <- tuneRF(x = d130_modified_cv_train_set[,c(2:49)], y = d130_modified_cv_train_set[,50], stepFactor = 0.5, improve = 0.00001, trace = TRUE, plot = TRUE, doBest = TRUE)

# Extract and print details of Random Forest 
print("=====================================",quote = FALSE)
print( c(" Details for Random Forest for the d130 Dataset are: "),quote = FALSE, justify = "left") 
print(c("Prediction Type :",best_mtry_rf_tune$type),quote = FALSE)
print(c("Number of Trees (ntree) :",best_mtry_rf_tune$ntree),quote = FALSE)
print(c("mtry value :",best_mtry_rf_tune$mtry),quote = FALSE)
print("=====================================",quote = FALSE)

# Prepare Predictions
pred_rf <- predict(best_mtry_rf_tune, newdata = d130_modified_cv_test_set[,c(2:49)], type = "response")

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_rf))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_modified_cv_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_rf == d130_modified_cv_test_set$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = pred_rf,reference = d130_modified_cv_test_set$readmitted)
print("================================",quote = FALSE)

# Extract and print the 10 most important Features identified by Random Forest
imp <- as.data.frame(randomForest::importance(best_mtry_rf_tune))
imp <- data.frame(Importance = imp$MeanDecreaseGini,
           names   = rownames(imp))
imp <- imp[order(imp$Importance, decreasing = TRUE),]

print("The first 10 Features in order of decreasing importance in prediction are:",quote = FALSE)
print("=============================================",quote = FALSE)
knitr::kable(x = imp[1:10,], col.names = c("Col Id", "Importance", "Names"), caption = "MIC Prediction - Variable Importance (MeanDecreaseGini)")
print("=============================================",quote = FALSE)
```

We can see that Random Forest is really struggling to Predict the "\<30" class. Very strangely, it is not able to make a single prediction from the "\<30" class.

```{r d130 Initial Analysis - Random Forest data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required

rm(d130_data_orig, d130_orig_train, d130_orig_test)
rm(d130_modified_cv_train_set,d130_modified_cv_test_set, best_mtry_rf_tune, pred_rf, imp)

gc()

```

\newpage

### Algorithms that work with only numerical inputs

Unlike the MIC dataset where the categorical variables were mostly of the ordinal type, in the d130 dataset, most of the categorical variables are just simple unordered factors.

To explore Algorithms like ANN and XGBoost, one-hot encoding of the categorical variables is necessary. Otherwise we will end up misconfiguring these algorithms and they will generate inaccurate predictions.

We will not explore these algorithms at this point because with the Current Number of Unique Values for "patient_nbr" (71518), "diag_1" (717), "diag_2" (749) and "diag_3" (790), any kind of one-hot encoding is far too unwieldy and causes memory issues on stock computing hardware.

We will explore them at the end of the next section.

There are two new ML Algorithms that we have not used so far, ADABoost and ADABag which are quite popular within the ML community. Please see note about them after the Analysis for XGBoost.

\newpage

## Dataset Specification for CVD and Initial Analysis

Let us now consider sub-setting our d130 dataset to suit our initial premise of evaluating datasets related to Cardiovascular Diseases.

Let us consult our modern encyclopaedia of knowledge to see what the ICD9 is

<https://en.wikipedia.org/wiki/International_Classification_of_Diseases>

and what the ICD9 Codes mean

<https://en.wikipedia.org/wiki/List_of_ICD-9_codes>

Look up the diabetes and related classification codes

<https://en.wikipedia.org/wiki/List_of_ICD-9_codes_240%E2%80%93279:_endocrine,_nutritional_and_metabolic_diseases,_and_immunity_disorders>

Look up the CVD and related classification codes

<https://en.wikipedia.org/wiki/List_of_ICD-9_codes_390%E2%80%93459:_diseases_of_the_circulatory_system>

We will record the following for the entire d130 dataset

1.  Number of unique patient_nbr
2.  Number of unique "diag_1" codes
3.  Number of unique "diag_2" codes
4.  Number of unique "diag_3" codes

```{r Perform Initial Consolidation of Unique Values - 1, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

############ Extract Summary about Patient Numbers and Diag Codes ############

print(c("============================="),quote = FALSE, justify = "Center")
# Number of Unique "patient_nbr for the entire d130 dataset"
print(c("The Total Number of Unique Patients in the entire d130 dataset is :",length(unique(d130_data$patient_nbr))), justify = "Left",quote = FALSE)

# Number of Unique "diag_1" codes for the entire d130 dataset
print(c("The Total Number of Unique 'diag_1' codes in the entire d130 dataset is :",length(unique(d130_data$diag_1))), justify = "Left",quote = FALSE)

# Number of Unique "diag_2" codes for the entire d130 dataset
print(c("The Total Number of Unique 'diag_2' codes in the entire d130 dataset is :",length(unique(d130_data$diag_2))), justify = "Left",quote = FALSE)

# Number of Unique "diag_3" codes for the entire d130 dataset
print(c("The Total Number of Unique 'diag_3' codes in the entire d130 dataset is :",length(unique(d130_data$diag_3))), justify = "Left",quote = FALSE)

print(c("============================="),quote = FALSE, justify = "Center")
```

We will create some simple regular expressions that can be used to filter our data according to the ICD9 codes for CVD and diabetes.

We will record

1.  The list of unique "diag_1" codes related to CVD

2.  The number of unique "diag_1" codes related to CVD

3.  The number of observations in 'diag_1' referring to CVD

    ------------------------------------------------------------------------

4.  The list of unique "diag_2" codes related to diabetes

5.  The number of unique "diag_2" codes related to diabetes

6.  The number of observations in 'diag_3' referring to diabetes

    ------------------------------------------------------------------------

7.  The list of unique "diag_3" codes related to diabetes

8.  The number of unique "diag_3" codes related to diabetes

9.  The number of observations in 'diag_3' referring to diabetes

This will give us a view of how many cases have CVD as the Primary diagnosis and how many have diabetes as the Secondary and Additional Secondary diagnosis.

```{r Perform Initial Consolidation of Unique Values - 2, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


############ Extract Summary about Diabetes and Diag Codes ############

# Build Regular Expression pattern for CVD and Diabetes based on ICD9 codes
cvd_pattern <- "^39([3-8])|^40(2|4)|(^4(1|2).+)|^785"
diabetes_pattern = "^2(49|50).*"


# Extract and Print diag_1 codes related to CVD
cvd_diag1_unique <- unique(str_extract(d130_data$diag_1, cvd_pattern)) %>% na.omit()

# List of Unique "diag_1" codes referring to CVD
print(c("============================="),quote = FALSE, justify = "Center")
print(c(" The list of Unique 'diag_1' codes referring to CVD is :"))
print(c(cvd_diag1_unique),quote = FALSE, justify = "Left")

#  Number of Unique "diag_1" codes referring to CVD
print(c("The Total Number of Unique 'diag_1' codes referring to CVD is :",length(cvd_diag1_unique)),quote = FALSE, justify = "Left")

# Number of Observations in "diag_1" referring to CVD
print(c("The Total Number of Observations in 'diag_1' referring to CVD is :", sum(str_detect(d130_data$diag_1, cvd_pattern))),quote = FALSE, justify = "Left")
print(c("============================="),quote = FALSE, justify = "Center")

# Extract and Print diag_2 codes related to diabetes
diabetes_diag2_unique <- unique(str_extract(d130_data$diag_2,diabetes_pattern)) %>% na.omit()

# List of Unique "diag_2" codes referring to Diabetes
print(c("============================="),quote = FALSE, justify = "Center")
print(c(" The list of Unique 'diag_2' codes referring to Diabetes is :"))
print(c(diabetes_diag2_unique),quote = FALSE, justify = "Left")

# Number of Unique "diag_2" codes referring to Diabetes
print(c("The Total Number of Unique 'diag_2' codes referring to Diabetes is :",length(diabetes_diag2_unique)),quote = FALSE, justify = "Left")

# Number of Observations in "diag_2" referring to Diabetes
print(c("The Total Number of Observations in 'diag_2' referring to Diabetes is :", sum(str_detect(d130_data$diag_2, diabetes_pattern))),quote = FALSE, justify = "Left")

print(c("============================="),quote = FALSE, justify = "Center")

# Extract and Print diag_2 codes related to diabetes
diabetes_diag3_unique <- unique(str_extract(d130_data$diag_3, diabetes_pattern)) %>% na.omit()

# List of Unique "diag_3" codes referring to Diabetes
print(c("============================="),quote = FALSE, justify = "Center")
print(c(" The list of Unique 'diag_3' codes referring to Diabetes is :"))
print(c(diabetes_diag3_unique),quote = FALSE, justify = "Left")

# Number of Unique "diag_3" codes referring to Diabetes
print(c("The Total Number of Unique 'diag_3' codes referring to Diabetes is :",length(diabetes_diag3_unique)),quote = FALSE, justify = "Left")

# Number of Observations in "diag_3" referring to Diabetes
print(c("The Total Number of Observations in 'diag_3' referring to Diabetes is :", sum(str_detect(d130_data$diag_3, diabetes_pattern))),quote = FALSE, justify = "Left")
print(c("============================="),quote = FALSE, justify = "Center")


```

We will remove observations that are not relevant for our analysis. These are observations where the patient expired during treatment, was moved to hospice or discharged to a long term care hospital.

To keep our ANN manageable and be able to run them on stock computing hardware without taking too much time, we will filter our data where the "diag_1" column contains ICD9 codes related to CVD and the "diag_2" or "diag_3" columns contain ICD9 codes related to diabetes.

We will collect and record some details regarding our newly created subset.

```{r Perform Initial Consolidation of Unique Values - 3, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

#########

# Clean up Data to remove entries where the Patient Expired or was Moved to Hospice
# Sum used for diagnostic purposes. Commented out for Report creation
# sum(d130_data$discharge_disposition_id == c(11,13,14,19,20,21))

d130_data_cleaned_up <- d130_data %>% filter(discharge_disposition_id != c(11,13,14,19,20,21,23)) 

d130_cvd <- d130_data_cleaned_up %>% filter(diag_1 %in% cvd_diag1_unique)

######### Check Counts for Diagnostic Purposes ##########
# Commented out for Report creation
# Count of observations with each diag_1 code
#d130_cvd %>% dplyr::count(diag_1)

# Count of observations with each diag_2 code 
#d130_cvd %>% dplyr::count(diag_2)

# Count of observations with each diag_3 code 
#d130_cvd %>% dplyr::count(diag_3)

# Retain only cases where Secondary or Additional Secondary Diagnoses indicate Diabetes
d130_cvd <-  d130_cvd %>% 
      filter( diag_2 %in% diabetes_diag2_unique | diag_3 %in% diabetes_diag3_unique)

# Number of Unique "patient_nbr" for the Filtered Dataset
print(c("The Number of Unique Patients for the Dataset filtered for CVD cases with diabetes is :", length(unique(d130_cvd$patient_nbr))),quote = FALSE, justify = "Left")

######### Check Counts for Diagnostic Purposes ##########
# Commented out for Report creation
# Count of observations with each diag_2 code after filtering
#d130_cvd %>% dplyr::count(diag_2,sort = TRUE)

# Count of observations with each diag_3 code after filtering
#d130_cvd %>% dplyr::count(diag_3, sort = TRUE)

# Print Structure of the Filtered Dataset
print(c("============================="),quote = FALSE, justify = "Center")

print(c("The Structure of the Filtered Datset is :"))
str(d130_cvd)

print(c("============================="),quote = FALSE, justify = "Center")

```

```{r Perform Initial Consolidation of Unique Values - 4, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required
rm(cvd_diag1_unique,cvd_diag2_unique,cvd_diag3_unique,diabetes_diag1_unique, diabetes_diag2_unique, diabetes_diag3_unique, diabetes_pattern, cvd_pattern, d130_data_cleaned_up)

gc()
```

We will modify our newly created dataset with the right variable types. We will then partition the dataset into Training and Testing sets and the Training set further into Training CV and Testing CV datasets.

```{r Modify d130 data for Analysis using right variable types, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Create a new modified dataset for analysis

# Convert Binary and Categorical Variables to Factors. 
# Set weight=0 for all observations

d130_cvd_modified  <- d130_cvd %>%
    mutate_at(c(2:49), ~(ifelse(. == "?", NA, .))) %>%
    mutate_at(c(d130_categorical_variables, d130_binary_variables,"readmitted"), ~as.factor(.)) %>% 
    mutate_at(c(d130_discrete_variables), ~as.integer(.)) %>% 
    mutate_at(c("weight"), ~as.numeric(0))

# Print Structure of the Filtered Dataset with right variable types
print(c("============================="),quote = FALSE, justify = "Center")

print(c("The Structure of the Filtered Datset with the right variable types is :"))
str(d130_cvd_modified)

print(c("============================="),quote = FALSE, justify = "Center")

# Split into Training and Testing Sets

set.seed(1024)

d130_cvd_test_index <- createDataPartition(y = d130_cvd_modified$readmitted, times = 1, p = 0.2, list = FALSE)
d130_cvd_modified_train <- d130_cvd_modified[-d130_cvd_test_index,]
d130_cvd_modified_test <- d130_cvd_modified[d130_cvd_test_index,]

# Create Datasets for Cross Validation 

set.seed(1024)

d130_cvd_test_index_cv <- createDataPartition(y = d130_cvd_modified_train$readmitted, times = 1, p = 0.2, list = FALSE)
d130_cvd_modified_cv_train_set <- d130_cvd_modified_train[-d130_cvd_test_index_cv,]
d130_cvd_modified_cv_test_set <- d130_cvd_modified_train[d130_cvd_test_index_cv,]


# Split of Responses among the different categories

print("================================",quote=FALSE)
print("The Table of Outcomes is",quote=FALSE)
table(d130_cvd$readmitted)

print(c("The Ratio of Patients who were NOT readmitted is :", round(sum(d130_cvd$readmitted == "NO")/nrow(d130_cvd), digits = 4)),quote = FALSE)

print(c("The Ratio of Patients who were readmitted in >30 days is :", round(sum(d130_cvd$readmitted == ">30")/nrow(d130_cvd), digits = 4)),quote = FALSE)

print(c("The Ratio of Patients who were readmitted in <30 days is :", round(sum(d130_cvd$readmitted == "<30")/nrow(d130_cvd), digits = 4)),quote = FALSE)


```

\newpage

### Initial Analysis

#### Naive Bayes

As earlier, let us start with Naive Bayes to get some sense of where we can get with our predictions.

##### Categorical Classification

We will use Poisson for all discrete variables, Gaussian for weight (which is not relevant anyway) and Laplace smoothing with a value of 1.

```{r Check Naive Bayes with Modified d130 Dataset for CVD, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Column names in the formula to Predict Complications for easy tracking. 

###########
# Fit Naive Bayes
fit_nb_native_readmitted_cvd <- naive_bayes(x = d130_cvd_modified_cv_train_set[,2:49], y = d130_cvd_modified_cv_train_set$readmitted, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
print("The Naive Bayes Summary is :",quote = FALSE)
summary(fit_nb_native_readmitted_cvd)

# Prepare Predictions
pred_nb_test_native_readmitted_cvd <-predict(object = fit_nb_native_readmitted_cvd, newdata = d130_cvd_modified_cv_test_set)

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_nb_test_native_readmitted_cvd))

# PrinttTable of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_cv_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_nb_test_native_readmitted_cvd == d130_cvd_modified_cv_test_set$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = pred_nb_test_native_readmitted_cvd, reference = d130_cvd_modified_cv_test_set$readmitted)
print("================================",quote = FALSE)

rm(fit_nb_native_readmitted_cvd, pred_nb_test_native_readmitted_cvd)


```

\newpage

##### Binary Classification

Let us see if we can do better with a Binary Classifier, As earlier, for the "readmitted" field, we will retain the "NO" value as-is. We will consolidate the "\>30" and "\<30" values into a much simpler "YES".

```{r Check Naive Bayes with Modified d130 Dataset with Binary Classification for CVD, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Consolidate "<30" and ">30" into a simple "YES"

d130_cvd_modified_cv_train_set_binary <- d130_cvd_modified_cv_test_set %>% mutate(readmitted = ifelse(readmitted == "NO", "NO", "YES")) %>% mutate(readmitted = as.factor(readmitted))

d130_cvd_modified_cv_test_set_binary <- d130_cvd_modified_cv_test_set %>% mutate(readmitted = ifelse(readmitted == "NO", "NO", "YES")) %>% mutate(readmitted = as.factor(readmitted))

###########
# Fit Naive Bayes
fit_nb_native_readmitted_binary_cvd <- naive_bayes(x = d130_cvd_modified_cv_train_set_binary[,2:49], y = d130_cvd_modified_cv_train_set_binary$readmitted, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Print Summary
print("The Naive Bayes Summary is :",quote = FALSE)
print("================================",quote = FALSE)
summary(fit_nb_native_readmitted_binary_cvd)
print("================================",quote = FALSE)

# Prepare Predictions
pred_nb_test_native_readmitted_binary_cvd <-predict(object = fit_nb_native_readmitted_binary_cvd, newdata = d130_cvd_modified_cv_test_set_binary)

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_nb_test_native_readmitted_binary_cvd))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_cv_test_set_binary$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_nb_test_native_readmitted_binary_cvd == d130_cvd_modified_cv_test_set_binary$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = pred_nb_test_native_readmitted_binary_cvd, reference = d130_cvd_modified_cv_test_set_binary$readmitted)
print("================================",quote = FALSE)

rm(fit_nb_native_readmitted_binary_cvd, pred_nb_test_native_readmitted_binary_cvd)

```

We have a dramatic improvement in the overall accuracy as well as the balanced accuracy.

As in the previous section with MIC, our predictions using Naive Bayes using a binary classifier are much better.

**This is one of the ways in which we can increase the accuracy of our predictions provided we have the flexibility to modify the outcome.**

```{r d130 CVD Initial Analysis - Naive Bayes data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Clean up data that is no longer required

rm(d130_data_modified)

rm (d130_cvd_modified_cv_train_set, d130_cvd_modified_cv_test_set)

rm(d130_cvd_modified_cv_train_set_binary, d130_cvd_modified_cv_test_set_binary)

gc()

```

\newpage

#### Random Forest

Let us check what Random Forest can do for the CVD dataset. Our hopes are tempered by what we have seen already.

As earlier, we will

1.  Zero out the columns related to weight
2.  We will retain the "?" to represent missing values in payer_code, medical_specialty and other columns

```{r Prepare Predictions with Random Forest for d130 CVD, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width= "50%"}


# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Original Dataset as Random Forest struggles with Large Numbers of Factors
# Mutate readmitted field as factors as we want Random Forest to perform classification
# Set weight=0 for all observations
d130_cvd_data_orig <- d130_cvd %>%   
              mutate_at(c("readmitted"), ~as.factor(.)) %>%
              mutate_at(c("weight"), ~(. = 0)) 

# Use Indices created earlier for creating the Training and Testing datasets 
d130_cvd_orig_train <- d130_cvd_data_orig[-d130_cvd_test_index,]
d130_cvd_orig_test <- d130_cvd_data_orig[d130_cvd_test_index,]


# Create Datasets for Cross Validation 
d130_cvd_modified_cv_train_set <- d130_cvd_orig_train[-d130_cvd_test_index_cv,]
d130_cvd_modified_cv_test_set <- d130_cvd_orig_train[d130_cvd_test_index_cv,]


print("=====================================",quote = FALSE)
print("The Random Forest mtry values and error rates are",quote = FALSE) 

set.seed(1024)
# Fit Random Forest
best_mtry_rf_tune_d130_cvd <- tuneRF(x = d130_cvd_modified_cv_train_set[,c(2:49)], y = d130_cvd_modified_cv_train_set[,50], stepFactor = 0.5, improve = 0.00001, trace = TRUE, plot = TRUE, doBest = TRUE)

# Extract and print details of Random Forest 
print("=====================================",quote = FALSE)
print( c(" Details for Random Forest  for the d130 Dataset are: "),quote = FALSE, justify = "left") 
print(c("Prediction Type :",best_mtry_rf_tune_d130_cvd$type),quote = FALSE)
print(c("Number of Trees (ntree) :",best_mtry_rf_tune_d130_cvd$ntree),quote = FALSE)
print(c("mtry value :",best_mtry_rf_tune_d130_cvd$mtry),quote = FALSE)
print("=====================================",quote = FALSE)

# Prepare Predictions
pred_rf_d130_cvd <- predict(best_mtry_rf_tune_d130_cvd, newdata = d130_cvd_modified_cv_test_set[,c(2:49)], type = "response")

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_rf_d130_cvd))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_cv_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_rf_d130_cvd == d130_cvd_modified_cv_test_set[,50])

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = pred_rf_d130_cvd,reference = d130_cvd_modified_cv_test_set[,50])


# Extract and print the 10 most important Features identified by Random Forest
imp <- as.data.frame(randomForest::importance(best_mtry_rf_tune_d130_cvd))
imp <- data.frame(Importance = imp$MeanDecreaseGini,
           names   = rownames(imp))
imp <- imp[order(imp$Importance, decreasing = TRUE),]

print("The first 10 Features in order of decreasing importance in prediction are:",quote = FALSE)
print("=============================================",quote = FALSE)
knitr::kable(x = imp[1:10,], col.names = c("Col Id", "Importance", "Names"), caption = "MIC Prediction - Variable Importance (MeanDecreaseGini)")
print("=============================================",quote = FALSE)

rm(best_mtry_rf_tune_d130_cvd, pred_rf_d130_cvd, imp, d130_cvd_data_orig, d130_cvd_orig_train, d130_cvd_orig_test)
```

As earlier, Random Forest is struggling badly with the CVD dataset too.

The list of important features has changed a bit with some features being reordered and some being replaced by others. The most important feature patient_nbr remains unchanged.

As earlier, Random Forest is unable to make even a single prediction in the "\<30" class. We will no longer explore Random Forest for this dataset.

```{r d130 CVD Initial Analysis - Random Forest data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required

rm(d130_data_orig, d130_orig_train, d130_orig_test)

rm(d130_modified_cv_train_set,d130_modified_cv_test_set)

gc()

```

\newpage

#### XGBoost

Let us see what kind of results we can obtain for the CVD dataset with XGBoost.

Before that we need to modify our dataset and expand the categorical variables using one-hot encoding. We will modify the dataset so that it is suitable for processing by XGBoost directly.

```{r Prepare d130 CVD Encounters Dataset for Initial Analysis using XGBoost - One Hot Encoding, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Some Cleaning up is required 

# There is a single observation for which the gender is not known. We will remove the observation as the patient_nbr is also unique and we cannot infer the value otherwise.

# Imputation does not work, 
# We will replace missing values with NA
# We will replace all rows in column 6 (weight) with 0

# Do not set outcome "readmitted" as we do not want to one_hot encode it

d130_cvd_boost  <- d130_cvd %>%
        filter( gender != "Unknown/Invalid") %>%
        mutate_at(c(2:49), ~(ifelse(. == "?", NA, .))) %>%
        mutate_at(c("weight"), ~(as.numeric(0))) %>%
        mutate_at(c(d130_categorical_variables, d130_binary_variables), ~as.factor(.)) %>% 
        mutate_at(c(d130_discrete_variables), ~as.integer(.)) 


################# Identify and Remove Columns with a Single Factor Level #################

single_level_factors <- c()

for (i in 1:ncol(d130_cvd_boost)){
  if (nlevels(d130_cvd_boost[,i]) == 1){
    single_level_factors <- c(single_level_factors,colnames(d130_cvd_boost)[i])
    }
}
rm(i)

# Remove above Columns as they cause unwanted issues during processing later

d130_cvd_boost <- d130_cvd_boost %>% select(-c(single_level_factors))


# Create a new vector for Dummy Variables with the names of the Level1 Factors removed


index_level1_factors <- which(!d130_categorical_variables %in% single_level_factors)

d130_categorical_variables_dummy <- d130_categorical_variables[index_level1_factors]

################# Expand Categorical values using Dummy Variables ################# 


# We will use a new data.table to store the Dummy Variables. We will remove Existing Columns in this new data.table to exclude them from further calculations. 

# Ensure Order of operations is as under. Else we could have erroneous results.

# Convert to data.table for one-hot encoding
d130_cvd_boost <- data.table(d130_cvd_boost)


# Create one-hot variables
d130_cvd_boost <- one_hot(dt = d130_cvd_boost, cols = c(d130_categorical_variables_dummy, d130_binary_variables), sparsifyNAs = FALSE, naCols = FALSE, dropCols = TRUE, dropUnusedLevels = TRUE)

d130_cvd_boost <- as.data.frame(d130_cvd_boost)

rm(index_categorical_var, d130_categorical_variables_dummy)

# Create Training and Testing Datasets

d130_cvd_boost_train <- d130_cvd_boost[-d130_cvd_test_index,] # Training & Validation Set
d130_cvd_boost_test <- d130_cvd_boost[d130_cvd_test_index,] # Holdout Set

# Create Datasets for Cross Validation 

d130_cvd_modified_cv_train_set <- d130_cvd_boost_train[-d130_cvd_test_index_cv,] # Training CV
d130_cvd_modified_cv_test_set <- d130_cvd_boost_train[d130_cvd_test_index_cv,] # Testing CV


# Remove Variables and Datasets that are not needed anymore
rm(index_level1_factors, single_level_factors)

```

```{r Check XGBoost with Modified d130 CVD Dataset, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


# For the sake of convenience and reproducibility, we are reusing variables after deleting them. However, for this to work, the variables need to be sanitised or deleted after use. If variables are not  sanitised or deleted, data could leak inadvertently.

# Use Datasets created earlier

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############
####### DOUBLE CHECK PARENTHESIS TO ENSURE THEY CHOOSE RIGHT COLUMNS #########

# Create XGBoost DMatrix

dtrain_d130_cvd <- xgb.DMatrix(data = as.matrix(d130_cvd_modified_cv_train_set[,c(2:(ncol(d130_cvd_modified_cv_train_set)-1))]), label = as.factor(d130_cvd_modified_cv_train_set$readmitted), nthread = 8)


# Fit XGBoost
fit_xgboost_d130_cvd <- xgb.train(
    data = dtrain_d130_cvd,
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 240,
    objective = "multi:softmax", 
    params = list("num_class" = 8, "booster" = "gbtree"),
    verbose = 0 # set verbose=2 during development, tuning and testing
)

# Print Summary
print("================================",quote = FALSE)
print("The XGBoost Summary is :",quote = FALSE)
fit_xgboost_d130_cvd
print("================================",quote = FALSE)

# Prepare Predictions

pred_xgboost_d130_cvd <-predict(object = fit_xgboost_d130_cvd, newdata = as.matrix(d130_cvd_modified_cv_test_set[,c(2:(ncol(d130_cvd_modified_cv_test_set)-1))]))

# Used for diagnostics during development and testing. commented out for report creation
# table(as.factor(pred_xgboost_d130_cvd))

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
pred_xgboost_d130_cvd_refactored <- ifelse(pred_xgboost_d130_cvd == 1, "<30", (ifelse(pred_xgboost_d130_cvd == 2, ">30", "NO")))
table(as.factor(pred_xgboost_d130_cvd_refactored))

# Print table of actual values
#print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_cv_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_xgboost_d130_cvd_refactored == d130_cvd_modified_cv_test_set$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
confusionMatrix(data = as.factor(pred_xgboost_d130_cvd_refactored), reference = as.factor(d130_cvd_modified_cv_test_set$readmitted))
print("================================",quote = FALSE)

rm(dtrain_d130_cvd, fit_xgboost_d130_cvd, pred_xgboost_d130_cvd, pred_xgboost_d130_cvd_refactored)

```

XGBoost is able to predict some observations in the "\<30" class but just two are correctly categorised. It is doing much better for the "\>30" and "NO" classes. However, we are far from having something useful from XGBoost for either the shape of the classes or the predictions for individual observations within the "\<30" and "\>30" classes.

```{r d130 CVD Initial Analysis - XGBoost and ADABoost data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required
  
rm(d130_cvd_modified_cv_train_set, d130_cvd_modified_cv_test_set)

rm(d130_cvd_data_xgboost)


gc()
```

#### ADABoost and ADABag

I have not included ADABoost and ADABag in the list of algorithms as they are unable to make any predictions in the "\<30" class with some reasonable parameters. Even with these parameters ADABoost and ADABag take a very long time to run compared to XGBoost. With parallel processing turned on, I have see them take up 8 cores (16 Threads with Hyperthreading) and almost 40GB of memory which is way above what XGBoost uses.

\newpage

#### Artificial Neural Networks

Let us try ANN Next, We have a much smaller dataset to work with so hopefully, we should be able to load the CVD dataset into Memory and process it.

##### Dataset Preparation

The dataset is by very nature complex and the expansion of the Categorical variables using "one-hot" encoding makes it even more unwieldy. We have to look at how best we can model the same using ANN.

We will first try to classify our predictors/features into related groups.

We will remove factors with a single level as they are not useful in prediction and also cause errors in one-hot encoding.

For one-hot encoding of Categorical Variables, we will use the "one_hot" function from the "mltools" package as it is a lot easier to use for our current case.

Once we have performed the one-hot encoding, for the Functional API, we will collect the column indices of the predictors/features as per the respective groups. This will ensure that our Functional API has all the variables which are one-hot encoded assigned to the correct groups. The code is not very complex or difficult to follow but has subtleties that we need to be aware of, particularly around variable identification using strings.We have already seen this with our MIC dataset.

```{r Prepare d130 CVD Encounters Dataset for Initial Analysis using Neural Networks - One Hot Encoding, include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Since Neural Networks do not understand Factors, we will use the Original Numeric Values. 

# Some Cleaning up is required 

################ Create Grouping of Variables for Functional API ############

# To prevent too much complexity in our models, we will only maintain 6 groups namely "demographics", "history", "hospitalisation", "medicine" and "diagnoses"

d130_patient_nbr <- c("patient_nbr")

d130_demographics_categorical <- c("race","age")  
d130_demographics_binary <- c("gender")

d130_medicine_categorical <- c("metformin","repaglinide","nateglinide","chlorpropamide","glimepiride","acetohexamide","glipizide","glyburide","tolbutamide","pioglsitazone","rosiglitazone","acarbose","miglitol","troglitazone","tolazamide","examide","citoglipton","insulin","glyburide.metformin","glipizide.metformin","glimepiride.pioglitazone","metformin.rosiglitazone","metformin.pioglitazone")

d130_medicine_binary <- c("change", "diabetesMed")

d130_history_discrete <- c("number_outpatient","number_emergency","number_inpatient")

d130_hospitalisation_discrete <- c("time_in_hospital","num_lab_procedures","num_procedures","num_medications","number_diagnoses")
d130_hospitalisation_categorical  <- c("admission_type_id","discharge_disposition_id","admission_source_id","payer_code","medical_specialty","max_glu_serum", "A1Cresult")

d130_diag_categorical <- c("diag_1", "diag_2","diag_3")

###############

# There is a single observation for which the gender is not known. We will remove the observation as the patient_nbr is also unique and we cannot infer the value otherwise.

# Imputation does not work, Instead we will replace all rows in column 6 (weight) with 0
# We will use columns 11 (payer_code) and 12 (medical_specialty) as-is since ANN cannot handle NA values and we will lose information in these columns if we zero them out.In the existing dataset, NA are already coded using "?" 
 

d130_cvd_modified  <- d130_cvd %>%
        filter( gender != "Unknown/Invalid") %>%
        mutate_at(c("weight"), ~(.=0)) %>%
        mutate_at(c(d130_categorical_variables, d130_binary_variables), ~as.factor(.)) %>% 
        mutate_at(c(d130_discrete_variables), ~as.integer(.)) %>% 
        mutate_at(c("readmitted"), ~(. = as.factor(.)))


# Create Training and Testing Sets

d130_cvd_train <- d130_cvd_modified[-d130_cvd_test_index,] # Training & Validation Set
d130_cvd_test <- d130_cvd_modified[d130_cvd_test_index,] # Holdout Set

##############
# Verify that all NA are removed before creating Datasets for Cross Validation
# Used during development and Testing. Commented out for Report Creation
# sum(is.na(d130_cvd_train))
# str(d130_cvd_train)

################# Identify and Remove Columns with a Single Factor Level #################

single_level_factors <- c()

for (i in 1:ncol(d130_cvd_modified)){
  if (nlevels(d130_cvd_modified[,i]) == 1){
    single_level_factors <- c(single_level_factors,colnames(d130_cvd_modified)[i])
    }
}
rm(i)

# Remove above Columns as they cause unwanted issues during processing later

d130_cvd_train <- d130_cvd_train %>% select(-c(single_level_factors))


# Create a new vector for Dummy Variables with the names of the Level1 Factors removed
# Add readmitted column to the vector for creation of Dummy Variables

index_level1_factors <- which(!d130_categorical_variables %in% single_level_factors)

d130_categorical_variables_dummy <- c(d130_categorical_variables[index_level1_factors], "readmitted")

################# Expand Categorical values using Dummy Variables ################# 


# We will use a new data.table to store the Dummy Variables. We will remove Existing Columns in this new data.table to exclude them from further calculations. 

# Ensure Order of operations is as under. Else we could have erroneous results.

# Store the Original Prediction Column as it will be lost during conversion later.  

d130_cvd_readmitted_train <- d130_cvd_train[,"readmitted"]

# Convert to data.table for one-hot encoding
d130_cvd_train <- data.table(d130_cvd_train)


# Create one-hot variables
d130_cvd_train <- one_hot(dt = d130_cvd_train, cols = c(d130_categorical_variables_dummy, d130_binary_variables), sparsifyNAs = FALSE, naCols = FALSE, dropCols = TRUE, dropUnusedLevels = TRUE)


d130_cvd_train <- as.data.frame(d130_cvd_train)


rm(index_categorical_var, d130_categorical_variables_dummy)

######### Extract relevant Column Names from Created Dummy Vars Set ###############

# Create respective groups of feature/predictor variables
# Not required for "patient_nbr" as it is only a single categorical entry
d130_demographics_all <- c(d130_demographics_binary, d130_demographics_categorical)
d130_medicine_all <- c(d130_medicine_categorical, d130_medicine_binary)
d130_hospitalisation_all <- c(d130_hospitalisation_discrete, d130_hospitalisation_categorical)
d130_diag_all <- c(d130_diag_categorical)

# Initialise Empty Vectors to collect and store respective column indices
d130_patient_nbr_col_indices <- c() 
d130_demographics_col_indices <- c()
d130_medicine_col_indices <- c()
d130_hospitalisation_col_indices <- c()
d130_diag_col_indices <- c()

# Extract the Column indices and store them in respective vectors. 

for (i in 1:(length(d130_patient_nbr))) {
  
 d130_patient_nbr_col <- str_which( string = colnames(d130_cvd_train), pattern = d130_patient_nbr[i])
 d130_patient_nbr_col_indices <- c(d130_patient_nbr_col_indices, d130_patient_nbr_col)
 rm(d130_patient_nbr_col)
}


for (i in 1:(length(d130_demographics_all))) {
  
 d130_demographics_col <- str_which( string = colnames(d130_cvd_train), pattern = d130_demographics_all[i])
 d130_demographics_col_indices <- c(d130_demographics_col_indices, d130_demographics_col)
 rm(d130_demographics_col)
}

for (i in 1:(length(d130_medicine_all))) {
  
 d130_medicine_col <- str_which( string = colnames(d130_cvd_train), pattern = (c(d130_medicine_all[i])))
 d130_medicine_col_indices <- c(d130_medicine_col_indices, d130_medicine_col)
 rm(d130_medicine_col)
}

for (i in 1:(length(d130_hospitalisation_all))) {
  
 d130_hospitalisation_col <- str_which( string = colnames(d130_cvd_train), pattern = d130_hospitalisation_all[i])
 d130_hospitalisation_col_indices <- c(d130_hospitalisation_col_indices, d130_hospitalisation_col)
 rm(d130_hospitalisation_col)
}

# Column Index Extraction for History is not required as it is very simple and consists of only 3 discrete predictors. The vriable "d130_history_discrete" can be used as-is

for (i in 1:(length(d130_diag_all))) {
  
 d130_diag_col <- str_which( string = colnames(d130_cvd_train), pattern = d130_diag_all[i])
 d130_diag_col_indices <- c(d130_diag_col_indices, d130_diag_col)
 rm(d130_diag_col)
}

rm(i)

# Used for Diagnostics during development and testing. Commented out for Report creation
# Check if we have collected them all
# sum(length(d130_demographics_col_indices)+ length(d130_history_discrete) + length(d130_hospitalisation_col_indices)) + +length(d130_medicine_col_indices) + length(d130_diag_col_indices)

# Create Datasets for Cross Validation
# Use existing Indices

d130_cvd_cv_train_set <- d130_cvd_train[-d130_cvd_test_index_cv,]
d130_cvd_cv_test_set <- d130_cvd_train[d130_cvd_test_index_cv,]

d130_cvd_readmitted_cv_train <- d130_cvd_readmitted_train[-d130_cvd_test_index_cv]
d130_cvd_readmitted_cv_test <- d130_cvd_readmitted_train[d130_cvd_test_index_cv]

# Remove Variables and Datasets that are not needed anymore
rm(index_level1_factors, single_level_factors)


```

For both Functional and Sequential API alike, we will need to scale the variables and centre the Testing set around the Training set.

##### Functional API

Let us check what ANN built with Functional API can do

```{r Perform Initial Analysis for d130 CVD Dataset using Neural Networks after expansion - Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, fig.align='center', out.width="50%"}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############

############# Create the Input Data#########################################

########## Patient Number ###########################

train_features_patient_nbr <- as.matrix(d130_cvd_cv_train_set[,c(d130_patient_nbr_col_indices)])
val_features_patient_nbr <- as.matrix(d130_cvd_cv_test_set[,c(d130_patient_nbr_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_patient_nbr <- colnames(train_features_patient_nbr)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_patient_nbr) == 0)

index_features_scaling <- which(!feature_names_patient_nbr  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_patient_nbr[,index_features_scaling])

train_features_patient_nbr <- train_features_patient_nbr[,c(feature_names_for_scaling)]
val_features_patient_nbr <- val_features_patient_nbr[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_patient_nbr %<>% scale()
val_features_patient_nbr %<>% 
        scale(center = attr(train_features_patient_nbr, "scaled:center"),
        scale = attr(train_features_patient_nbr, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Demographic ###########################

train_features_demographic <- as.matrix(d130_cvd_cv_train_set[,c(d130_demographics_col_indices)])
val_features_demographic <- as.matrix(d130_cvd_cv_test_set[,c(d130_demographics_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic <- colnames(train_features_demographic)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic) == 0)

index_features_scaling <- which(!feature_names_demographic  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic[,index_features_scaling])

train_features_demographic <- train_features_demographic[,c(feature_names_for_scaling)]
val_features_demographic <- val_features_demographic[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic %<>% scale()
val_features_demographic %<>% 
        scale(center = attr(train_features_demographic, "scaled:center"),
        scale = attr(train_features_demographic, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## Hospitalisation ###########################

train_features_hospitalisation <- as.matrix(d130_cvd_cv_train_set[,c(d130_hospitalisation_col_indices)])
val_features_hospitalisation <- as.matrix(d130_cvd_cv_test_set[,c(d130_hospitalisation_col_indices)])

####### Remove Columns which cause NaN to be generated ########

feature_names_hospitalisation <- colnames(train_features_hospitalisation)


d130_train_list_col_sd_eq_0 <- which(colSds(train_features_hospitalisation) == 0)

index_features_scaling <- which(!feature_names_hospitalisation  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_hospitalisation[,index_features_scaling])

train_features_hospitalisation <- train_features_hospitalisation[,c(feature_names_for_scaling)]
val_features_hospitalisation <- val_features_hospitalisation[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_hospitalisation %<>% scale()
val_features_hospitalisation %<>% 
        scale(center = attr(train_features_hospitalisation, "scaled:center"),
        scale = attr(train_features_hospitalisation, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## History ###########################

train_features_history <- as.matrix(d130_cvd_cv_train_set[,c(d130_history_discrete)])
val_features_history <- as.matrix(d130_cvd_cv_test_set[,c(d130_history_discrete)])

####### Normal Scaling  ############ 

train_features_history %<>% scale()
val_features_history %<>% 
        scale(center = attr(train_features_history, "scaled:center"),
        scale = attr(train_features_history, "scaled:scale"))

########## Medicine ###########################

train_features_medicine <- as.matrix(d130_cvd_cv_train_set[,c(d130_medicine_col_indices)])
val_features_medicine <- as.matrix(d130_cvd_cv_test_set[,c(d130_medicine_col_indices)])

####### Remove Columns which cause NaN to be generated ########

feature_names_medicine <- colnames(train_features_medicine)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Diagnoses ###########################

train_features_diagnoses <- as.matrix(d130_cvd_cv_train_set[,c(d130_diag_col_indices)])
val_features_diagnoses <- as.matrix(d130_cvd_cv_test_set[,c(d130_diag_col_indices)])

####### Remove Columns which cause NaN to be generated Before Scaling  ########

feature_names_diagnoses <- colnames(train_features_diagnoses)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_diagnoses) == 0)

index_features_scaling <- which(!feature_names_diagnoses  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_diagnoses[,index_features_scaling])

train_features_diagnoses <- train_features_diagnoses[,c(feature_names_for_scaling)]
val_features_diagnoses <- val_features_diagnoses[,c(feature_names_for_scaling)]


####### Normal Scaling  ############ 

train_features_diagnoses %<>% scale()
val_features_diagnoses %<>% scale(center = attr(train_features_diagnoses, "scaled:center"),
                        scale = attr(train_features_diagnoses, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

###################################

####### Create Training & Validation Targets #######################

train_targets <- as.matrix(d130_cvd_cv_train_set[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])
val_targets <- as.matrix(d130_cvd_cv_test_set[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])

###################################

# Let us define the input shapes. 
input_shape_patient_nbr<- ncol(train_features_patient_nbr)
input_shape_demographic <- ncol(train_features_demographic)
input_shape_hospitalisation <- ncol(train_features_hospitalisation)
input_shape_history <- ncol(train_features_history)
input_shape_medicine <- ncol(train_features_medicine)
input_shape_diagnoses <- ncol(train_features_diagnoses)

# Let us build the Keras Inputs & Features
input_patient_nbr <- keras_input(shape(input_shape_patient_nbr), name = "patient_nbr")
input_demographic <- keras_input(shape(input_shape_demographic), name = "demographic")
input_hospitalisation <- keras_input(shape(input_shape_hospitalisation), name = "hospitalisation")
input_history <- keras_input(shape(input_shape_history), name = "history")
input_medicine <-  keras_input(shape(input_shape_medicine), name = "medicine")
input_diagnoses <-  keras_input(shape(input_shape_diagnoses), name = "diagnoses")

# Let us build the ANN


patient_nbr_features <-  
    layer_dense(object = input_patient_nbr, units = 992, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024)    

demographic_features <- 
    layer_dense(object = input_demographic, units = 128, activation = "relu") |>  
    layer_dropout(rate = 0.3, seed = 1024) 

hospitalisation_features <- 
    layer_dense(object = input_hospitalisation, units = 736 , activation = "relu") |>  
    layer_dropout(rate = 0.3, seed = 1024) 

history_features <-  
    layer_dense(object = input_history, units = 24, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 
  
medicine_features <- 
    layer_dense(object = input_medicine, units = 456, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

diagnoses_features <- 
    layer_dense(object = input_diagnoses, units = 2880, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 



# Let us combine the Feature Layers together


combined_features <- layer_concatenate(list(patient_nbr_features, demographic_features, hospitalisation_features, history_features, medicine_features, diagnoses_features))


pred_functional_api <- layer_dense(object = combined_features, units = 3, activation = "softmax")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_patient_nbr, input_demographic, input_hospitalisation, input_history, input_medicine, input_diagnoses),
  outputs = list(pred_functional_api)
)

# Plot model for Visualisation
# Commented out for Report Creation
# plot(functional_api_model, show_shapes = TRUE)

# summary(functional_api_model)

# Collect counts for generating initial weights if required
counts <- table(d130_cvd_readmitted_cv_train) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(d130_cvd_readmitted_cv_test) # Counts for Validation Set


# Train the Model 

functional_api_model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

######  Fit model ##############

history <- functional_api_model |> 
  fit(
  x = list(patient_nbr = train_features_patient_nbr,demographic = train_features_demographic, hospitalisation = train_features_hospitalisation, history = train_features_history, medicine = train_features_medicine, diagnoses = train_features_diagnoses),
  y = train_targets,
  validation_data = list(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses),val_targets),
  batch_size = 8192,
  epochs = 30,
  verbose = 0 # set verbose=2 during development, tuning and testing
)

#### Print History for Reference ########

print("Plot of metrics and trends during training")
plot(history)

######## Evaluate the Model against the Validation Targets ########
# commented out for Report creation
#functional_api_model |> evaluate(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses), val_targets)


# Prepare Predictions

d130_probs_functional_api <- functional_api_model |> predict(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses))

pred_ann_d130_functional_api <- max.col(d130_probs_functional_api) - 1L

#Used for development and diagnostics. Commented out for Report creation
#table(pred_ann_d130_functional_api)

pred_ann_d130_functional_api_refactored <- ifelse(pred_ann_d130_functional_api == 0, "<30", (ifelse(pred_ann_d130_functional_api == 1, ">30", "NO")))

print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_ann_d130_functional_api_refactored))

print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(as.factor(d130_cvd_readmitted_cv_test))

print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_ann_d130_functional_api_refactored == d130_cvd_readmitted_cv_test)

print("================================",quote = FALSE)
print("The Confusion Matrix is :",quote = FALSE)
confusionMatrix(data = as.factor(pred_ann_d130_functional_api_refactored), reference = as.factor(d130_cvd_readmitted_cv_test))
print("================================",quote = FALSE)

#### Remove variables that are not required ##########

rm(metrics, counts, feature_names, model, pred_ann_d130_functional_api, pred_ann_d130_functional_api_refactored)

rm( patient_nbr_features, demographic_features,history_features, hospitalisation_features,medicine_features, diagnoses_features,combined_features)

rm( input_patient_nbr, input_demographic,input_history, input_hospitalisation, input_medicine, input_diagnoses)

rm( train_features_patient_nbr, train_features_demographic, train_features_history, train_features_hospitalisation, train_features_medicine, train_features_diagnoses)

rm( val_features_patient_nbr, val_features_demographic, val_features_history, val_features_hospitalisation, val_features_medicine, val_features_diagnoses)

rm( feature_names_patient_nbr, feature_names_demographic, feature_names_hospitalisation, feature_names_medicine, feature_names_diagnoses)

rm( input_shape_patient_nbr, input_shape_demographic, input_shape_history, input_shape_hospitalisation, input_shape_medicine, input_shape_diagnoses)

rm(d130_patient_nbr_col_indices, d130_demographics_col_indices, d130_hospitalisation_col_indices, d130_medicine_col_indices, d130_diag_col_indices)

rm(functional_api_model, pred_functional_api, d130_probs_functional_api)

rm(class_weight)

rm(train_features, train_targets, val_features, val_targets, history)



```

We have excellent shape to our predictions. We are even able to improve upon the predictions made for the "\<30" and "\>30" classes. The overall accuracy is poorer than XGBoost because XGBoost can predict live cases far better.

\newpage

##### Sequential API

We now turn our attention to ANN built with Sequential API

```{r Perform Initial Analysis for d130 CVD Dataset using Neural Networks after expansion - All Variables, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, out.width="50%"}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################

#All Variables. We will use a Sequential API to build a Model and check its prediction abilities

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############
####### DOUBLE CHECK PARENTHESIS TO ENSURE THEY CHOOSE RIGHT COLUMNS #########

feature_names <- colnames(d130_cvd_cv_train_set[2:(ncol(d130_cvd_cv_train_set)-3)])

train_features <- as.matrix(d130_cvd_cv_train_set[feature_names])
train_targets <- as.matrix(d130_cvd_cv_train_set[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])
val_features <- as.matrix(d130_cvd_cv_test_set[feature_names])
val_targets <- as.matrix(d130_cvd_cv_test_set[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])

# We also need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
####### Remove Columns which cause NaN to be generated Before Scaling  ########

feature_names <- colnames(train_features)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]


####### Normal Scaling  ############ 


train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

sum(is.nan(train_features))
sum(is.nan(val_features))

# Let us build the Nueral Network 


model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(units = 768, activation = 'relu') |> #768 
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 512, activation = 'relu') |> #512 
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 192) |> #192
  layer_dense(units = 192, activation = 'relu') |> #192
  layer_dropout(rate = 0.3) |>
  layer_dense(3, activation = "softmax")


# Train the Model 

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)


##############

history <- model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  batch_size = 8192,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)

#### Print History for Reference ########

print("Plot of metrics and trends during training")
plot(history)

# Evaluate Model
# Connented out for Report creation
# model |> evaluate(val_features, val_targets)

# Prepare Predictions

d130_probs_discrete <- model |> predict(val_features)

pred_ann_d130_discrete <- max.col(d130_probs_discrete) - 1L

# Used duing development and diagnosis. Commented out for Report creation
# table(pred_ann_d130_discrete)

# Convert predictions to values that are comparable with actual values
pred_ann_d130_discrete_refactored <- ifelse(pred_ann_d130_discrete == 0, "<30", (ifelse(pred_ann_d130_discrete == 1, ">30", "NO")))

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_ann_d130_discrete_refactored))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(as.factor(d130_cvd_readmitted_cv_test))

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_ann_d130_discrete_refactored == d130_cvd_readmitted_cv_test)

# Print confusion matrix
print("================================",quote = FALSE)
print("The Confusion Matrix is :",quote = FALSE)
confusionMatrix(data = as.factor(pred_ann_d130_discrete_refactored), reference = as.factor(d130_cvd_readmitted_cv_test))
print("================================",quote = FALSE)

# Remove Data that is no longer required

rm(d130_train_list_cols_sum_eq_0, d130_train_list_cols_sd_eq_0,d130_val_list_cols_sum_eq_0, feature_names_for_scaling, index_features_scaling, pred_ann_d130_discrete, pred_ann_d130_discrete_refactored)

rm(d130_probs_discrete, history, model, train_features, train_targets, val_features, val_targets)



```

With Sequential API, we do not have as much success as Functional API in increasing the Predictions for the "\<30" and "\>30" classes. Instead we have an improvement in predictions for the "NO" class.

Again not to belabour the point. It all depends on how we tune our ANN

```{r d130 CVD Initail Analysis -ANN data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

rm(d130_cvd_readmitted_train, d130_cvd_readmitted_cv_train, d130_cvd_readmitted_cv_test)

rm(d130_cvd_cv_train_set, d130_cvd_cv_test_set)

rm(d130_test_index_cv)

gc()

```

\newpage

### Predictions for Holdout Set

Let us evaluate the performance of our Algorithms against the Holdout set

#### Naive Bayes

We evaluate Naive Bayes first

```{r Final Analysis with Naive Bayes for Modified d130 CVD Holdout Dataset, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}


# Use Column names in the formula to Predict outcomes for easy tracking.

###########
# Fit Naive Bayes
fit_nb_native_readmitted_cvd_final <- naive_bayes(x = d130_cvd_modified_train[,2:49], y = d130_cvd_modified_train$readmitted, laplace = 1, usekernel = FALSE, usepoisson = TRUE)

# Prepare Predictions
pred_nb_test_native_readmitted_cvd_final <-predict(object = fit_nb_native_readmitted_cvd_final, newdata = d130_cvd_modified_test[,2:49])

# Print Table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_nb_test_native_readmitted_cvd_final))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_test$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_nb_test_native_readmitted_cvd_final == d130_cvd_modified_test$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
d130_cvd_nb_cm_final <- confusionMatrix(data = pred_nb_test_native_readmitted_cvd_final, reference = d130_cvd_modified_test$readmitted)
d130_cvd_nb_cm_final
print("================================",quote = FALSE)

rm(fit_nb_native_readmitted_cvd_final)



```

The shape of predictions with Naive Bayes is quite decent. The Predictions within the "\<30" class are quite poor. Predictions within the "\>30" class have improved.

```{r d130 CVD Final Analysis - Naive Bayes data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required and run the garbage collector
rm(d130_data_modified)

rm (d130_cvd_modified_train, d130_cvd_modified_test)


gc()
```

\newpage

#### XGBoost

Let us evaluate XGBoost

```{r Final Analysis with XGBoost for Modified d130 CVD Holdout Dataset, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# For the sake of convenience, we are reusing variables after deleting them. However, this is not recommended as data could leak inadvertently  


# Use Datasets created earlier

d130_cvd_modified_train_set <- d130_cvd_boost_train  

d130_cvd_modified_test_set <- d130_cvd_boost_test 

########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############
####### DOUBLE CHECK PRENTHESIS TO ENSURE THEY CHOOSE RIGHT COLUMNS #########

# Create XGBoost DMatrix

dtrain_d130_cvd_final <- xgb.DMatrix(data = as.matrix(d130_cvd_modified_train_set[,c(2:(ncol(d130_cvd_modified_train_set)-1))]), label = as.factor(d130_cvd_modified_train_set$readmitted), nthread = 8)


# Fit XGBoost
fit_xgboost_d130_cvd_final <- xgb.train(
    data = dtrain_d130_cvd_final,
    max_depth = 10,
    eta = 0.15,
    nthread = 8,
    nrounds = 240,
    objective = "multi:softmax", 
    params = list("num_class" = 8, "booster" = "gbtree"),
    verbose = 0 # set verbose=2 during development, tuning and testing
)


# Prepare Predictions

pred_xgboost_d130_cvd_final <-predict(object = fit_xgboost_d130_cvd_final, newdata = as.matrix(d130_cvd_modified_test_set[,c(2:(ncol(d130_cvd_modified_test_set)-1))]))

# Used for diagnostics during development and testing. commented out for report creation
# table(as.factor(pred_xgboost_d130_cvd_final))

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
pred_xgboost_d130_cvd_final_refactored <- ifelse(pred_xgboost_d130_cvd_final == 1, "<30", (ifelse(pred_xgboost_d130_cvd_final == 2, ">30", "NO")))
table(as.factor(pred_xgboost_d130_cvd_final_refactored))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(d130_cvd_modified_test_set$readmitted)

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_xgboost_d130_cvd_final_refactored == d130_cvd_modified_test_set$readmitted)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
d130_cvd_xgb_cm_final <- confusionMatrix(data = as.factor(pred_xgboost_d130_cvd_final_refactored), reference = as.factor(d130_cvd_modified_test_set$readmitted))
d130_cvd_xgb_cm_final
print("================================",quote = FALSE)

rm(dtrain_d130_cvd_final ,fit_xgboost_d130_cvd_final, pred_xgboost_d130_cvd_final,  d130_cvd_modified_cv_test_set_readmitted_refact_final)

```

XGBoost continues to struggle with the shape of the predictions and predictions with the "\<30" class. It achieves greater accuracy by making more predictions in the "\>30" and "NO" classes.

```{r d130 CVD Final Analysis - XGBoost data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

# Remove Data that is no longer required and run the garbage collector
rm(d130_cvd_boost, d130_cvd_boost_train, d130_cvd_boost_test)

gc()

```

XGBoost too has plenty of tuning parameters that we can explore to better match the shape and increase accuracy.

\newpage

#### Artificial Neural Networks

```{r Prepare d130 CVD Encounters Dataset for Final Analysis using Neural Networks - One Hot Encoding, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}
  
# Used during development and Testing. Commented out for Report creation
# Verify that all NA are removed before creating Datasets for Training and Testing
#sum(is.na(d130_cvd_modified))


##############
# Used during development and Testing. Commented out for Report creation
#str(d130_cvd_modified)

##### Identify and Remove Columns with a Single Factor Level ######

single_level_factors <- c()

for (i in 1:ncol(d130_cvd_modified)){
  if (nlevels(d130_cvd_modified[,i]) == 1){
    single_level_factors <- c(single_level_factors,colnames(d130_cvd_modified)[i])
    }
}
rm(i)

# Remove above Columns as they cause unwanted issues during processing later

d130_cvd_modified <- d130_cvd_modified %>% select(-c(single_level_factors))


# Create a new vector for Dummy Variables with the names of the Level1 Factors removed
# Add readmitted column to the vector for creation of Dummy Vars

index_level1_factors <- which(!d130_categorical_variables %in% single_level_factors)

d130_categorical_variables_dummy <- c(d130_categorical_variables[index_level1_factors], "readmitted")

# Expand Categorical values using Dummy Variables


# We will use a new data.table to store the Dummy Vars. We will remove Existing Columns in this new data.table to exclude them from further calculations. We will bind them to the Original Dataframe later to ensure that we recreate the original variables and the Dummy Vars in one Table. 

# Ensure Order of operations is as under. Else we could have erroneous results.

# Store the Original Prediction Column as it will be lost during conversion later.  

d130_cvd_readmitted_modified <- d130_cvd_modified[,"readmitted"]

# Convert to data.table for one_hot 
# Since this is the last set of algorithms for which we are performing the predictions, we are recycling the same variable. Can be changed to a new variable if required.

d130_cvd_modified <- data.table(d130_cvd_modified)



d130_cvd_modified <- one_hot(dt = d130_cvd_modified, cols = c(d130_categorical_variables_dummy, d130_binary_variables), sparsifyNAs = FALSE, naCols = FALSE, dropCols = TRUE, dropUnusedLevels = TRUE)


d130_cvd_modified <- as.data.frame(d130_cvd_modified)


rm(d130_categorical_variables_dummy)

######### Extract relevant Column Names from Created Dummy Vars Set ###############

# Create respective groups of feature/predictor variables
# Not required for "patient_nbr" as it is only a single categorical entry
d130_demographics_all <- c(d130_demographics_binary, d130_demographics_categorical)
d130_medicine_all <- c(d130_medicine_categorical, d130_medicine_binary)
d130_hospitalisation_all <- c(d130_hospitalisation_discrete, d130_hospitalisation_categorical)
d130_diag_all <- c(d130_diag_categorical)

# Initialise Empty Vectors to collect and store respective column indices
d130_patient_nbr_col_indices <- c() 
d130_demographics_col_indices <- c()
d130_medicine_col_indices <- c()
d130_hospitalisation_col_indices <- c()
d130_diag_col_indices <- c()

# Extract the Column indices and store them in respective vectors. 

for (i in 1:(length(d130_patient_nbr))) {
  
 d130_patient_nbr_col <- str_which( string = colnames(d130_cvd_modified), pattern = d130_patient_nbr[i])
 d130_patient_nbr_col_indices <- c(d130_patient_nbr_col_indices, d130_patient_nbr_col)
 rm(d130_patient_nbr_col)
}


for (i in 1:(length(d130_demographics_all))) {
  
 d130_demographics_col <- str_which( string = colnames(d130_cvd_modified), pattern = d130_demographics_all[i])
 d130_demographics_col_indices <- c(d130_demographics_col_indices, d130_demographics_col)
 rm(d130_demographics_col)
}

for (i in 1:(length(d130_medicine_all))) {
  
 d130_medicine_col <- str_which( string = colnames(d130_cvd_modified), pattern = (c(d130_medicine_all[i])))
 d130_medicine_col_indices <- c(d130_medicine_col_indices, d130_medicine_col)
 rm(d130_medicine_col)
}

for (i in 1:(length(d130_hospitalisation_all))) {
  
 d130_hospitalisation_col <- str_which( string = colnames(d130_cvd_modified), pattern = d130_hospitalisation_all[i])
 d130_hospitalisation_col_indices <- c(d130_hospitalisation_col_indices, d130_hospitalisation_col)
 rm(d130_hospitalisation_col)
}

# Column Index Extraction for History is not required as it is very simple and consists of only 3 discrete predictors. The vriable "d130_history_discrete" can be used as-is

for (i in 1:(length(d130_diag_all))) {
  
 d130_diag_col <- str_which( string = colnames(d130_cvd_modified), pattern = d130_diag_all[i])
 d130_diag_col_indices <- c(d130_diag_col_indices, d130_diag_col)
 rm(d130_diag_col)
}

rm(i)

# Used during development and Testing. Commented out for Report creation
# Check if we have collected them all
# sum(length(d130_demographics_col_indices)+ length(d130_history_discrete) + length(d130_hospitalisation_col_indices)) + +length(d130_medicine_col_indices) + length(d130_diag_col_indices)

# Create Datasets for Cross Validation 
# Use Indices created earlier

d130_cvd_train <- d130_cvd_modified[-d130_cvd_test_index,] # Training & Validation Set
d130_cvd_test <- d130_cvd_modified[d130_cvd_test_index,] # Holdout Set

d130_readmitted_train <- d130_cvd_readmitted_modified[-d130_cvd_test_index]
d130_readmitted_test <- d130_cvd_readmitted_modified[d130_cvd_test_index]

# Remove Variables and Datasets that are not needed anymore
rm(d130_cvd_test_index, d130_cvd_test_index_cv, single_level_factors, index_level1_factors)


```

##### Functional API

Next we evaluate ANN built using Functional API

```{r Perform Final Analysis for d130 CVD Dataset using Neural Networks after expansion - Functional API, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE, fig.align='center'}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############

############# Create the Input Data#########################################


########## Patient Number ###########################

train_features_patient_nbr <- as.matrix(d130_cvd_train[,c(d130_patient_nbr_col_indices)])
val_features_patient_nbr <- as.matrix(d130_cvd_test[,c(d130_patient_nbr_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_patient_nbr <- colnames(train_features_patient_nbr)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_patient_nbr) == 0)

index_features_scaling <- which(!feature_names_patient_nbr  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_patient_nbr[,index_features_scaling])

train_features_patient_nbr <- train_features_patient_nbr[,c(feature_names_for_scaling)]
val_features_patient_nbr <- val_features_patient_nbr[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_patient_nbr %<>% scale()
val_features_patient_nbr %<>% 
        scale(center = attr(train_features_patient_nbr, "scaled:center"),
        scale = attr(train_features_patient_nbr, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Demographic ###########################

train_features_demographic <- as.matrix(d130_cvd_train[,c(d130_demographics_col_indices)])
val_features_demographic <- as.matrix(d130_cvd_test[,c(d130_demographics_col_indices)])

####### Remove Columns which cause NaN ########

feature_names_demographic <- colnames(train_features_demographic)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_demographic) == 0)

index_features_scaling <- which(!feature_names_demographic  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_demographic[,index_features_scaling])

train_features_demographic <- train_features_demographic[,c(feature_names_for_scaling)]
val_features_demographic <- val_features_demographic[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_demographic %<>% scale()
val_features_demographic %<>% 
        scale(center = attr(train_features_demographic, "scaled:center"),
        scale = attr(train_features_demographic, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )

########## Hospitalisation ###########################

train_features_hospitalisation <- as.matrix(d130_cvd_train[,c(d130_hospitalisation_col_indices)])
val_features_hospitalisation <- as.matrix(d130_cvd_test[,c(d130_hospitalisation_col_indices)])

####### Remove Columns which cause NaN to be generated ########

feature_names_hospitalisation <- colnames(train_features_hospitalisation)


d130_train_list_col_sd_eq_0 <- which(colSds(train_features_hospitalisation) == 0)

index_features_scaling <- which(!feature_names_hospitalisation  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_hospitalisation[,index_features_scaling])

train_features_hospitalisation <- train_features_hospitalisation[,c(feature_names_for_scaling)]
val_features_hospitalisation <- val_features_hospitalisation[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_hospitalisation %<>% scale()
val_features_hospitalisation %<>% 
        scale(center = attr(train_features_hospitalisation, "scaled:center"),
        scale = attr(train_features_hospitalisation, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## History ###########################

train_features_history <- as.matrix(d130_cvd_train[,c(d130_history_discrete)])
val_features_history <- as.matrix(d130_cvd_test[,c(d130_history_discrete)])

####### Normal Scaling  ############ 

train_features_history %<>% scale()
val_features_history %<>% 
        scale(center = attr(train_features_history, "scaled:center"),
        scale = attr(train_features_history, "scaled:scale"))

########## Medicine ###########################

train_features_medicine <- as.matrix(d130_cvd_train[,c(d130_medicine_col_indices)])
val_features_medicine <- as.matrix(d130_cvd_test[,c(d130_medicine_col_indices)])

####### Remove Columns which cause NaN to be generated ########

feature_names_medicine <- colnames(train_features_medicine)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_medicine) == 0)

index_features_scaling <- which(!feature_names_medicine  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_medicine[,index_features_scaling])

train_features_medicine <- train_features_medicine[,c(feature_names_for_scaling)]
val_features_medicine <- val_features_medicine[,c(feature_names_for_scaling)]

####### Normal Scaling  ############ 

train_features_medicine %<>% scale()
val_features_medicine %<>% 
        scale(center = attr(train_features_medicine, "scaled:center"),
        scale = attr(train_features_medicine, "scaled:scale"))

rm(d130_train_list_col_sd_eq_0, index_features_scaling, feature_names_for_scaling )


########## Diagnoses ###########################

train_features_diagnoses <- as.matrix(d130_cvd_train[,c(d130_diag_col_indices)])
val_features_diagnoses <- as.matrix(d130_cvd_test[,c(d130_diag_col_indices)])

####### Remove Columns which cause NaN to be generated Before Scaling  ########

feature_names_diagnoses <- colnames(train_features_diagnoses)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features_diagnoses) == 0)

index_features_scaling <- which(!feature_names_diagnoses  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features_diagnoses[,index_features_scaling])

train_features_diagnoses <- train_features_diagnoses[,c(feature_names_for_scaling)]
val_features_diagnoses <- val_features_diagnoses[,c(feature_names_for_scaling)]


####### Normal Scaling  ############ 

train_features_diagnoses %<>% scale()
val_features_diagnoses %<>% scale(center = attr(train_features_diagnoses, "scaled:center"),
                        scale = attr(train_features_diagnoses, "scaled:scale"))

rm(d130_train_list_cols_sum_eq_0, index_features_scaling, feature_names_for_scaling )

###################################

####### Create Training & Validation Targets #######################

train_targets <- as.matrix(d130_cvd_train[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])
val_targets <- as.matrix(d130_cvd_test[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])

###################################

# Let us define the input shapes. 
input_shape_patient_nbr<- ncol(train_features_patient_nbr)
input_shape_demographic <- ncol(train_features_demographic)
input_shape_hospitalisation <- ncol(train_features_hospitalisation)
input_shape_history <- ncol(train_features_history)
input_shape_medicine <- ncol(train_features_medicine)
input_shape_diagnoses <- ncol(train_features_diagnoses)

# Let us build the Keras Inputs & Features
input_patient_nbr <- keras_input(shape(input_shape_patient_nbr), name = "patient_nbr")
input_demographic <- keras_input(shape(input_shape_demographic), name = "demographic")
input_hospitalisation <- keras_input(shape(input_shape_hospitalisation), name = "hospitalisation")
input_history <- keras_input(shape(input_shape_history), name = "history")
input_medicine <-  keras_input(shape(input_shape_medicine), name = "medicine")
input_diagnoses <-  keras_input(shape(input_shape_diagnoses), name = "diagnoses")

# Let us build the ANN

patient_nbr_features <-  
    layer_dense(object = input_patient_nbr, units = 992, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024)    

demographic_features <- 
    layer_dense(object = input_demographic, units = 128, activation = "relu") |>  
    layer_dropout(rate = 0.3, seed = 1024) 

hospitalisation_features <- 
    layer_dense(object = input_hospitalisation, units = 736 , activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

history_features <-  
    layer_dense(object = input_history, units = 24, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 
  
medicine_features <- 
    layer_dense(object = input_medicine, units = 456, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 

diagnoses_features <- 
    layer_dense(object = input_diagnoses, units = 2880, activation = "relu") |> 
    layer_dropout(rate = 0.3, seed = 1024) 


# Let us combine the feature layers together

combined_features <- layer_concatenate(list(patient_nbr_features, demographic_features, hospitalisation_features, history_features, medicine_features, diagnoses_features))


pred_functional_api <- layer_dense(object = combined_features, units = 3, activation = "softmax")

# Instantiate an end-to-end model 

functional_api_model <- keras_model(
  inputs = list(input_patient_nbr, input_demographic, input_hospitalisation, input_history, input_medicine, input_diagnoses),
  outputs = list(pred_functional_api)
)

# Collect counts to generate initial weights if required
counts <- table(d130_readmitted_train) # Counts for Training Set

# Used for Diagnostic purposes. Commented out for Report creation
# print("counts for training set")
# counts
# print("counts for testing set")
# table(d130_readmitted_test) # counts for Validation Set


# Train the Model 

functional_api_model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy','categorical_accuracy')
)

######  Fit model ##############

functional_api_model |> 
  fit(
  x = list(patient_nbr = train_features_patient_nbr,demographic = train_features_demographic, hospitalisation = train_features_hospitalisation, history = train_features_history, medicine = train_features_medicine, diagnoses = train_features_diagnoses),
  y = train_targets,
  validation_data = list(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses),val_targets),
  batch_size = 8192,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)


######## Evaluate the Model against the Validation Targets ########
# Commented out for report creation
#functional_api_model |> evaluate(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses), val_targets)


# Prepare Predictions

d130_probs_functional_api <- functional_api_model |> predict(list(val_features_patient_nbr,val_features_demographic, val_features_hospitalisation, val_features_history, val_features_medicine, val_features_diagnoses))

pred_ann_d130_functional_api <- max.col(d130_probs_functional_api) - 1L

# Used for development and diagnostics. commented out for Report creation
#table(pred_ann_d130_functional_api)

# Convert predictions so that they can be compared with actual values
pred_ann_d130_functional_api_refactored <- ifelse(pred_ann_d130_functional_api == 0, "<30", (ifelse(pred_ann_d130_functional_api == 1, ">30", "NO")))

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_ann_d130_functional_api_refactored))

# Print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(as.factor(d130_readmitted_test))

# Print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_ann_d130_functional_api_refactored == d130_readmitted_test)

# Print confusion matrix
print("================================",quote = FALSE)
print("The confusion matrix is :",quote = FALSE)
d130_cvd_ann_func_cm_final <- confusionMatrix(data = as.factor(pred_ann_d130_functional_api_refactored), reference = as.factor(d130_readmitted_test))
d130_cvd_ann_func_cm_final
print("================================",quote = FALSE)

#### Remove variables that are not required ##########

rm(metrics, counts, feature_names, model, pred_ann_d130_functional_api)

rm( patient_nbr_features, demographic_features,history_features, hospitalisation_features,medicine_features, diagnoses_features,combined_features)

rm( input_patient_nbr, input_demographic,input_history, input_hospitalisation, input_medicine, input_diagnoses)

rm( train_features_patient_nbr, train_features_demographic, train_features_history, train_features_hospitalisation, train_features_medicine, train_features_diagnoses)

rm( val_features_patient_nbr, val_features_demographic, val_features_history, val_features_hospitalisation, val_features_medicine, val_features_diagnoses)

rm( feature_names_patient_nbr, feature_names_demographic, feature_names_hospitalisation, feature_names_medicine, feature_names_diagnoses)

rm(input_shape_patient_nbr, input_shape_demographic, input_shape_history, input_shape_hospitalisation, input_shape_medicine, input_shape_diagnoses)

rm(d130_patient_nbr_col_indices, d130_demographics_col_indices, d130_hospitalisation_col_indices, d130_medicine_col_indices, d130_diag_col_indices)

rm(functional_api_model, pred_functional_api, d130_probs_functional_api)

rm(class_weight)

rm(train_features, train_targets, val_features, val_targets, history)



```

ANN Built using the Functional API offer the best shape as yet. They also offer the most number of accurate predictions within the "\<30" class.

\newpage

##### Sequential API

Finally we evaluate ANN built using Sequential API

```{r Perform Final Analysis for d130 CVD Dataset using Neural Networks after expansion - All Variables, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

########## Hack to achieve Consistent results between Keras Runs ############
# Unload and Reload Keras
unload(package = "keras3", quiet = TRUE)
library(keras3)
# Set Up Environment
set.seed(1024)
keras3::set_random_seed(1024)
#tensorflow::set_random_seed(1024, disable_gpu = TRUE) #Uncomment if using TensorFlow
reticulate::py_set_seed(0)
#############################################################################


########################## WARNING WARNING WARNING ##########################
############## DOUBLE CHECK INPUT FEATURES TO PREVENT LEAKAGE ###############
####### DOUBLE CHECK PARENTHESIS TO ENSURE THEY CHOOSE RIGHT COLUMNS #########

feature_names <- colnames(d130_cvd_train[2:(ncol(d130_cvd_train)-3)])

train_features <- as.matrix(d130_cvd_train[feature_names])
train_targets <- as.matrix(d130_cvd_train[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])

val_features <- as.matrix(d130_cvd_test[feature_names])
val_targets <- as.matrix(d130_cvd_test[,c("readmitted_<30", "readmitted_>30", "readmitted_NO")])

# We also need to ensure that the input variables are scaled & centered so that they we do not have any one factor unnecessarily influencing the outcome and all have adequate representation
####### Remove Columns which cause NaN to be generated Before Scaling  ########

feature_names <- colnames(train_features)

d130_train_list_col_sd_eq_0 <- which(colSds(train_features) == 0)

index_features_scaling <- which(!feature_names  %in% names(d130_train_list_col_sd_eq_0))

feature_names_for_scaling <- colnames(train_features[,index_features_scaling])

train_features <- train_features[,c(feature_names_for_scaling)]
val_features <- val_features[,c(feature_names_for_scaling)]


####### Normal Scaling  ############ 


train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))

sum(is.nan(train_features))
sum(is.nan(val_features))

# Let us build the Nueral Network 

model <-
  keras3::keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(units = 768, activation = 'relu') |> #768 
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 512, activation = 'relu') |> #512 
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 192) |> #192
  layer_dense(units = 192, activation = 'relu') |> #192
  layer_dropout(rate = 0.3) |>
  layer_dense(3, activation = "softmax")


# Train the Model 

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)



model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  batch_size = 8192,
  epochs = 30,
  verbose = 0 # Set verbose=2 during development, tuning and testing
)



# Evaluate Model
# Commented out for Report creation
# model |> evaluate(val_features, val_targets)

# Prepare Predictions

d130_probs_seq_api_final <- model |> predict(val_features)

pred_ann_d130_seq_api_final <- max.col(d130_probs_seq_api_final) - 1L

# Used for development and diagnostics. Commented out for Report creation 
#table(d130_probs_seq_api_final)

# Convert predictions so that the can be compared to actual values
pred_ann_d130_seq_api_refactored <- ifelse(pred_ann_d130_seq_api_final == 0, "<30", (ifelse(pred_ann_d130_seq_api_final == 1, ">30", "NO")))

# Print table of predictions
print("================================",quote = FALSE)
print("The Table of predictions is :",quote = FALSE)
table(as.factor(pred_ann_d130_seq_api_refactored))

# print table of actual values
print("================================",quote = FALSE)
print("The Table of actual values is :",quote = FALSE)
table(as.factor(d130_readmitted_test))

# print overall accuracy
print("================================",quote = FALSE)
print("The Overall Accuracy is :",quote = FALSE)
mean(pred_ann_d130_seq_api_refactored == d130_readmitted_test)

# print confusion matrix
print("================================",quote = FALSE)
print("The Confusion Matrix is :",quote = FALSE)
d130_cvd_ann_seq_cm_final <- confusionMatrix(data = as.factor(pred_ann_d130_seq_api_refactored), reference = as.factor(d130_readmitted_test))
d130_cvd_ann_seq_cm_final

##### Remove Variables that are not required anymore ######

rm(d130_train_list_col_sd_eq_0,d130_val_list_cols_sum_eq_0, feature_names_for_scaling, feature_names, index_features_scaling, pred_ann_d130_seq_api_final)

rm(d130_probs_seq_api_final, history, model, train_features, train_targets, val_features, val_targets)

```

Sequential API offer decent shape, and slightly better predictions in the "\<30" class when compared to Naive Bayes. The predictions for the "\>30" and "NO" classes is better than those provided by ANN built with functional API.

Let us compare and summarise the results for all of our models in the next section.

\newpage

## Results & Inference

The d130 dataset has lesser features than the MIC dataset but is far more difficult to make predictions against as all the algorithms struggle with the overall accuracy as well as the shape of predictions.

We have also seen that the predictions for the Validation set and the Holdout sets also vary between algorithms.

1.  With ANN, depending on the way the ANN is built, the predictions can vary a lot. Another situation to watch out for with ANN is that when they are very well optimised for the Validation set, they often perform poorly against the Holdout set. The behaviour is similar to what we can see with over-training or over-fitting. This just as observed with the MIC dataset.

2.  To reiterate what was stated in the MIC Analysis as well. XGBoost and ANN have a lot of tuning parameters that can greatly enhance their accuracy. Whatever I have built are just the most basic for purposes of comparison. With the right amount of skill and time, they can be tuned to offer far more accurate predictions.

Let us summarise and review our analysis so far

```{r d130 CVD Results Summary - Categorical Outcome, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Print Split of Responses among the different categories for reference
print("================================",quote=FALSE)
print("The Ratios of Outcomes for reference is",quote=FALSE)

print(c("Ratio of NO :",round(sum(d130_cvd$readmitted == "NO")/nrow(d130_data), digits = 4)), quote=FALSE)

print(c("Ratio of >30 :", round(sum(d130_cvd$readmitted == ">30")/nrow(d130_data), digits = 4)), quote=FALSE)

print(c("Ratio of <30 ", round(sum(d130_cvd$readmitted == "<30")/nrow(d130_data), digits = 4)), quote=FALSE)
print("================================",quote=FALSE)

# Prepare final results table for categorical outcome
d130_cvd_summary <- data.frame(c(d130_cvd_nb_cm_final$overall["Accuracy"]),c( d130_cvd_xgb_cm_final$overall["Accuracy"]), c(d130_cvd_ann_seq_cm_final$overall["Accuracy"]),c( d130_cvd_ann_func_cm_final$overall["Accuracy"]))
rownames(d130_cvd_summary) <- "Accuracy"
colnames(d130_cvd_summary) <- c("Naive Bayes", "XGBoost", "ANN Sequential API", "ANN Functional API")

# Print results 
knitr::kable(x = d130_cvd_summary, caption = "d130 CVD - Results Summary", digits = 4) %>% kable_styling(font_size = 8)

```

```{r d130 CVD Results Summary - Distributions of predictions, include=TRUE, warning=FALSE, echo=FALSE, message = FALSE}

# Prepare Tables of Actual and Predicted values for easy visualisation

d130_cvd_readmitted_actuals_final <- data.frame(table(d130_readmitted_test))
colnames(d130_cvd_readmitted_actuals_final) <- c("readmitted","Actual Values")

pred_nb_test_native_readmitted_cvd_final_table <- data.frame(table(pred_nb_test_native_readmitted_cvd_final))
colnames(pred_nb_test_native_readmitted_cvd_final_table) <- c("readmitted","Naive Bayes")

pred_xgboost_d130_cvd_final_refactored_table <- data.frame(table(pred_xgboost_d130_cvd_final_refactored))
colnames(pred_xgboost_d130_cvd_final_refactored_table) <- c("readmitted","XGBoost")

pred_ann_d130_seq_api_refactored_table <- data.frame(table(pred_ann_d130_seq_api_refactored))
colnames(pred_ann_d130_seq_api_refactored_table) <- c("readmitted","ANN Sequential API")

pred_ann_d130_functional_api_refactored_table <- data.frame(table(pred_ann_d130_functional_api_refactored))
colnames(pred_ann_d130_functional_api_refactored_table) <- c("readmitted","ANN Functional API")

d130_cvd_distributions_final <- d130_cvd_readmitted_actuals_final %>% 
    left_join(pred_nb_test_native_readmitted_cvd_final_table, by = "readmitted") %>% 
    left_join(pred_xgboost_d130_cvd_final_refactored_table, by = "readmitted") %>% 
    left_join(pred_ann_d130_seq_api_refactored_table, by = "readmitted") %>% 
    left_join(pred_ann_d130_functional_api_refactored_table, by = "readmitted") %>% 
    replace_na(repl = 0)


# Print distributions

knitr::kable(x = d130_cvd_distributions_final, col.names = c("readmitted","Actual Values", "Naive Bayes","XGBoost", "ANN Sequential API", "ANN Functional API"), caption = "d130 CVD - Table of distributions") %>% kable_styling(font_size = 8)



# Print Accurate Values

d130_cvd_accuracy_final <- data.frame(c(d130_cvd_readmitted_actuals_final[1,1], d130_cvd_readmitted_actuals_final[2,1], d130_cvd_readmitted_actuals_final [3,1]) , c(d130_cvd_readmitted_actuals_final[1,2], d130_cvd_readmitted_actuals_final[2,2], d130_cvd_readmitted_actuals_final [3,2]) , c(d130_cvd_nb_cm_final$table[1,1], d130_cvd_nb_cm_final$table[2,2], d130_cvd_nb_cm_final$table[3,3]), c(d130_cvd_xgb_cm_final$table[1,1], d130_cvd_xgb_cm_final$table[2,2], d130_cvd_xgb_cm_final$table[3,3]), c(d130_cvd_ann_seq_cm_final$table[1,1], d130_cvd_ann_seq_cm_final$table[2,2], d130_cvd_ann_seq_cm_final$table[3,3]), c( d130_cvd_ann_func_cm_final$table[1,1],  d130_cvd_ann_func_cm_final$table[2,2],  d130_cvd_ann_func_cm_final$table[3,3]))


knitr::kable(x = d130_cvd_accuracy_final, col.names = c("readmitted","Actual Values", "Naive Bayes","XGBoost", "ANN Sequential API", "ANN Functional API"),caption = "d130 CVD - Table of Accurate Predictions") %>% kable_styling(font_size = 8)


rm(d130_cvd_readmitted_actuals_final, pred_nb_test_native_readmitted_cvd_final_table, pred_xgboost_d130_cvd_final_refactored_table, pred_ann_d130_seq_api_refactored_table, pred_ann_d130_functional_api_refactored_table, d130_cvd_distributions_final, d130_cvd_accuracy_final)
```

In terms of Accuracy, XGBoost offers the best overall accuracy, but it achieves this by predicting more outcomes in the "NO" class which is counter productive to our goals for this dataset.

Naive Bayes provides perhaps the best results for the efforts expended. It has good accuracy comparatively and even the shape of the predictions is quite good. We must state here that the configuration efforts and computational resources required for Naive Bayes are very low compared to the other Algorithms.

Where all the Algorithms struggle is the ability to predict the outcomes within the "\<30" class. The Sensitivity is extremely poor with all algorithms.

ANN built using Functional API provide the best balance in terms of overall accuracy and the shape of the predictions. The shape of the predictions is useful to those who are looking at a more higher level into the outcomes and the predictions at the level of the class and not individual observations within each class.

The efforts required are quite high and another issue with ANN is that they need to be tuned for every dataset against which we need to make predictions.

For this dataset, if quick results are desired, Naive Bayes is a much better choice.

For those who with the required skills and who are willing to spend the time and effort, ANN or even XGBoost can be a good choice.

```{r d130 CVD Final Analysis -ANN data cleanup , include=FALSE, warning=FALSE, echo=FALSE, message = FALSE}

rm(d130_binary_variables, d130_categorical_variables, d130_continuous_variables, d130_discrete_variables)

rm(d130_demographics_all, d130_demographics_binary, d130_demographics_categorical, d130_diag_all, d130_diag_categorical, d130_history_discrete, d130_hospitalisation_all, d130_hospitalisation_categorical, d130_hospitalisation_discrete, d130_medicine_all, d130_medicine_binary, d130_medicine_categorical, d130_patient_nbr )

rm(pred_nb_test_native_readmitted_cvd_final, pred_xgboost_d130_cvd_final_refactored, pred_ann_d130_seq_api_refactored, pred_ann_d130_functional_api_refactored)

rm(d130_cvd_nb_cm_final, d130_cvd_xgb_cm_final, d130_cvd_ann_seq_cm_final, d130_cvd_ann_func_cm_final, d130_cvd_summary)

rm(d130_test_index)

rm(d130_cvd, d130_cvd_modified, d130_cvd_train, d130_cvd_test, d130_cvd_modified_train_set, d130_cvd_modified_test_set, d130_data, d130_ids_mapping)

rm(d130_cvd_readmitted_modified, d130_readmitted_train, d130_readmitted_test)

##########################################################
# End Analysis of d130 Dataset
##########################################################

rm(nCores)

gc()

##########################################################
# End Project
##########################################################
```

\newpage

# Final Results

It has been a long project and fairly long report. With results summarised for each dataset in their respective sections.

We have seen various datasets and the performance of various algorithms

1.  Heart Failure Predictions (HFP) dataset
    1.  Extremely simple dataset with 12 features, no missing values and 5000 observations
    2.  Simple binary outcome
    3.  Random Forest is a clear choice both for accuracy and amount of effort required.
    4.  ANN do a good job with results close to what Random Forest provides but require a lot more programming effort and computing resources.
2.  Myocardial Infarction Complication (MIC) dataset
    1.  Complex dataset with many features (111) and feature types (4)
    2.  Much smaller number of observations (1700)
    3.  Missing values in several features. Imputation possible
    4.  Categorical outcome. Binary as an option.
    5.  XGBoost is a very good choice for both accuracy and amount of effort required. Another big advantage with XGBoost is that it is extremely fast and requires a lot less computing resources when compared to ANN
    6.  ANN are seemingly the better choice for those who have the skills, who can spend the time and effort. However XGBoost with adequate tuning can be just as good an option.
3.  Diabetes 130-US Hospitals for Years 1999-2008 (d130) dataset
    1.  Complex dataset with many features (48) and feature types (4)
    2.  Larger number of observations (101766)
        1.  We have subset the dataset to obtain 5910 observations for running ANN
    3.  Categorical outcome. Binary as an option.
    4.  Missing values in several features. Imputation not possible
    5.  Naive Bayes is a decent choice for both accuracy and the amount of effort required. Similar to XGBoost for MIC, another big advantage with Naive Bayes is that it is extremely fast and requires a lot less computing resources when compared to ANN.
    6.  Naive Bayes for Binary predictions is a lot more accurate than for categorical predictions.
    7.  Again similar to the MIC case, ANN are the better choice for those who have the skills, who can spend the time and effort.

We can see that each dataset can be analysed using several different algorithms and the type of outcome desired (Binary or Categorical) with differing levels of accuracy in predictions.

We have also seen how we can handle missing values in our data. In the HFP dataset, we had no missing values. In the MIC dataset, we used imputation for filling in missing values and for the d130 dataset, we used a common label to represent missing values and went ahead with what was available. In both the MIC and d130 datasets, we judiciously chose what to do with missing values that could neither be imputed nor replaced with a label that made sense.

A common theme across all the 3 datasets has been ANN. ANN are extremely popular with ML learners, developers and enthusiasts because of their flexible and programmable nature. As we have seen, the enthusiasm of the ML community for ANN is not misplaced.

ANN are very flexible and can be built for the task at hand, but an important thought to bear in mind is that the ANN are suited to the dataset that they are built for and often cannot be reused as-built for a different dataset. This makes their extension and maintenance a costly proposition in terms of time and efforts. ANN also require plenty of computing resources.

For quick checks and also for those who do not yet have the skills, lack time or cannot expend the effort required to build ANN. Algorithms like Random Forest, Naive Bayes and XGBoost provide extremely good alternatives.

Based on what we have seen so far. we can use a simple common sense approach to answer the initial questions we had when we started off with the Project

1.  How to assess the dataset and the associated complexity
    a.  Starting to analyse data without understanding its basic characteristics and purpose will lead us astray.
    b.  We can understand that complexity can stem from the number of features, the types of features, diversity in the feature characteristics, types of outcomes and number of outcomes among others.
    c.  The first thing to do is look for a detailed dataset description to understand what the dataset contains and what the data is structured like. It not as glamorous as preparing and presenting reports but the menial work needs to be done.
    d.  Verifying that the data matches the description is the next step. Sometimes dataset descriptions may not exactly match. An example to cite is that the d130 dataset has been in the wild since 2014 and the research article indicates that there are 24 features for the medications and so do most of the descriptions of the dataset. Actually there are only 23. Not that it matters much in this project but verification is necessary.
    e.  Having the services of a Subject Matter Expert (SME) can greatly help us understand the data and how to process it. As an example, In the survival analysis of a Heart Failure Patient what exactly does Ejection Factor (EF) mean? How many different types of EF do we have? How important is it as a marker of the patient's survivability? If it is missing, can it be assumed or imputed somehow? If we just used statistical methods to impute the EF, does that really make sense? What about a Heart Attack Prediction? Which ECG parameters are the most important? These are questions that a SME can answer but the Data Analyst will struggle with.
2.  How to get started with analysing the data
    a.  When we need to get started with analysing the data, often the first question to ask is Who is looking for our analysis and what do they desire?
    b.  Can we reduce the complexity of the outcome? As an example, For prediction of Lethal Events, is it required that we also accurately predict the category by which the event might occur? Is a binary prediction good enough? If categorisation is required, how accurately should we be able to predict the categories and the individual events within the categories
    c.  we can often begin with a simple divide and conquer approach. We look at each feature to understand if it is needed? will it contribute to our analysis? can we look at keeping only what is important? Is some data missing? how does the missing data impact the analysis? Can we fill in the missing data somehow or do away with the feature?
    d.  Using column names rather than column numbers is a good practice when dealing with complex datasets where the features are spread across. That way, we can create subsets and feed the algorithms with the features that make the most sense.
3.  What kind of processing might be required
    a.  Is the data ready to be processed as it is or do we need to perform some processing to get it into shape.
    b.  Do we need processing steps like imputation, one-hot encoding and NA replacement among others.
    c.  We also need to have a good understanding of how our Algorithms work and what tuning is required based on the data. Each Algorithm and its implementation has its own specifics and quirks. Random Forest cannot handle too many factors, Naive Bayes requires us to configure the features using the right variable types in addition to the Naive Bayes configuration, XGBoost and ANN can only work with numeric data. What does the ETA in XGBoost mean?
4.  Which Algorithms to use to get some initial insight
    a.  Simple answer, use the simplest ones like Naive Bayes or XGBoost first. Random Forest is a simple Algorithm too but can be computationally expensive for large datasets. Naive Bayes and XGBoost are not very computationally expensive and are extremely fast compared to other Algorithms.
    b.  ANN are usually a bad choice for initial analysis as they need to be custom built for most tasks
5.  Which ones to use for more detailed analysis.
    a.  Difficult answer, we will get to know on a case by case basis.

First up, I must be honest in admitting that we have not made any great discovery but have arrived at some simple practices that can make our lives a little bit easier by providing a logical, methodical way to approach any Data Analysis project.

We can now conclude that the choice of Algorithms is to a large extent dependent on the data and the structure of the dataset. It is hard to know beforehand which Algorithm will perform best for any specific dataset & task combination. While many of the Algorithms can be tuned to suit the task at hand. It is wise to perform initial tests using simpler Algorithms before delving into more complex ones.

\newpage

# Conclusion

As we approach the end of this project and record the concluding notes, we can recollect our Project Approach.

Our main goals were to find public, unencumbered datasets around the topic of Cardiovascular Diseases (CVD) and build something practically useful around them.

We have seen how different datasets represent different aspects of CVD,

1.  The Heart Failure Prediction (HFP) dataset represents the survival of patients affected by Heart Failure
2.  The Myocardial Infarction Complications (MIC) dataset represents the survival of patients affected by Heart Attacks.
3.  We modified the Diabetes 130-US Hospitals for Years 1999-2008 (d130) dataset to only extract details of patients whose primary diagnosis was CVD with Diabetes as secondary and additional secondary diagnoses.

When we look at any broad topic like CVD, we are faced with many challenges and just as many approaches to those challenges.

The first and foremost challenge is to find datasets that are current and ready for analysis. I have searched for and tried to obtain datasets from the WHO and several country government sites. Many of them are meant as health indicator reports and the results summarise the disease and fatality rates across different countries, provinces, states or even districts/counties based on Age, Ethnicity, Geographical Location, Lifestyle choices, Smoking, Alcohol Consumption among others.

Please see below for some examples.

<https://www.cdc.gov/brfss/>

<https://chronicdata.cdc.gov/browse?category=Heart+Disease+%26+Stroke+Prevention&sortBy=relevance&pageSize=20>

Though summarised data is readily available, finding raw data is quite difficult. In most places registration is required and conditional access is provided only for explicit research purposes with emphasis being placed on the same restrictions being applicable to other downstream recipients of the data.

There are several commercial vendors available who provide curated datasets that are ready for analysis at a price and with restrictions around usage and sharing of the data.

The next challenge is to do something with the dataset that has not been done before and can be considered worth the readers' and evaluators' time. This is just as difficult because many of the publicly available unencumbered datasets have been analysed and reports generated by several different organisations, researchers and students.

The other challenge that researchers face is that the R Language does not have a lot of native implementations of ANN packages. The current fully featured native implementation available is ["torch for R"](https://torch.mlverse.org/) which is based on ["pyTorch"](https://pytorch.org/). Most of the larger ML community seems to use Python more than R.

My choice of Keras as API was more due to the smoother learning curve for Keras as compared to other ANN packages and because [POSIT](https://posit.co/) (the creators of RStudio and many open source R products) maintains the R Interface for Keras.

In terms of future work, something that I tried to do initially but could not successfully complete was to obtain a dataset that was current, complete and unencumbered, even with missing data, which could be cleaned up and be shared with the broader community. My first choice of dataset was one that could be used to diagnose early onset of diabetes and/or CVD or establish a causal link between them. The other choice was to look at the contributions to mortality by Infectious diseases like Tuberculosis among populations in Africa and South Asia. This is definitely something very useful if it can be done.

The other thing that can be done is use "torch for R" to build the ANN which is native to R and does not require Python in the background like Keras does.

I did not delve into feature engineering as it would elongate an already long project. I have just done enough to get the Algorithms to work and provide comparable results. Feature engineering can help increase accuracy and also reduce computing time and resources. Definitely something worth doing. The d130 dataset in particular has many categorical features that seem to increment the accuracy by very little and counteract each other.

For some reason, I could not get ADABoost and ADABag to provide sufficiently accurate results to include in the report. They also consumed more time and computing resources than the other Algorithms. They are both extremely capable, so worth exploring.

While this project does not help in the diagnosis of CVD any better, it provides students of Data Science researching CVD or other Medical Topics with some indicators towards the performance of various ML algorithms and the basic work required to make them work. They can build and improve upon what is available.

I will end the report with a note of thanks to the Readers and Evaluators for their time and patience and Everyone who has helped, directly and indirectly, in the Execution of the Project and the Creation of this Report.

\newpage

# References {.unnumbered}

1.  Clore, J., Cios, K., DeShazo, J., & Strack, B. (2014). Diabetes 130-US Hospitals for Years 1999-2008 [Dataset]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5230J>.

2.  Strack, Beata, DeShazo, Jonathan P., Gennings, Chris, Olmo, Juan L., Ventura, Sebastian, Cios, Krzysztof J., Clore, John N., Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records, *BioMed Research International*, 2014, 781670, 11 pages, 2014. <https://doi.org/10.1155/2014/781670>

3.  Golovenkin, S., Shulman, V., Rossiev, D., Shesternya, P., Nikulina, S., Orlova, Y., & Voino-Yasenetsky, V. (2020). Myocardial infarction complications [Dataset]. UCI Machine Learning Repository. <https://doi.org/10.24432/C53P5M>.

4.  Golovenkin, S.E.; Gorban, Alexander; Mirkes, Evgeny; Shulman, V.A.; Rossiev, D.A.; Shesternya, P.A.; et al. (2020). Myocardial infarction complications Database. University of Leicester. Dataset. <https://doi.org/10.25392/leicester.data.12045261.v3>

5.  Joaquin Vanschoren and Jan N. van Rijn and Bernd Bischl and Luis Torgo. ***OpenML: networked science in machine learning.***SIGKDD Explorations 15(2), pp 49-60, 2013

6.  Giuseppe Casalicchio and Jakob Bossek and Michel Lang and Dominik Kirchhoff and Pascal Kerschke and Benjamin Hofner and Heidi Seibold and Joaquin Vanschoren and Bernd Bischl. ***OpenML: An R package to connect to the machine learning platform OpenML.***Computational Statistics 32(3), pp 1-15, 2017

7.  van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate Imputation by Chained Equations in R. *Journal of Statistical Software*, *45*(3), 1–67. <https://doi.org/10.18637/jss.v045.i03>

\newpage

# Appendix A - MIC Dataset Features & Outcomes {.unnumbered}

For accommodating differences in the Encoding and Analysis methods, we will produce a modified version of the description. We will order the descriptions of the Features and Targets as they appear in the dataset.

**Table of abbreviations**

FC is the functional class of angina pectoris in the last year

CHD is coronary heart disease.

HF is heart failure.

ECG is electrocardiogram.

AV is atrioventricular block.

LBBB is left bundle branch block.

RBBB is right bundle branch block.

QRS is [[QRS complex]{.underline}](https://en.wikipedia.org/wiki/QRS_complex) in ECG

IU is international unit.

ICU is intensive care unit.

ESR is erythrocyte sedimentation rate.

NSAID is non-steroidal anti-inflammatory drugs.

## Attributes/Features

The dataset has 1700 observations and the following features. Not all observations have all features.

1\. Record ID (ID): Unique identifier. Cannot be related to participant. It can be used for reference only.

Type : Numeric/ Integer

2\. Age (AGE): Age of patient.

Type: Numeric/ Real.

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| Age       | 26     | 92     | 61.86  | 11.26  | 8             | 0.47%            |
+-----------+--------+--------+--------+--------+---------------+------------------+

3\. Gender (SEX):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | female     | 635   | 37.35%   |
| 1     | male       | 1065  | 62.65%   |
|       | Missing    |       | 0 0%     |

4\. Quantity of myocardial infarctions in the anamnesis (INF_ANAM):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding

|       |                |       |          |
|-------|----------------|-------|----------|
| Value | Represents     | Cases | Fraction |
| 0     | zero           | 1060  | 62.35%   |
| 1     | one            | 410   | 24.12%   |
| 2     | two            | 147   | 8.65%    |
| 3     | three and more | 79    | 4.65%    |
|       | Missing        | 4     | 0.24%    |

5 .Exertional angina pectoris in the anamnesis (STENOK_AN):

Type:Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                       |       |          |
|-------|-----------------------|-------|----------|
| Value | Represents            | Cases | Fraction |
| 0     | never                 | 661   | 38.88%   |
| 1     | during the last year  | 146   | 8.59%    |
| 2     | one year ago          | 137   | 8.06%    |
| 3     | two years ago         | 117   | 6.88%    |
| 4     | three years ago       | 76    | 4.47%    |
| 5     | 4-5 years ago         | 125   | 7.35%    |
| 6     | more than 5 years ago | 332   | 19.53%   |
|       | Missing               | 106   | 6.24%    |

6\. Functional class (FC) of angina pectoris in the last year (FK_STENOK):

Type: Ordinal/Categorical/Factor

Ordinal attribute. Possible usage of cumulative dummy coding.

|       |                             |       |          |
|-------|-----------------------------|-------|----------|
| Value | Represents                  | Cases | Fraction |
| 0     | there is no angina pectoris | 661   | 38.88%   |
| 1     | I FC                        | 47    | 2.76%    |
| 2     | II FC                       | 854   | 50.24%   |
| 3     | III FC                      | 54    | 3.18%    |
| 4     | IV FC                       | 11    | 0.65%    |
|       | Missing                     | 73    | 4.29%    |

7\. Coronary heart disease (CHD) in recent weeks, days before admission to hospital (IBS_POST):

Type: Ordinal/Categorical/Factor

Ordinal attribute. Possible usage of cumulative dummy coding.

|       |                            |       |          |
|-------|----------------------------|-------|----------|
| Value | Represents                 | Cases | Fraction |
| 0     | there was no CHD           | 418   | 24.59%   |
| 1     | exertional angina pectoris | 548   | 32.24%   |
| 2     | unstable angina pectoris   | 683   | 40.18%   |
|       | Missing                    | 51    | 3.00%    |

8\. Heredity on CHD (IBS_NASL):

Type: Binary/Nominal

|       |                |       |          |
|-------|----------------|-------|----------|
| Value | Represents     | Cases | Fraction |
| 0     | Isn’t burdened | 45    | 2.65%    |
| 1     | burdened       | 27    | 1.59%    |
|       | Missing        | 1628  | 95.76%   |

9\. Presence of an essential hypertension (GB):

Type: Ordinal/Categorical/Factor

Ordinal attribute. Possible usage of cumulative dummy coding.

|       |                                    |       |          |
|-------|------------------------------------|-------|----------|
| Value | Represents                         | Cases | Fraction |
| 0     | there is no essential hypertension | 605   | 35.59%   |
| 1     | Stage 1                            | 11    | 0.65%    |
| 2     | Stage 2                            | 880   | 51.76%   |
| 3     | Stage 3                            | 195   | 11.47%   |
|       | Missing                            | 9     | 0.53%    |

10\. Symptomatic hypertension (SIM_GIPERT):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1635  | 96.18%   |
| 1     | yes        | 57    | 3.35%    |
|       | Missing    | 8     | 0.47%    |

11\. Duration of arterial hypertension (DLIT_AG):

Type: Ordinal/Categorical/Factor

Ordinal attribute. Possible usage of cumulative dummy coding.

|       |                                    |       |          |
|-------|------------------------------------|-------|----------|
| Value | Represents                         | Cases | Fraction |
| 0     | there was no arterial hypertension | 551   | 32.41%   |
| 1     | one year                           | 93    | 5.47%    |
| 2     | two years                          | 58    | 3.41%    |
| 3     | three years                        | 58    | 3.41%    |
| 4     | four years                         | 22    | 1.29%    |
| 5     | five years                         | 73    | 4.29%    |
| 6     | 6-10 years                         | 165   | 9.71%    |
| 7     | more than 10 years                 | 432   | 25.41%   |
|       | Missing                            | 248   | 14.59%   |

12\. Presence of chronic Heart failure (HF) in the anamnesis (ZSN_A):

Type: Partially ordered attribute: Considered as Ordinal/Categorical/Factor

there are two lines of severities:

0\<1\<2\<4, 0\<1\<3\<4.

State 4 means simultaneous states 2 and 3

Possible usage of cumulative dummy coding.

+--------+----------------------------------------------------------------------------------+--------+----------+
| Value  | Represents                                                                       | Cases  | Fraction |
+--------+----------------------------------------------------------------------------------+--------+----------+
| 0      | there is no chronic heart failure                                                | 1468   | 86.35%   |
+--------+----------------------------------------------------------------------------------+--------+----------+
| 1      | I stage                                                                          | 103    | 6.06%    |
+--------+----------------------------------------------------------------------------------+--------+----------+
| 2      | II stage (heart failure due to right ventricular systolic dysfunction)           | 27     | 1.59%    |
+--------+----------------------------------------------------------------------------------+--------+----------+
| 3      | III stage (heart failure due to left ventricular systolic dysfunction)           | 29     | 1.71%    |
+--------+----------------------------------------------------------------------------------+--------+----------+
| 4      | IIB stage (heart failure due to left and right ventricular systolic dysfunction) | 19     | 1.12%    |
+--------+----------------------------------------------------------------------------------+--------+----------+
|        | Missing                                                                          | 54     | 3.18%    |
+--------+----------------------------------------------------------------------------------+--------+----------+

13\. Observing of arrhythmia in the anamnesis (nr_11):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1637  | 96.29%   |
| 1     | yes        | 42    | 2.47%    |
|       | Missing    | 21    | 1.24%    |

14\. Premature atrial contractions in the anamnesis (nr_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1675  | 98.53%   |
| 1     | yes        | 4     | 0.24%    |
|       | Missing    | 21    | 1.24%    |

15\. Premature ventricular contractions in the anamnesis (nr_02):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1660  | 97.65%   |
| 1     | yes        | 19    | 1.12%    |
|       | Missing    | 21    | 1.24%    |

16\. Paroxysms of atrial fibrillation in the anamnesis (nr_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1644  | 96.71%   |
| 1     | yes        | 35    | 2.06%    |
|       | Missing    | 21    | 1.24%    |

17\. A persistent form of atrial fibrillation in the anamnesis (nr_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1650  | 97.06%   |
| 1     | yes        | 29    | 1.71%    |
|       | Missing    | 21    | 1.24%    |

18\. Ventricular fibrillation in the anamnesis (nr_07):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1678  | 98.71%   |
| 1     | yes        | 1     | 0.06%    |
|       | Missing    | 21    | 1.24%    |

19\. Ventricular paroxysmal tachycardia in the anamnesis (nr_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1675  | 98.53%   |
| 1     | yes        | 4     | 0.24%    |
|       | Missing    | 21    | 1.24%    |

20\. First-degree AV block in the anamnesis (np_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1680  | 98.82%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 18    | 1.06%    |

21\. Third-degree AV block in the anamnesis (np_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1679  | 98.76%   |
| 1     | yes        | 3     | 0.18%    |
|       | Missing    | 18    | 1.06%    |

22\. LBBB (anterior branch) in the anamnesis (np_05):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1671  | 98.29%   |
| 1     | yes        | 11    | 0.65%    |
|       | Missing    | 18    | 1.06%    |

23\. Incomplete LBBB in the anamnesis (np_07):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1681  | 98.88%   |
| 1     | yes        | 1     | 0.06%    |
|       | Missing    | 18    | 1.06%    |

24\. Complete LBBB in the anamnesis (np_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1676  | 98.59%   |
| 1     | yes        | 6     | 0.35%    |
|       | Missing    | 18    | 1.06%    |

25\. Incomplete RBBB in the anamnesis (np_09):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1680  | 98.82%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 18    | 1.06%    |

26\. Complete RBBB in the anamnesis (np_10):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1679  | 98.76%   |
| 1     | yes        | 3     | 0.18%    |
|       | Missing    | 18    | 1.06%    |

27\. Diabetes mellitus in the anamnesis (endocr_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1461  | 85.94%   |
| 1     | yes        | 228   | 13.41%   |
|       | Missing    | 11    | 0.65%    |

28\. Obesity in the anamnesis (endocr_02):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1648  | 96.94%   |
| 1     | yes        | 42    | 2.47%    |
|       | Missing    | 10    | 0.59%    |

29\. Thyrotoxicosis in the anamnesis (endocr_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1677  | 98.65%   |
| 1     | yes        | 13    | 0.76%    |
|       | Missing    | 10    | 0.59%    |

30\. Chronic bronchitis in the anamnesis (zab_leg_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1559  | 91.71%   |
| 1     | yes        | 134   | 7.88%    |
|       | Missing    | 7     | 0.41%    |

31\. Obstructive chronic bronchitis in the anamnesis (zab_leg_02):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1572  | 92.47%   |
| 1     | yes        | 121   | 7.12%    |
|       | Missing    | 7     | 0.41%    |

32\. Bronchial asthma in the anamnesis (zab_leg_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1656  | 97.41%   |
| 1     | yes        | 37    | 2.18%    |
|       | Missing    | 7     | 0.41%    |

33\. Chronic pneumonia in the anamnesis (zab_leg_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1684  | 99.06%   |
| 1     | yes        | 9     | 0.53%    |
|       | Missing    | 7     | 0.41%    |

34\. Pulmonary tuberculosis in the anamnesis (zab_leg_06):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1684  | 99.06%   |
| 1     | yes        | 9     | 0.53%    |
|       | Missing    | 7     | 0.41%    |

35\. Systolic blood pressure according to Emergency Cardiology Team (S_AD_KBRIG) (mmHg):

Type: Numeric/Real

+------------+--------+--------+--------+--------+---------------+------------------+
| Attribute  | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+------------+--------+--------+--------+--------+---------------+------------------+
| S_AD_KBRIG | 0      | 260    | 136.91 | 34.97  | 1076          | 63.29%           |
+------------+--------+--------+--------+--------+---------------+------------------+

36\. Diastolic blood pressure according to Emergency Cardiology Team (D_AD_KBRIG) (mmHg):

Type: Numeric/Real

+------------+--------+--------+--------+--------+---------------+------------------+
| Attribute  | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+------------+--------+--------+--------+--------+---------------+------------------+
| D_AD_KBRIG | 0      | 190    | 81.39  | 19.73  | 1076          | 63.29%           |
+------------+--------+--------+--------+--------+---------------+------------------+

37\. Systolic blood pressure according to intensive care unit (S_AD_ORIT) (mmHg):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| S_AD_ORIT | 0      | 260    | 134.59 | 31.34  | 267           | 15.71%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

38\. Diastolic blood pressure according to intensive care unit (D_AD_ORIT) (mmHg):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| D_AD_ORIT | 0      | 190    | 82.75  | 18.31  | 267           | 15.71%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

39\. Pulmonary edema at the time of admission to intensive care unit (O_L_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1578  | 92.82%   |
| 1     | yes        | 110   | 6.47%    |
|       | Missing    | 12    | 0.71%    |

40\. Cardiogenic shock at the time of admission to intensive care unit (K_SH_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1639  | 96.41%   |
| 1     | yes        | 46    | 2.71%    |
|       | Missing    | 15    | 0.88%    |

41\. Paroxysms of atrial fibrillation at the time of admission to intensive care unit, (or at a pre-hospital stage) (MP_TP_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1572  | 92.47%   |
| 1     | yes        | 114   | 6.71%    |
|       | Missing    | 14    | 0.82%    |

42\. Paroxysms of supraventricular tachycardia at the time of admission to intensive care unit, (or at a pre-hospital stage) (SVT_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1680  | 98.82%   |
| 1     | yes        | 8     | 0.47%    |
|       | Missing    | 12    | 0.71%    |

43\. Paroxysms of ventricular tachycardia at the time of admission to intensive care unit, (or at a pre-hospital stage) (GT_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1680  | 98.82%   |
| 1     | yes        | 8     | 0.47%    |
|       | Missing    | 12    | 0.71%    |

44\. Ventricular fibrillation at the time of admission to intensive care unit, (or at a pre-hospital stage) (FIB_G_POST):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1673  | 98.41%   |
| 1     | yes        | 15    | 0.88%    |
|       | Missing    | 12    | 0.71%    |

45\. Presence of an anterior myocardial infarction (left ventricular) (ECG changes in leads V1: V4 ) (ant_im):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | there is no infarct in this location | 660   | 38.82%   |
| 1     | QRS has no changes                   | 392   | 23.06%   |
| 2     | QRS is like QR-complex               | 39    | 2.29%    |
| 3     | QRS is like Qr-complex               | 34    | 2.00%    |
| 4     | QRS is like QS-complex               | 492   | 28.94%   |
|       | Missing                              | 83    | 4.88%    |

46\. Presence of a lateral myocardial infarction (left ventricular) (ECG changes in leads V5: V6 , I, AVL) (lat_im):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | there is no infarct in this location | 576   | 33.88%   |
| 1     | QRS has no changes                   | 838   | 49.29%   |
| 2     | QRS is like QR-complex               | 97    | 5.71%    |
| 3     | QRS is like Qr-complex               | 72    | 4.24%    |
| 4     | QRS is like QS-complex               | 37    | 2.18%    |
|       | Missing                              | 80    | 4.71%    |

47\. Presence of an inferior myocardial infarction (left ventricular) (ECG changes in leads III, AVF, II). (inf_im):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | there is no infarct in this location | 937   | 55.12%   |
| 1     | QRS has no changes                   | 195   | 11.47%   |
| 2     | QRS is like QR-complex               | 191   | 11.24%   |
| 3     | QRS is like Qr-complex               | 121   | 7.12%    |
| 4     | QRS is like QS-complex               | 176   | 10.35%   |
|       | Missing                              | 80    | 4.71%    |

48\. Presence of a posterior myocardial infarction (left ventricular) (ECG changes in V7: V9, reciprocity changes in leads V1:V3) (post_im):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | there is no infarct in this location | 1370  | 80.59%   |
| 1     | QRS has no changes                   | 157   | 9.24%    |
| 2     | QRS is like QR-complex               | 52    | 3.06%    |
| 3     | QRS is like Qr-complex               | 35    | 2.06%    |
| 4     | QRS is like QS-complex               | 14    | 0.82%    |
|       | Missing                              | 72    | 4.24%    |

49\. Presence of a right ventricular myocardial infarction (IM_PG_P):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1649  | 97.00%   |
| 1     | yes        | 50    | 2.94%    |
|       | Missing    | 1     | 0.06%    |

50\. ECG rhythm at the time of admission to hospital: sinus (with a heart rate 60-90) (ritm_ecg_p_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 519   | 30.53%   |
| 1     | yes        | 1029  | 60.53%   |
|       | Missing    | 152   | 8.94%    |

51\. ECG rhythm at the time of admission to hospital: atrial fibrillation (ritm_ecg_p_02):

Type Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1453  | 85.47%   |
| 1     | yes        | 95    | 5.59%    |
|       | Missing    | 152   | 8.94%    |

52\. ECG rhythm at the time of admission to hospital: atrial (ritm_ecg_p_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1525  | 89.71%   |
| 1     | yes        | 23    | 1.35%    |
|       | Missing    | 152   | 8.94%    |

53\. ECG rhythm at the time of admission to hospital: idioventricular (ritm_ecg_p_06):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1547  | 91.00%   |
| 1     | yes        | 1     | 0.06%    |
|       | Missing    | 152   | 8.94%    |

54\. ECG rhythm at the time of admission to hospital: sinus with a heart rate above 90 (tachycardia) (ritm_ecg_p_07):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1195  | 70.29%   |
| 1     | yes        | 353   | 20.76%   |
|       | Missing    | 152   | 8.94%    |

55\. ECG rhythm at the time of admission to hospital: sinus with a heart rate below 60 (bradycardia) (ritm_ecg_p_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1502  | 88.35%   |
| 1     | yes        | 46    | 2.71%    |
|       | Missing    | 152   | 8.94%    |

56\. Premature atrial contractions on ECG at the time of admission to hospital (n_r_ecg_p_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1527  | 89.82%   |
| 1     | yes        | 58    | 3.41%    |
|       | Missing    | 115   | 6.76%    |

57\. Frequent premature atrial contractions on ECG at the time of admission to hospital (n_r_ecg_p_02):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1577  | 92.76%   |
| 1     | yes        | 8     | 0.47%    |
|       | Missing    | 115   | 6.76%    |

58\. Premature ventricular contractions on ECG at the time of admission to hospital (n_r_ecg_p_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1381  | 81.24%   |
| 1     | yes        | 204   | 12.00%   |
|       | Missing    | 115   | 6.76%    |

59\. Frequent premature ventricular contractions on ECG at the time of admission to hospital (n_r_ecg_p_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1516  | 89.18%   |
| 1     | yes        | 69    | 4.06%    |
|       | Missing    | 115   | 6.76%    |

60\. Paroxysms of atrial fibrillation on ECG at the time of admission to hospital (n_r_ecg_p_05):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1515  | 89.12%   |
| 1     | yes        | 70    | 4.12%    |
|       | Missing    | 115   | 6.76%    |

61\. Persistent form of atrial fibrillation on ECG at the time of admission to hospital (n_r_ecg_p_06):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1553  | 91.35%   |
| 1     | yes        | 32    | 1.88%    |
|       | Missing    | 115   | 6.76%    |

62\. Paroxysms of supraventricular tachycardia on ECG at the time of admission to hospital (n_r_ecg_p_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1581  | 93.00%   |
| 1     | yes        | 4     | 0.24%    |
|       | Missing    | 115   | 6.76%    |

63\. Paroxysms of ventricular tachycardia on ECG at the time of admission to hospital (n_r_ecg_p_09):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1583  | 93.12%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 115   | 6.76%    |

64\. Ventricular fibrillation on ECG at the time of admission to hospital (n_r_ecg_p_10):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1583  | 93.12%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 115   | 6.76%    |

65\. Sinoatrial block on ECG at the time of admission to hospital (n_p_ecg_p_01):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1583  | 93.12%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 115   | 6.76%    |

66\. First-degree AV block on ECG at the time of admission to hospital (n_p_ecg_p_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1553  | 91.35%   |
| 1     | yes        | 32    | 1.88%    |
|       | Missing    | 115   | 6.76%    |

67\. Type 1 Second-degree AV block (Mobitz I/ Wenckebach) on ECG at the time of admission to hospital (n_p_ecg_p_04):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1580  | 92.94%   |
| 1     | yes        | 5     | 0.29%    |
|       | Missing    | 115   | 6.76%    |

68\. Type 2 Second-degree AV block (Mobitz II/Hay) on ECG at the time of admission to hospital (n_p_ecg_p_05):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1583  | 93.12%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 115   | 6.76%    |

69\. Third-degree AV block on ECG at the time of admission to hospital (n_p_ecg_p_06):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1558  | 91.65%   |
| 1     | yes        | 27    | 1.59%    |
|       | Missing    | 115   | 6.76%    |

70\. LBBB (anterior branch) on ECG at the time of admission to hospital (n_p_ecg_p_07):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1483  | 87.24%   |
| 1     | yes        | 102   | 6.00%    |
|       | Missing    | 115   | 6.76%    |

71\. LBBB (posterior branch) on ECG at the time of admission to hospital (n_p_ecg_p_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1578  | 92.82%   |
| 1     | yes        | 7     | 0.41%    |
|       | Missing    | 115   | 6.76%    |

72\. Incomplete LBBB on ECG at the time of admission to hospital (n_p_ecg_p_09):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1575  | 92.65%   |
| 1     | yes        | 10    | 0.59%    |
|       | Missing    | 115   | 6.76%    |

73\. Complete LBBB on ECG at the time of admission to hospital (n_p_ecg_p_10):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1551  | 91.24%   |
| 1     | yes        | 34    | 2.00%    |
|       | Missing    | 115   | 6.76%    |

74\. Incomplete RBBB on ECG at the time of admission to hospital (n_p_ecg_p_11):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1557  | 91.59%   |
| 1     | yes        | 28    | 1.65%    |
|       | Missing    | 115   | 6.76%    |

75\. Complete RBBB on ECG at the time of admission to hospital (n_p_ecg_p_12):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1507  | 88.65%   |
| 1     | yes        | 78    | 4.59%    |
|       | Missing    | 115   | 6.76%    |

76\. Fibrinolytic therapy by Celiasum 750k IU (fibr_ter_01): Celiasum

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1677  | 98.65%   |
| 1     | yes        | 13    | 0.76%    |
|       | Missing    | 10    | 0.59%    |

77\. Fibrinolytic therapy by Celiasum 1m IU (fibr_ter_02):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1674  | 98.47%   |
| 1     | yes        | 16    | 0.94%    |
|       | Missing    | 10    | 0.59%    |

78\. Fibrinolytic therapy by Celiasum 3m IU (fibr_ter_03):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1622  | 95.41%   |
| 1     | yes        | 68    | 4.00%    |
|       | Missing    | 10    | 0.59%    |

79\. Fibrinolytic therapy by Streptase (fibr_ter_05):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1686  | 99.18%   |
| 1     | yes        | 4     | 0.24%    |
|       | Missing    | 10    | 0.59%    |

80\. Fibrinolytic therapy by Celiasum 500k IU (fibr_ter_06):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1681  | 98.88%   |
| 1     | yes        | 9     | 0.53%    |
|       | Missing    | 10    | 0.59%    |

81\. Fibrinolytic therapy by Celiasum 250k IU (fibr_ter_07):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1684  | 99.06%   |
| 1     | yes        | 6     | 0.35%    |
|       | Missing    | 10    | 0.59%    |

82\. Fibrinolytic therapy by Streptodecase 1.5m IU (fibr_ter_08):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1688  | 99.29%   |
| 1     | yes        | 2     | 0.12%    |
|       | Missing    | 10    | 0.59%    |

83\. Hypokalemia ( \< 4 mmol/L) (GIPO_K):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 797   | 46.88%   |
| 1     | yes        | 534   | 31.41%   |
|       | Missing    | 369   | 21.71%   |

84\. Serum potassium content (K_BLOOD) (mmol/L):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| K_BLOOD   | 2.3    | 8.2    | 4.19   | 0.75   | 371           | 21.82%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

85\. Increase of sodium in serum (more than 150 mmol/L) (GIPER_Na):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1295  | 76.18%   |
| 1     | yes        | 30    | 1.76%    |
|       | Missing    | 375   | 22.06%   |

86\. Serum sodium content (NA_BLOOD) (mmol/L):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| NA_BLOOD  | 117    | 169    | 136.55 | 6.51   | 375           | 22.06%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

87\. Serum AlAT content (ALT_BLOOD) (IU/L):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| ALT_BLOOD | 0.03   | 3      | 0.48   | 0.39   | 284           | 16.71%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

88\. Serum AsAT content (AST_BLOOD) (IU/L):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| AST_BLOOD | 0.04   | 2.15   | 0.26   | 0.2    | 285           | 16.67%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

89\. Serum CPK content (KFK_BLOOD) (IU/L):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| KFK_BLOOD | 1.2    | 3.6    | 2      | 0.95   | 1696          | 99.76%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

\*Note: There are too many missing values for us to make any sense of how the feature affects the outcomes. We will not consider this feature in our Analysis.

90\. White blood cell count (billions per liter) (L_BLOOD):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| L_BLOOD   | 2      | 27.9   | 8.78   | 3.40   | 125           | 7.35%            |
+-----------+--------+--------+--------+--------+---------------+------------------+

91\. ESR (Erythrocyte sedimentation rate) (ROE) (mm):

Type: Numeric/Real

+-----------+--------+--------+--------+--------+---------------+------------------+
| Attribute | Min    | Max    | Mean   | STD    | Missing cases | Missing fraction |
+-----------+--------+--------+--------+--------+---------------+------------------+
| ROE       | 1      | 140    | 13.44  | 11.29  | 203           | 19.94%           |
+-----------+--------+--------+--------+--------+---------------+------------------+

92\. Time elapsed from the beginning of the attack of CHD to the hospital (TIME_B_S):

Type: Ordinal/Categorical/Factor

Ordinal attribute. Possible usage of cumulative dummy coding.

|       |                   |       |          |
|-------|-------------------|-------|----------|
| Value | Represents        | Cases | Fraction |
| 1     | less than 2 hours | 198   | 11.65%   |
| 2     | 2-4 hours         | 360   | 21.18%   |
| 3     | 4-6 hours         | 175   | 10.29%   |
| 4     | 6-8 hours         | 87    | 5.12%    |
| 5     | 8-12 hours        | 92    | 5.41%    |
| 6     | 12-24 hours       | 151   | 8.88%    |
| 7     | more than 1 days  | 141   | 8.29%    |
| 8     | more than 2 days  | 101   | 5.94%    |
| 9     | more than 3 days  | 269   | 15.82%   |
|       | Missing           | 126   | 7.41%    |

93\. Relapse of the pain in the first hours of the hospital period (R_AB_1_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                     |       |          |
|-------|---------------------|-------|----------|
| Value | Represents          | Cases | Fraction |
| 0     | there is no relapse | 1282  | 75.41%   |
| 1     | only one            | 298   | 17.53%   |
| 2     | 2 times             | 78    | 4.59%    |
| 3     | 3 or more times     | 26    | 1.53%    |
|       | Missing             | 16    | 0.94%    |

94\. Relapse of the pain in the second day of the hospital period (R_AB_2_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                     |       |          |
|-------|---------------------|-------|----------|
| Value | Represents          | Cases | Fraction |
| 0     | there is no relapse | 1414  | 83.18%   |
| 1     | only one            | 133   | 7.82%    |
| 2     | 2 times             | 44    | 2.59%    |
| 3     | 3 or more times     | 1     | 0.06%    |
|       | Missing             | 108   | 6.35%    |

95\. Relapse of the pain in the third day of the hospital period (R_AB_3_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

+-------------+---------------------+-------------+----------------------------------------+
| Value       | Represents          | Cases       | Fraction                               |
+-------------+---------------------+-------------+----------------------------------------+
| 0           | there is no relapse | 1469        | 86.41%                                 |
+-------------+---------------------+-------------+----------------------------------------+
| 1           | only one            | 86          | 5.06%                                  |
+-------------+---------------------+-------------+----------------------------------------+
| 2           | 2 times             | 15          | 0.88%                                  |
+-------------+---------------------+-------------+----------------------------------------+
| 3           | 3 or more times     | 2           | 0.12%                                  |
+-------------+---------------------+-------------+----------------------------------------+
|             | Missing             | 128         | 7.53% (Incorrectly Marked in Original) |
+-------------+---------------------+-------------+----------------------------------------+

\*Note: There is a very small Typographic Error in the Original Document in the "Missing" value. It has been corrected here.

96\. Use of opioid drugs by the Emergency Cardiology Team (NA_KB):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 425   | 25.00%   |
| 1     | yes        | 618   | 36.35%   |
|       | Missing    | 657   | 38.65%   |

97\. Use of NSAIDs by the Emergency Cardiology Team (NOT_NA_KB):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 313   | 18.41%   |
| 1     | yes        | 701   | 41.23%   |
|       | Missing    | 375   | 22.06%   |

98.Use of lidocaine by the Emergency Cardiology Team (LID_KB):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 627   | 36.88%   |
| 1     | yes        | 396   | 23.29%   |
|       | Missing    | 677   | 39.82%   |

99\. Use of liquid nitrates in the ICU (NITR_S):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1496  | 88.00%   |
| 1     | yes        | 195   | 11.47%   |
|       | Missing    | 9     | 0.53%    |

100\. Use of opioid drugs in the ICU in the first hours of the hospital period (NA_R_1_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |             |       |          |
|-------|-------------|-------|----------|
| Value | Represents  | Cases | Fraction |
| 0     | no          | 1108  | 65.18%   |
| 1     | once        | 409   | 24.06%   |
| 2     | twice       | 132   | 7.76%    |
| 3     | three times | 35    | 2.06%    |
| 4     | four times  | 11    | 0.65%    |
|       | Missing     | 5     | 0.29%    |

101\. Use of opioid drugs in the ICU in the second day of the hospital period (NA_R_2_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |             |       |          |
|-------|-------------|-------|----------|
| Value | Represents  | Cases | Fraction |
| 0     | no          | 1474  | 86.71%   |
| 1     | once        | 87    | 5.12%    |
| 2     | twice       | 30    | 1.76%    |
| 3     | three times | 1     | 0.06%    |
|       | Missing     | 108   | 6.35%    |

102\. Use of opioid drugs in the ICU in the third day of the hospital period (NA_R_3_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1493  | 87.82%   |
| 1     | once       | 60    | 3.53%    |
| 2     | twice      | 16    | 0.94%    |
|       | Missing    | 131   | 7.71%    |

103\. Use of NSAIDs in the ICU in the first hours of the hospital period (NOT_NA_1_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |                    |       |          |
|-------|--------------------|-------|----------|
| Value | Represents         | Cases | Fraction |
| 0     | no                 | 1237  | 72.76%   |
| 1     | once               | 376   | 22.12%   |
| 2     | twice              | 53    | 3.12%    |
| 3     | three times        | 17    | 1.00%    |
| 4     | four or more times | 7     | 0.41%    |
|       | Missing            | 10    | 0.59%    |

104\. Use of NSAIDs in the ICU in the second day of the hospital period (NOT_NA_2_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |             |       |          |
|-------|-------------|-------|----------|
| Value | Represents  | Cases | Fraction |
| 0     | no          | 1454  | 85.53%   |
| 1     | once        | 95    | 5.59%    |
| 2     | twice       | 38    | 2.24%    |
| 3     | three times | 3     | 0.18%    |
|       | Missing     | 110   | 6.47%    |

105\. Use of NSAIDs in the ICU in the third day of the hospital period (NOT_NA_3_n):

Type: Ordinal/Categorical/Factor

Ordinal attribute can be interpreted as numerical. Possible encoding as is or cumulative dummy coding.

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1474  | 86.71%   |
| 1     | once       | 57    | 3.35%    |
| 2     | twice      | 38    | 2.24%    |
|       | Missing    | 131   | 7.71%    |

106\. Use of lidocaine in the ICU (LID_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1211  | 71.24%   |
| 1     | yes        | 479   | 28.18%   |
|       | Missing    | 10    | 0.59%    |

107\. Use of beta-blockers in the ICU (B_BLOK_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1474  | 86.71%   |
| 1     | yes        | 215   | 12.65%   |
|       | Missing    | 11    | 0.65%    |

108\. Use of calcium channel blockers in the ICU (ANT_CA_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 562   | 33.06%   |
| 1     | yes        | 1125  | 66.18%   |
|       | Missing    | 13    | 0.76%    |

109\. Use of anticoagulants (heparin) in the ICU (GEPAR_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 480   | 28.24%   |
| 1     | yes        | 1203  | 70.76%   |
|       | Missing    | 17    | 1.00%    |

110\. Use of acetylsalicylic acid in the ICU (ASP_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 431   | 25.35%   |
| 1     | yes        | 1252  | 73.65%   |
|       | Missing    | 17    | 1.00%    |

111\. Use of Ticlid in the ICU (TIKL_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1654  | 97.29%   |
| 1     | yes        | 30    | 1.76%    |
|       | Missing    | 16    | 0.94%    |

112\. Use of Trental in the ICU (TRENT_S_n):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1343  | 79.00%   |
| 1     | yes        | 341   | 20.06%   |
|       | Missing    | 16    | 0.94%    |

## **Complications**

113\. Atrial fibrillation (FIBR_PREDS):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1530  | 90.00%   |
| 1     | yes        | 170   | 10.00%   |
|       | Missing    |       | 0.0%     |

114\. Supraventricular tachycardia (PREDS_TAH):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1680  | 98.82%   |
| 1     | yes        | 20    | 1.18%    |
|       | Missing    |       | 0.0%     |

115\. Ventricular tachycardia (JELUD_TAH):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1658  | 97.53%   |
| 1     | yes        | 42    | 2.47%    |
|       | Missing    |       | 0.0%     |

116\. Ventricular fibrillation (FIBR_JELUD):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1629  | 95.82%   |
| 1     | yes        | 71    | 4.18%    |
|       | Missing    |       | 0.0%     |

117\. Third-degree AV block (A_V_BLOK):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1643  | 96.65%   |
| 1     | yes        | 57    | 3.35%    |
|       | Missing    |       | 0.0%     |

118\. Pulmonary edema (OTEK_LANC):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1541  | 90.65%   |
| 1     | yes        | 159   | 9.35%    |
|       | Missing    |       | 0.0%     |

119\. Myocardial rupture (RAZRIV):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1646  | 96.82%   |
| 1     | yes        | 54    | 3.18%    |
|       | Missing    |       | 0.0%     |

120\. Dressler syndrome (DRESSLER):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1625  | 95.59%   |
| 1     | yes        | 75    | 4.41%    |
|       | Missing    |       | 0.0%     |

121\. Chronic heart failure (ZSN):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1306  | 76.82%   |
| 1     | yes        | 394   | 23.18%   |
|       | Missing    |       | 0.0%     |

122\. Relapse of the myocardial infarction (REC_IM):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1541  | 90.65%   |
| 1     | yes        | 159   | 9.35%    |
|       | Missing    |       | 0.0%     |

123\. Post-infarction angina (P_IM_STEN):

Type: Binary/Nominal

|       |            |       |          |
|-------|------------|-------|----------|
| Value | Represents | Cases | Fraction |
| 0     | no         | 1552  | 91.29%   |
| 1     | yes        | 148   | 8.71%    |
|       | Missing    |       | 0.0%     |

124\. Lethal outcome (cause) (LET_IS):

Type: Category (Not Ordered): Considered as Categorical/Factor

|       |                                      |       |          |
|-------|--------------------------------------|-------|----------|
| Value | Represents                           | Cases | Fraction |
| 0     | unknown (alive)                      | 1429  | 84.06%   |
| 1     | cardiogenic shock                    | 110   | 6.47%    |
| 2     | pulmonary edema                      | 18    | 1.06%    |
| 3     | myocardial rupture                   | 54    | 3.18%    |
| 4     | progress of congestive heart failure | 23    | 1.35%    |
| 5     | thromboembolism                      | 12    | 0.71%    |
| 6     | asystole                             | 27    | 1.59%    |
| 7     | ventricular fibrillation             | 27    | 1.59%    |
|       | Missing                              |       | 0.0%     |
